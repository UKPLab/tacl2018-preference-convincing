\documentclass[11pt,letterpaper]{article}
\usepackage[letterpaper]{geometry}
\usepackage{acl2012}
\usepackage{times}
\usepackage{latexsym}
\usepackage[fleqn]{amsmath}
\usepackage{multirow}
\usepackage{url}
\makeatletter
\newcommand{\@BIBLABEL}{\@emptybiblabel}
\newcommand{\@emptybiblabel}[1]{}
\makeatother
\usepackage[hidelinks]{hyperref}
\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

% \documentclass[11pt]{article}
% \usepackage{acl2016}
% \usepackage{times}
% \usepackage{url}
% \usepackage{latexsym}
% 
% \usepackage[fleqn]{amsmath}
% \usepackage{amssymb}
% \usepackage{amstext}
% \usepackage{amsthm}
% 
% \usepackage{cite}

\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{algorithm2e}
\usepackage{array}
\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{url}
\usepackage{tabularx}
\usepackage{numprint}
\usepackage{multirow}

\newcommand{\bs}{\boldsymbol}  
\newcommand{\wrtd}{\mathrm{d}}

\makeatletter
\makeatother %some sort of hack related to the symbol @

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{ 
Scalable Bayesian Preference Learning from Crowds
}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}
\begin{document}

\maketitle

\begin{abstract}
We show how to make collaborative preference learning work at scale and how it can be used to learn
a target preference function from crowdsourced data or other noisy preference labels. 
The collaborative model captures the reliability of each worker or data source and models their biases and error rates. 
It uses latent factors to share information between similar workers and a target preference function.
We devise an SVI inference schema to enable the model to scale to real-world datasets.
Experiments compare results using standard variational inference, laplace approximation and SVI.
On real-world data we show the benefit of the personalised model over a GP preference learning approach 
that treats all labels as coming from the same source,
as well as established alternative methods and classifier baselines.
We show that the model is able to identify a number of latent features for the workers and for textual arguments.
\end{abstract}

% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
%\IEEEpeerreviewmaketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{sections/intro}
\input{sections/method}
\input{sections/experiments}
\input{sections/discussion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% use section* for acknowledgment
\section*{Acknowledgments}

\cleardoublepage

% \addcontentsline{toc}{chapter}{Bibliography}
%\bibliographystyle{apalike}
\bibliographystyle{acl2012}
\bibliography{simpson_scalabel_bayesian_preference_learning_from_crowds}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage
\appendix
\input{sections/appendix}

\end{document}
