\section{Scalable Inference}\label{sec:inf}

%n the single user case, the goal is to infer the posterior distribution over the utilities of test items, $\bs f^*$, 
%given a set of pairwise training labels, $\bs y$. In the multi-user case, w
Given a set of pairwise training labels, $\bs y$,
we aim to find the posterior over the matrix
$\bs F^*=\bs V^{*T} \bs W^*$ of utilities for test items and test users,
and the posterior over consensus utilities for test items, $\bs t^*$.
The non-Gaussian likelihood (Equation \ref{eq:plphi})
makes exact inference intractable, hence previous work uses
 the Laplace approximation for GPPL~\citep{chu2005preference}
or combines expectation propagation (EP) with variational Bayes for a 
multi-user model~\citep{houlsby2012collaborative}.
The Laplace approximation is a maximum a-posteriori solution that
takes the most probable values of parameters rather than integrating over their distributions,
and has been shown to perform poorly for classification compared to EP~\citep{nickisch2008approximations}. 
However, 
%for a latent factor model, the 
%EP and VB approximate the true posterior with a simpler, factorised distribution.
%%that can be learned using an iterative algorithm.
%For crowdGPPL, the true posterior is multi-modal, 
%In a latent factor model, the latent factors can be re-ordered arbitrarily without
%affecting $\bs F$, causing a \emph{non-identifiability problem}.
%Since using EP would average these modes and produce uninformative predictions over $\bs F$, so
%\citet{houlsby2012collaborative} incorporate a VB step that approximates a single mode.
a drawback of EP is that convergence is not guaranteed
%, even when distributions are conjugate 
~\citep{minka2001expectation}.
%do they also linearise in the same way? -- both linearise. But EP uses a joint over y and f as its approximation to p(y|f), then optimises the parameters iteratively. It's not guaranteed to converge. Variational EGP instead approximates
% p(y|f) directly with the best fit Gaussian. It's not clear whether this could be updated iteratively but it doesn't
% seem to work if done simultaneously with the other variables we need to learn (the linearisation), 
% perhaps because the algorithm for learning the weights breaks if the variance of q(y|f), Q, keeps changing. 
% Possibly because Q does not change incrementally. So it's
% possible that an outer loop could be used.
%TODO: remove redundancy with the related work section. Consider whether this should actually be in a background section. NM^2 is limiting, not just the other costs. The other costs NP etc come into play in this pairwise model only.
More importantly, inference for a GP using either method
has computational complexity $\mathcal{O}(N^3)$ 
and memory complexity $\mathcal{O}(N^2)$, where $N$ is the number of data points.

The cost of inference can be reduced using a \emph{sparse} approximation based on a set of 
\emph{inducing points}, which act as substitutes for the points in the training dataset.
By choosing a fixed number of inducing points, $M \ll N$, the computational cost is cut to $\mathcal{O}(NM^2)$,
and the memory complexity to $\mathcal{O}(NM)$.
Inducing points must be selected %to give a good approximation
using either heuristics or by optimising their positions to maximise an estimate of the 
marginal likelihood. 
One such sparse approximation is the \emph{generalized fully independent training conditional} (GFITC)~\citep{NIPS2007_3351,snelson2006sparse}, 
used by \citet{houlsby2012collaborative} for collabGP.
%which generalizes the fully independent training conditional 
%(FITC)~\citep{snelson2006sparse} method to non-Gaussian likelihoods.
%GFITC assumes that each training and test point is independent of all other points
%given the function values at the inducing points.
%This may be inappropriate for the pairwise likelihood (Equation \ref{eq:plphi})
%because it ignores the covariance between the utilities of the items.
However, time and memory costs that grow linearly with $\mathcal{O}(N)$
start to become a problem with thousands of data points,
as all data must be processed in every iterative update,
before any other parameters such as $s$ are updated,
making GFITC unsuitable for very large datasets~\citep{hensman2015scalable}.
%and distributed computation cannot be applied to GFITC to tackle the growing
%computational costs as the objective function does not contain a sum over observations~

We derive a more scalable approach for GPPL and crowdGPPL using
stochastic variational inference (SVI)~\citep{hoffman2013stochastic}.
%an iterative scheme that 
%limits the computational and memory costs at
%each iteration.
%and allows training data to be split into mini-batches for parallel processing.
%As we explain below,
%this allows us to reduce 
For GPPL, this reduces the time complexity of each iteration %of the algorithm 
%from $\mathcal{O}(NM^2)$ 
to $\mathcal{O}(P_i M^2 + Pi^2 M + M^3)$,
and memory complexity %from $\mathcal{O}(NM)$ 
to $\mathcal{O}(P_i M + M^2  + Pi^2)$,
where $P_i$ is a mini-batch size that we choose in advance.
Neither $P_i$ nor $M$ are dependent on the size of the dataset, meaning that SVI 
can be run with arbitrarily large datasets, 
and other model parameters such as $s$ can be updated before processing all data
to encourage faster convergence.
First, we define a suitable likelihood approximation to enable the use of SVI.

\subsection{Approximating the Posterior with a Pairwise Likelihood}

The preference likelihood in Equation \ref{eq:plphi} 
is not conjugate with the Gaussian process, which means there is no analytic expression for
the exact posterior.
For single-user GPPL, we therefore
approximate the preference likelihood with a Gaussian:
%This avoids the need for quadrature methods, as in \cite{hensman2015scalable} or ...
\begin{flalign}
p(\bs f | \bs y, s) & \propto \prod_{p=1}^P p\left(y_p | z_p\right) p\left(\bs f | \bs K, s\right)
= \prod_{p=1}^P \Phi\left(z_p\right) \mathcal{N}\left(\bs f; \bs 0, \bs K/s\right)
%= \mathbb{E}\left[\prod_{p=1}^P \Phi(z_p)\right] = \prod_{p=1}^P \Phi(\hat{z}_p) 
%\approx \mathcal{N}(\bs y; \Phi(\hat{\bs z}), \bs Q),
& \\
& \approx \prod_{p=1}^P \mathcal{N}\left(y_p; \Phi(z_p), Q_{p,p}\right) 
\mathcal{N}\left(\bs f; \bs 0, \bs K/s\right)
 = \mathcal{N}\left(\bs y; \Phi(\bs z), \bs Q\right) \mathcal{N}\left(\bs f; \bs 0, \bs K/s\right), &\nonumber 
\end{flalign}
where $\bs Q$ is a diagonal noise covariance matrix.
For crowdGPPL, we use the same approximation to the likelihood, but
replace $\bs f$ with $\bs F$.
% and the priors over $\bs v$, $\bs t$ and $\bs w$
%throughout this section.
%There are two problems with this approximation so far:
%firstly, $\Phi(\bs z)$ is a nonlinear function of $\bs f$,
%which makes the posterior intractable:
%\begin{flalign}
%p(\bs f | \bs y) %\propto p(\bs y | \bs f)\mathcal{N}(\bs f; \bs 0, \bs K/s)
%\approx \frac{ \mathcal{N}(\bs y; \Phi(\bs z), \bs Q)\mathcal{N}(\bs f; \bs 0, \bs K/s) }{
%\int \mathcal{N}(\bs y; \Phi(\bs z), \bs Q)\mathcal{N}(\bs f'; \bs 0, \bs K/s) df'}
%\end{flalign}
%Secondly, we need to estimate the diagonal variance terms in $\bs Q$.
We estimate the diagonals of $\bs Q$ 
by moment matching our approximate likelihood with $\Phi(z_p)$,
which defines a Bernoulli distribution with variance $Q_{p,p} = \Phi(z_p)(1 - \Phi(z_p))$.
However, this means that $\bs Q$ % the variance of the approximate likelihood
depends on $\bs z$ and therefore on $\bs f$,
so the posterior remains intractable.
To resolve this, we approximate $Q_{p,p}$ 
using an estimated posterior over $\Phi(z_p)$ computed
independently for each pairwise label, $p$.
%thereby replacing intractable expectations with respect to $p(\bs f|\bs y)$
%with simple 
We obtain this estimate
 by updating the parameters of the conjugate prior for the Bernoulli likelihood,
 which is
a beta distribution with parameters $\gamma$ and $\lambda$.
We find $\gamma$ and $\lambda$ by 
matching the moments of the beta prior to the mean and variance of $\Phi(z_p)$
defined by the Gaussian process prior, 
$p(\Phi(z_p) | \bs K_{\theta}, \alpha_0, \beta_0)$,  
estimated using numerical integration. 
%Assuming a beta prior over $\Phi(z_p)$ means that $y_p$ has a beta-Bernoulli
%distribution. 
Given the observed label $y_p$, we estimate the diagonals in $\bs Q$
as the variance of the posterior beta-Bernoulli:
\begin{flalign}
Q_{p,p} & \approx \frac{ (\gamma + y_p)(\lambda + 1 - y_p) }{(\gamma + \lambda + 1)^2}. &
\end{flalign}
This approximation performs well empirically
for Gaussian process classification~\citep{reece2011determining,simpson2017bayesian} and 
classification using extended Kalman filters~\citep{lee2010sequential,lowne2010sequential}. 

Unfortunately, the nonlinear term $\Phi(\bs z)$ means that the posterior is still intractable, 
so we replace $\Phi(\bs z)$ with a linear function of $\bs f$ by taking
the first-order Taylor series expansion of $\Phi(\bs z)$ 
about the expectation $\mathbb{E}[\bs f] = \hat{\bs f}$:
\begin{flalign}
\Phi(\bs z) &\approx \tilde{\Phi}(\bs z) = \bs G \left(\bs f-\hat{\bs f}\right) 
+ \Phi(\hat{\bs z}), & \\
G_{p,i} &= \frac{\partial \Phi(\hat{z_p})} {\partial f_i}
= \Phi(\hat{z}_p)\left(1 - \Phi(\hat{z}_p)\right) \left(2y_p - 1\right)\left( [i = a_p] - [i = b_p]\right), &
\end{flalign}
%where $\bs G$ is a matrix whose elements 
%$G_{p,i}= \Phi(\hat{z}_p)(1 - \Phi(\hat{z}_p)) (2y_p - 1)( [i = a_p] - [i = b_p])$ 
%are the partial derivatives of $\Phi(\hat{z_p})$ %the pairwise likelihood 
%with respect to $f_i$.
%of the latent function values, $\bs f$.
where $\hat{\bs z}$ is the expectation of $\bs z$ computed using Equation \ref{eq:predict_z}.
There is a circular dependency between $\hat{\bs f}$,
which is needed to compute $\hat{\bs z}$, and $\bs G$. %the linearization terms in the likelihood,
We estimate these terms using a variational inference procedure
that iterates between updating $\bs f$ and $\bs G$~\citep{steinberg2014extended}
as part of Algorithm \ref{al:singleuser}.
%and is described in more detail below.
The complete approximate posterior for GPPL is now as follows:
\begin{flalign}
p(\bs f | \bs y, s) 
\approx %\frac{1}{Z}
\mathcal{N}(\bs y; \bs G (\bs f-\mathbb{E}[\bs f]) + \Phi(\hat{\bs z}), \bs Q) \mathcal{N}(\bs f; \bs 0, \bs K/s) / Z = \mathcal{N}(\bs f; \hat{\bs f}, \bs C), &&
\label{eq:likelihood_approx} 
\end{flalign}
where $Z$ is a normalisation constant.
Linearisation means that our approximate likelihood is conjugate to the prior,
so the approximate posterior is also Gaussian. 
%TODO any other variant in Nickish that uses linearization? Can we cite nickisch to say that linearization is good, m'kay?
%TODO show the posterior without further SVI approximations?
Gaussian approximations to the posterior have shown strong empirical results for 
classification~\citep{nickisch2008approximations} and
preference learning~\citep{houlsby2012collaborative},
%including the 
%expectation propagation method~\citep{rasmussen_gaussian_2006}.
and linearisation using a Taylor expansion has been widely tested
in the extended Kalman filter~\citep{haykin2001kalman}
as well as Gaussian processes~\citep{steinberg2014extended,bonilla2016extended}.
%\todo{generate a synthetic plot showing the 
%difference between a true posterior over f and our
%approximation on synthetic data. Do what Nickisch did?}
%Given our approximate posterior, we now derive an efficient inference scheme using SVI.

\subsection{SVI for Single User GPPL}

%TODO what's going on here? First, we need to learn s. Second, we need a more efficient way to learn
% f without inverting K and without using all observations at once.
Using the linear approximation in the previous section, 
posterior inference requires inverting
$\bs K$ with computational cost $\mathcal{O}(N^3)$
and taking an expectation with respect to $s$, which remains intractable. 
We address these problems using stochastic variational inference (SVI)
with a sparse approximation to the GP that limits
the size of the covariance matrices we need to invert.
We introduce $M \ll N$ inducing items with inputs 
$\bs X_m$,
utilities $\bs f_m$, and covariance $\bs K_{mm}$. The
covariance between the observed and inducing items is $\bs K_{nm}$.
% The inducing points act as proxies for the observed points during inference,
% and thereby reduce the number of data points we have to perform costly operations % over.
%We modify the variational approximation in Equation \ref{eq:vb_approx} to introduce the inducing points 
For clarity, we omit $\theta$ from this point on.
We assume a \emph{mean-field} approximation to the joint posterior over 
inducing and training items
that factorises between different sets of latent variables:
\begin{flalign}
p\left(\bs f, \bs f_m, s | \bs y, \bs X, \bs X_m, k_{\theta}, \alpha_0, \beta_0 \right) 
&\approx q\left(\bs f, \bs f_m, s\right) = q(s)q\left(\bs f\right)q\left(\bs f_m\right), \label{eq:svi_approx} &&
\end{flalign}
where $q(.)$ are \emph{variational factors} defined below. 
Each factor corresponds to a subset of latent variables, $\bs \zeta_i$, and
takes the form $\ln q(\bs \zeta_i) = \mathbb{E}_{j \neq i}[\ln p(\bs \zeta_i, \bs x, \bs y)]$.
That is, the expectation with respect
to all other latent variables, $\bs\zeta_j,\forall j \neq i$, of the log joint distribution
of the observations and latent variables, $\bs \zeta_i$.
To obtain the factor for $\bs f_m$, we marginalise $\bs f$ and take expectations with respect to $q(s)$:
\begin{flalign}
\ln q\left(\bs f_m\right) &= \ln \mathcal{N}\!\left(\bs y; \tilde{\Phi}(\bs z), \bs Q\right)
+ \ln\mathcal{N}\left(\bs f_m; \bs 0, \frac{\bs K_{mm}}{\mathbb{E}\left[s\right]}\right) \!  + \textrm{const} %& \nonumber \\
 = \ln \mathcal{N}\left(\bs f_m; \hat{\bs f}_m, \bs S \right), &
 \label{eq:fhat_m}
\end{flalign}
where the variational parameters $\hat{\bs f}_m$ and $\bs S$ are computed using 
an iterative SVI procedure described below.
We choose an approximation of $q(\bs f)$ that depends only on the inducing point utilities, $\bs f_m$, and is independent of the observations:
 \begin{flalign}
\ln q\left(\bs f\right) & = \ln \mathcal{N}\left(\bs f; \bs A \hat{\bs f}_m, 
\bs K + \bs A (\bs S - \bs K_{mm}/\mathbb{E}[s]) \bs A^T \right), &
\end{flalign}
where $\bs A=\bs K_{nm} \bs K^{-1}_{mm}$.
Therefore, we no longer need to invert an $N \times N$ covariance matrix to compute $q(\bs f)$.
The factor $q(s)$ also depends only the inducing points:
\begin{flalign}
& \ln q(s) = \mathbb{E}_{q(\bs f_m)}[\ln\mathcal{N}(\bs f_m| \bs 0, \bs K_{mm}/s)] + \ln \mathcal{G}(s; \alpha_0, \beta_0) + \mathrm{const}
= \ln \mathcal{G}(s; \alpha, \beta), & \label{eq:qs}
\end{flalign}
where $\alpha= \alpha_0 + \frac{M}{2}$ and $\beta = \beta_0 + \frac{1}{2}
\textrm{tr}\left(\bs K^{-1}_{mm}\left(S + \hat{\bs f}_m \hat{\bs f}_m^T\right)\right)$.
The expected value is  
$\mathbb{E}[s] = \frac{\alpha}{\beta}$.

We apply variational inference to iteratively reduce the KL-divergence between our approximate posterior
%$q(s)q(\bs f)q(\bs f_m)$
and the true posterior (Equation \ref{eq:svi_approx}) %, $p(s, \bs f, \bs f_m | \bs K, \alpha_0, \beta_0, \bs y)$,
by maximising a lower bound, $\mathcal{L}$, on the log marginal likelihood (detailed equations in Appendix \ref{sec:vb_eqns}), which is given by:
%(see also Equation \ref{eq:full_L_singleuser} in the Appendix):%, $\ln p(\bs y | \bs K, \alpha_0, \beta_0)$ :
\begin{flalign}
&\ln p(\bs y | \bs K, \alpha_0, \beta_0) = \textrm{KL}\left(q\left(\bs f, \bs f_m, s\right)  || p\left(\bs f, \bs f_m, s | \bs y, \bs K, \alpha_0, \beta_0\right)\right) 
+ \mathcal{L} & \label{eq:lowerbound}
\\
%\end{flalign}
%Taking expectations with respect to the variational $q$ distributions, $\mathcal{L}$ is:
%\begin{flalign}
&\mathcal{L} = \mathbb{E}_{q(\bs f)}\left[\ln p(\bs y | \bs f)\right]
+ \mathbb{E}_{q\left(\bs f_m, s\right)}\left[\ln p\left(\bs f_m, s | \bs K, 
\alpha_0, \beta_0 \right) -\ln q\left(\bs f_m\right) - \ln q(s)\right]. & \nonumber
\end{flalign}
%         invK_mm_expecFF = self.invK_mm.dot(self.uS + self.um_minus_mu0.dot(self.um_minus_mu0.T))
%         self.rate_s = self.rate_s0 + 0.5 * np.trace(invK_mm_expecFF)
To optimise $\mathcal{L}$,
we initialise the $q$ factors randomly, then
update each one in turn, taking expectations with respect to the other factors. 

The only term in $\mathcal{L}$ that refers to the observations, $\bs y$, 
is a sum of $P$ terms, each of which refers to one observation only.
This means that $\mathcal{L}$ can be maximised by considering a random subset of 
observations at each iteration~\citep{hensman2013gaussian}.
%Therefore, the SVI solution replaces Equations \ref{eq:fhat_m} and \ref{eq:S} for computing
%$\hat{\bs f}_m$ and $\bs S$ over all observations with a sequence of stochastic updates.
For the $i$th update of $q(\bs f_m)$, we randomly select $P_i$ 
observations $\bs y_i = \{ y_p \forall p \in \bs P_i \}$, 
where $\bs P_i$ is a random subset of indexes of observations,
and $P_i$ is a mini-batch size.
The items referred to by the pairs in the subset are 
$\bs N_i = \{a_p \forall p \in \bs P_i \} \cup \{ b_p \forall p \in \bs P_i\}$.
We  perform updates using $\bs Q_i$ (rows and columns of $\bs Q$ for pairs in $\bs P_i$),
$\bs K_{im}$ and $\bs A_i$ (rows of $\bs K_{nm}$ and $\bs A$ in $\bs N_i$),
$\bs G_i$ (rows of $\bs G$ in $\bs P_i$ and columns in $\bs N_i$), and
$\hat{\bs z}_i = \left\{ \hat{\bs z}_p \forall p \in P_i \right\}$.
%All matrices with subscript $_i$ contain only the subset of elements relating to 
%observations in $\bs P_i$.
% The linearization matrix $\bs G_i$ is the subset of elements in $\bs G$ relating to observations in $\bs P_i$, 
%  is the corresponding subset of elements in $\bs Q$,
%  is the covariance between the items referred to by pairs in $\bs P_i$ 
% and the inducing points,
% and  contains the corresponding rows of $\bs A$.
The updates optimise the natural parameters of the Gaussian distribution by following the
natural gradient~\citep{hensman2015scalable}:
\begin{flalign}
\bs S^{-1}_i  & = (1 - \rho_i) \bs S^{-1}_{i-1} + \rho_i\left( \mathbb{E}[s]\bs K_{mm}^{-1} + \pi_i\bs A_i^T \bs G^T_{i} \bs Q^{-1}_i \bs G_{i} \bs A_{i} \right)& 
\label{eq:S_stochastic} \\
\hat{\bs f}_{m,i}  & = \bs S_i \left( \! (1 - \rho_i) \bs S^{-1}_{i-1} \hat{\bs f}_{m,i-1}  + 
%\right. \nonumber \\
%& \left.\hspace{1.5cm} 
\rho_i \pi_i  
\bs A_{i}^{T} \bs G_{i}^T \bs Q_i^{-1}\! \left( \bs y_i  - \Phi(\hat{\bs z}_i) + \bs G_{i} \bs A_i \hat{\bs f}_{m,i-1} \! \right) \! \right) & 
\label{eq:fhat_stochastic}
\end{flalign}
where
$\rho_i=(i + \epsilon)^{-r}$ is a mixing coefficient that controls the update rate,
$\pi_i = \frac{P}{P_i}$ weights each update according to sample size,
 $\epsilon$ is a delay hyperparameter and $r$ is a forgetting rate~\citep{hoffman2013stochastic}.

By performing updates in terms of mini-batches, 
the time complexity of Equations \ref{eq:S_stochastic} and
\ref{eq:fhat_stochastic} is
%has order $\mathcal{O}(M^3)$, and the second term has order 
$\mathcal{O}(P_i M^2 + P_i^2 M + M^3)$ and
%The $P_i^2$ term arises due to $\bs G_i$, which is an $N_i \times P_i$ matrix, where $N_i \leq 2P_i$ is the number of 
%items referred to by the pairwise labels in the mini-batch.
memory complexity is  $\mathcal{O}(M^2 + P_i^2 + M P_i)$.
%, where each complexity term is due
%to the sizes of $K_{mm}$, $G_i$ and $K_{im}$.
The only parameters that must be stored between iterations relate to the 
inducing points, hence the memory consumption does not grow with the dataset size 
as in the GFITC approximation used by \citet{houlsby2012collaborative}.
A further advantage of stochastic updating is that the $s$ parameter (and any other global
parameters not immediately depending on the data) can be learned
before the entire dataset has been processed,
which means that poor initial estimates of $s$ are rapidly improved
and the algorithm can converge faster.

\begin{algorithm}
 \KwIn{ Pairwise labels, $\bs y$, training item features, $\bs x$, 
 test item features $\bs x^*$}
 \nl Select inducing point locations $\bs x_{mm}$ and compute kernel matrices $\bs K$, $\bs K_{mm}$ and $\bs K_{nm}$ given $\bs x$ \;
 \nl Initialise $\mathbb{E}[s]$ and $\hat{\bs f}_m$ to prior means
 and $\bs S$ to prior covariance $\bs K_{mm}$\;
 \While{$\mathcal{L}$ not converged}
 {
 \nl Select random sample, $\bs P_i$, of $P$ observations\;
 \While{$\bs G_i$ not converged}
  {
  \nl Compute $\mathbb{E}[\bs f_i]$ \;
  \nl Compute $\bs G_i$ given $\mathbb{E}[\bs f_i]$ \;
  \nl Compute $\hat{\bs f}_{m,i}$ and $\bs S_{i}$ \;
  }
 \nl Update $q(s)$ and compute $\mathbb{E}[s]$ and $\mathbb{E}[\ln s]$\;
 }
\nl Compute kernel matrices for test items, $\bs K_{**}$ and $\bs K_{*m}$, given $\bs x^*$ \;
\nl Use converged values of $\mathbb{E}[\bs f]$and $\hat{\bs f}_m$ to estimate
posterior over $\bs f^*$ at test points \;
\KwOut{ Posterior mean of the test values, $\mathbb{E}[\bs f^*]$ and covariance, $\bs C^*$ }
\vspace{0.2cm}
\caption{The SVI algorithm for GPPL: preference learning with a single user.}
\label{al:singleuser}
\end{algorithm}
The complete SVI algorithm is summarised in Algorithm \ref{al:singleuser}.
It uses a nested loop to learn $\bs G_i$, which avoids storing the complete matrix, 
$\bs G$.
It is possible to distribute computation in lines 3-6 by selecting multiple random samples
to process in parallel. A global estimate of $\hat{\bs f}_m$ and $\bs S$
is passed to each compute node, which runs the loop over lines 4 to 6.
The resulting updated $\hat{\bs f}_m$ and $\bs S$ values are then passed back to a 
central node that combines them by taking a mean weighted by $\pi_i$ to account for 
the size of each batch. 
%This does not require modifying Equations \ref{eq:S_stochastic} and \ref{eq:fhat_stochastic}, since they already contain a sum weighted by $\pi_i$.

Inducing point locations can be learned
as part of the variational inference procedure, which
%or by optimising a bound on the log marginal likelihood.
breaks convergence guarantees, or by an expensive optimisation process~\citep{hensman2015scalable}. 
We obtain good performance by choosing inducing points up-front 
using K-means++~\citep{arthur2007k} with $M$ clusters to cluster
the feature vectors, 
then taking the cluster centres as inducing points that represent the distribution of observations.

The inferred distribution over the inducing points can be used 
to estimate the posteriors of test items, $f(\bs x^*)$, according to:
\begin{flalign}
\bs f^* \! \! &= \bs K_{*m} \bs K_{mm}^{-1} \hat{\bs f}_m, &
\bs C^* \! \! = \bs K_{**} + \bs K_{*m} \bs K_{mm}^{-1} (\bs S - \bs K_{mm} / \mathbb{E}[s] ) \bs K_{mm}^{-1}\bs K_{*m}^T ,
\end{flalign}
where $\bs C^*$ is the posterior covariance of the test items, $\bs K_{**}$ is their prior covariance, and
$\bs K_{*m}$ is the covariance between test and inducing items.
%It is possible to recover the lower bound proposed by 
%\citet{hensman2015scalable} for classification by generalizing the
%likelihood to arbitrary nonlinear functions, and omitting terms relating to $p(s|\alpha_0,\beta_0)$ and $q(s)$.
% However, our approach avoids expensive quadrature methods by linearizing the likelihood to enable analytical updates. We also infer $s$ in a Bayesian manner, 
% rather than treating as a hyper-parameter, which is important for preference learning where $s$ controls the noise level of the observations relative to  $f$. 

\subsection{SVI for CrowdGPPL}

We now provide the variational posterior for the crowdGPPL model defined in Equation \ref{eq:joint_crowd}:
\begin{flalign}
& p\left( \bs V, \bs V_m, \bs W, \bs W_m, \bs t, \bs t_m, s^{(v)}_1, .., s^{(v)}_C,
s^{(w)}_1, .., s^{(w)}_C, s^{(t)} | \bs y, \bs X, \bs X_m, \bs U, \bs U_m, k, \alpha_0, \beta_0 \right) 
& \nonumber \\
& \approx q(\bs t) q(\bs t_m)q\left(s^{(t)}\right)\prod_{c=1}^{C} q(\bs v_{c})q(\bs w_c)q(\bs v_{c,m})q(\bs w_{c,m})
q\left(s^{(v)}_c\right)q\left(s^{(w)}_c\right), & %\nonumber \\
%& = q(\bs F) q(s^{(t)}) \prod_{c=1}^C q(s^{(v)}_c), &
\end{flalign}
where $\bs U_m$ are the feature vectors of inducing users and the variational $q$ factors are defined below.
We use SVI to optimise the lower bound on the log marginal likelihood 
(detailed in Appendix \ref{sec:crowdL}), which is given by:
\begin{flalign}
& \mathcal{L}_{cr} = 
\mathbb{E}_{q(\bs F)}%(\bs t, \bs t_m, \bs V, \bs V_m, \bs W, \bs W_m, s_1,...,s^{(v)}_c,s^{(t)})
[\ln p(\bs y | \bs F)] 
+ \mathbb{E}_{q\left(\bs t_m, s^{(t)}\right)} \left[\ln p\left(\bs t_m, s^{(t)} | \bs K_{mm}, \alpha_0^{(t)}, \beta_0^{(t)}\right)
- \ln q(\bs t_m)  - \ln q\left(s^{(t)}\right) \right]  & \nonumber \\
&
+ \sum_{c=1}^C \!\! \bigg\{  \mathbb{E}_{q\left(\bs v_{m,c},s^{(v)}_c\right)}\left[\ln p\left(\bs v_{m,c}, s^{(v)}_c | \bs K_{mm}, \alpha_0^{(v)}, \beta_0^{(v)}\right) - \ln q(\bs v_{m,c}) - \ln q\left(s_c^{(v)}\right) \right]
&  \nonumber \\ 
& 
+  \mathbb{E}_{q\left(\bs w_{m,c}, s_c^{(w)}\right)}\left[\ln p\left(\bs w_{m,c},s^{(w)}_c | \bs L_{mm}, \alpha_0^{(w)}, \beta_0^{(w)} \right)
  - \ln q(\bs w_{m,c} )  - \ln q\left(s_c^{(w)} \right) \right] \bigg\} . & 
  \label{eq:lowerbound_crowd}
\end{flalign}
The SVI algorithm 
follows the same pattern as Algorithm \ref{al:singleuser}, 
updating each $q$ factor in turn by computing means and covariances
for  $\bs V_m$, $\bs W_m$ and $\bs t_m$ instead of $\bs f_m$ (see Algorithm \ref{al:crowdgppl}).
The time and memory complexity of each update are
%the same as for single-user GPPL, 
%except that we now have $C$ updates, and the number of inducing points for items and users 
%may be different:
$\mathcal{O}(CM_{\mathrm{items}}^3 + CM_{\mathrm{items}}^2 P_i + CM_{\mathrm{items}} P_i^2$
$ + CM_{\mathrm{users}}^3 + CM_{\mathrm{users}}^2 P_i + CM_{\mathrm{users}} P_i^2 )$ 
%Memory complexity for crowdGPPL is
and 
$\mathcal{O}(CM_{\mathrm{items}}^2 + P_i^2 + M_{\mathrm{items}} P_i + CM_{\mathrm{users}}^2 + M_{\mathrm{users}} P_i)$, respectively.
%
The variational factor for the $c$th inducing item component is:
\begin{flalign}
\ln q(\bs v_{m,c})  = & 
\ln \mathcal{N}\left( \bs y; \tilde{\Phi}(\bs z), Q \right) % \right] 
+ \ln\mathcal{N}\left(\bs v_{\!m,c}; \bs 0, \frac{\bs K_{mm}}{\mathbb{E}[s^{(v)}_c]}\right) 
+ \textrm{const} %& \nonumber \\
% are the dimensions collapsed to a single MVN?
= \! \ln \mathcal{N}\left(\bs v_{\!m,c}; \hat{\bs v}_{\!m,c}, \bs S_c^{(v)} \right)\!, &
\end{flalign}
where posterior mean $\hat{\bs v}_{m,c}$ and covariance $\bs S_c^{(v)}$ are computed using 
equations of the same form as % those of the single user GPPL in 
Equations \ref{eq:S_stochastic} and \ref{eq:fhat_stochastic}, except $\bs Q^{-1}$
 is scaled by expectations over $\bs w_{m,c}$,
and $\hat{\bs f}_{m,i}$ is replaced by $\hat{\bs v}_{m,c,i}$.
The factor for the inducing points of $\bs t$ follows a similar pattern to $\bs v_{m,c}$:
\begin{flalign}
\ln q(\bs t_m) & = %\mathbb{E}_{q(\bs F)}[
\ln \mathcal{N}\left( \bs y; \tilde{\Phi}(\bs z), Q \right) %] 
+ \ln\mathcal{N}\left( \! \bs t_m; \bs 0, \frac{\bs K_{mm}}{\mathbb{E}[s^{(t)}]} \! \right)
\!+\! \textrm{const} %& \nonumber \\
= \ln \mathcal{N}\left( \bs t_m; \hat{\bs t}_{m}, \bs S^{(t)} \right), & 
\end{flalign}
where the equations for $\hat{\bs t}$ and $\bs S^{(t)}$ 
are the same as Equations \ref{eq:S_stochastic} and \ref{eq:fhat_stochastic}, 
except $\hat{\bs f}_{m,i}$ is replaced by $\hat{\bs t}_{m,i}$. 
%(see also Equations \ref{eq:St} and \ref{eq:hatt}).
Finally, %require a different linearization matrix, $\bs J \in P \times U$, containing partial derivatives 
%of the pairwise likelihood with respect to $\hat{w}_c$. Its elements are given by:
%\begin{flalign}
%J_{p,j} = \Phi(\mathbb{E}[z_p])(1 - \Phi(\mathbb{E}[z_p]) (2y_p - 1) [u_p = j] % needs to be added or subtracted depending on a or b
%\end{flalign} 
%now multiply by V. What about covariances between v?
the variational distribution for each inducing user's component is:% then as follows:
\begin{flalign}
\ln q(\bs w_{\! m,c} )  = & %\mathbb{E}_{q(\bs F)}\left[
\ln \mathcal{N}\! \left( \bs y; \tilde{\Phi}(\bs z), Q \right) %\right] 
+ \ln\mathcal{N}\!\left(\bs w_{\! m,c}; \bs 0, \frac{\bs L_{mm}}{\mathbb{E}[s^{(w)}_c]} \right)
+ \textrm{const} % & \nonumber \\
= \ln \mathcal{N}\!\left( \bs w_{\! m,c}; \hat{\bs w}_{\!m,c}, \bs \Sigma_c \right)\!, & 
\end{flalign}
where $\hat{\bs w}_c$ and $\bs \Sigma_{c}$ also follow the pattern of
Equations \ref{eq:S_stochastic} and \ref{eq:fhat_stochastic},
with $\bs Q^{-1}$ scaled by expectations of
$\bs w_{c,m}$,
 and $\hat{\bs f}_{m,i}$ replaced by $\hat{\bs w}_{m,c,i}$.
%(see also Appendix \ref{sec:post_params}, Equations \ref{eq:Sigma} and \ref{eq:what}).
We provide the complete equations for the variational means 
and covariances for $\bs v_{m,c}$, $\bs t_m$ and $\bs w_{m,c}$ in 
Appendix \ref{sec:post_params}.
The expectations for inverse scales, $s^{(v)}_1,..,s^{(v)}_c$, $s^{(w)}_1,..,s^{(w)}_c$
 and $s^{(t)}$ can be computed using Equation \ref{eq:qs} by
substituting the corresponding terms for $\bs v_c$, $\bs w_c$ or $\bs t$ instead of $\bs f$. 

% The equations for the means and covariances 
% can be adapted for stochastic updating by applying weighted sums over
% the stochastic update and the previous values in the 
% same way as  Equation \ref{eq:S_stochastic} and \ref{eq:fhat_stochastic}.
% The stochastic updates for the inducing points of the latent factors depend 
% on expectations with respect to the observed points. 
% As with the single user case, the variational factors at the observed items are independent of the observations given the variational factors of the inducing points
% (likewise for the observed users):
% \begin{flalign}
% \ln q(\bs V) & = \sum_{c=1}^C \ln \mathcal{N}\left( \bs v_c; \bs A_v\hat{\bs v}_{m,c}, 
% \frac{\bs K_{v}}{\mathbb{E}[s^{(v)}_c]} + \bs A_v (\bs S_{m,c} - \frac{\bs K_{mm}}{\mathbb{E}[s^{(v)}_c]})\bs A_v \right) & \label{eq:qv} \\
% \ln q(\bs t) & = \ln \mathcal{N}\left( \bs t; \bs A_t \hat{\bs t}_m, 
% \frac{\bs K_{t}}{\mathbb{E}[s^{(t)}]} + \bs A_t (\bs s^{(t)} - \frac{\bs K_{mm}}{\mathbb{E}[s^{(t)}]})\bs A_t \right)  & \label{eq:qt}\\
% \ln q(\bs W) & = \sum_{c=1}^C \ln \mathcal{N}\left( \bs w_c; \bs A_w \hat{\bs w}_{m,c}, \bs L_{} + \bs A_w (\bs\Sigma - \bs L_{mm}/s^{(w)}_c) \bs A_w \right). &
% \label{eq:qw}
% \end{flalign}
%As with GPPL, the stochastic updates are amenable to parallel computation within one iteration 
%of the variational inference algorithm,
% by performing computations for mini-batches of training data in parallel. 

Predictions for crowdGPPL can be made by computing the posterior mean utilities, $\bs F^*$, 
and the covariance $\bs \Lambda_u^*$ for each user, $u$, in the test set:
\begin{flalign} \label{eq:predict_crowd}
&\bs F^* = \hat{\bs t}^* + \sum_{c=1}^C \hat{\bs v}_{c}^{*T} \hat{\bs w}_{c}^*, \hspace{1cm} \bs \Lambda_u^* = \bs C_{t}^* + \sum_{c=1}^C \omega_{c,u}^* \bs C_{v,c}^* + \hat{w}_{c,u}^2  \bs C_{v,c}^*  +\omega_{c,u}^* \hat{\bs v}_{c}\hat{\bs v}_{c}^T, &
\end{flalign}
where $\hat{\bs t}^*$, $\hat{\bs v}_{c}^*$ and $\hat{\bs w}_{c}^*$ are posterior test means,
$\bs C_{t}^*$ and $\bs C_{v,c}^*$ are posterior covariances of the test items,
and $\omega_{c,u}^*$ is the posterior variance of the user components for $u$. 
(see Appendix \ref{sec:predictions}, Equations \ref{eq:tstar} to \ref{eq:omegastar}).
The mean $\bs F^*$ and covariances $\Lambda^*_u$ can be inserted into Equation \ref{eq:plphi} to predict pairwise labels.
In practice, the full covariance terms are needed only for Equation \ref{eq:plphi}, so need only be computed
between items for which we wish to predict pairwise labels. 