\section{Experiments}\label{sec:expts}

% TODO: needs to show comparison of no. iterations required for optimizing using LBFGS vs. without gradient. Or compare performance after 20 iterations of each.

\begin{table}[h]
\small
\begin{tabularx}{\textwidth}{| p{1.8cm} | X | X | X | X | X | X | X | X |}
\hline
Dataset & Folds /subs-amples & Users & Items & Pairs, train & Pairs, test & Pref vals, test & Item features & User features \\
\hline\hline
%Simulation (a) & 25 & 25 & 100 & 900 & 0  & 100 & 2 & 2\\
Simulation a & 25 & 25 & 100 & 900 & 0 & 100 & 2 & 2 \\
Simulation b & 25 & 25 & 100 & 900 & 0 & 100 & 2 & 2\\
Simulation c & 25 & 25 & 100 & 36-2304 & 0 & 100 & 2 & 2\\
\hline
Sushi A-small & 25 & 100 & 10 & 500 & 2500 & 1000 & 18 & 123 \\
Sushi A & 25 & 1000 & 10 & 15000 & 5000 & 10000 & 18 & 123 \\
Sushi B & 25 & 5000 & 100 & 50000 & 5000 & 500000 &  18 & 123 \\
\hline
UKPConvArg-CrowdSample & 32 & 1442 & 1052 & 16398 & 529 & 33 & 32310 & 0
\\ \hline
\end{tabularx}
\caption{Summary of datasets showing mean counts per subsample or per fold. For the simulation datasets, we generate the subsamples of data independently, for Sushi we select subsamples independently from the dataset.  
UKPConvArgCrowdSample is divided into folds, where the test data in each fold corresponds to a single topic and stance. The numbers of features are given after categorical labels have been converted to one-hot encoding, counting
each category as a separate feature.
}
\label{tab:datasets}
\end{table}
% NOTE: possible problem with convincingness data is that some users have very few observations in training data.
We use the datasets summarized in Table \ref{tab:datasets} to test the key aspects of our proposed methods: recovering an underlying consensus from noisy pairwise labels; modeling personal preferences from pairwise labels; and the scalability of our proposed Bayesian preference learning methods, GPPL and crowdGPPL using SVI.
In Section \ref{sec:exp_synth}, we use simulated data to test the robustness of our method to noise, small training sets and unknown numbers of latent components.
% from noisy data when the correct number of latent factors is unknown. 
Section \ref{sec:exp_scale} evaluates the ability of our approach to
recover personal and consensus utilities on an NLP task with high-dimensional feature vectors.
GPPL previously established the best performance on this dataset in \citet{simpson2018finding}.
%, which 
%The task  involves using pairwise judgments of arguments from online debate forums
%to learn a function of argument \emph{convincingness}. We use the \emph{UKPConvArgSample} dataset,
%sampled from data provided by \citet{habernal2016argument} dataset.
Using the same dataset, we then analyze the scalability of our SVI approach. 
In Section \ref{sec:sushi}, 
we compare our method against previous approaches for predicting the 
personal preferences of thousands of users on the \emph{Sushi} datasets~\citep{kamishima2003nantonac}.
Finally, Section \ref{sec:components} evaluates whether the Bayesian approach was able to ignore redundant
components when $C$ is set larger than necessary.

%\subsection{Methods Compared}
%note that these are compared in sushi and synth tests only. Should be mentioned inside those?

% Ranking-SVM baseline -- only easy to compare in the single user case. My GPPL paper perhaps needs
% this adding in any follow up works.

% Houlsby tests: with/without user features (without is better with few users). + a hierarchical 
% model, BI (multi task preference learning, Birlitiu et al), and the GPPL-joint model. None of 
% these are done at scale, which we can do with our inference method --> *this is a new claim i.e. 
% new empirical results*. They also test a per-user model.

\subsection{Simulated Noisy Data}\label{sec:exp_synth}

First, we test how well crowdGPPL is able to recover an underlying consensus function
from pairwise labels with varying amounts of noise.
We compare crowdGPPL against \emph{GPPL} trained on all users' preference labels to learn a single  function, %and a Gaussian process over the joint feature space of users and items 
%(\emph{joint-GPPL}), as proposed by \citet{guo2010gaussian}.
%For datasets up to 100 users (simulated data, subsamples of the real datasets), 
and separate GPPL instances per user with no collaborative
learning (\emph{GPPL-per-user}). The consensus for GPPL-per-user is the mean of all users' predicted utilities. 
For crowdGPPL, we set $C=25$
and for all models, we set $\alpha_0 = 1$, $\beta_0 = 100$,
and use Mat\'ern 3/2 kernels with length-scales chosen by a median heuristic:
\begin{flalign}
 l_{d,MH} = D \mathrm{median}( \{ |x_{i,d} - x_{j,d}| \forall i=1,..,N, \forall j=1,...,N\} ).
\end{flalign}
%The motivation is that the median will normalize the feature, so that features
%are equally weighted regardless of their scaling. By using a median to perform this 
%normalization, 
This is a computationally frugal heuristic for choosing the length-scales, 
which normalizes features and prevents the average covariance between items or users from shrinking as the number of features grows.
% This heuristic has been shown to work reasonably well for the task of 
% comparing distributions~\citep{gretton2012optimal}, but has %is a simple heursitic with
%  no guarantees of optimality. 
The SVI hyperparameters were set to 
 $r=0.9$, $|P_i|=1000$ and $\epsilon=1$.

For the first test,
we generate data by creating a $20\times 20$ grid of points and choosing $400$ pairs of these points at random,
which are split into  50\% training and test sets.
For each point, we generate pairwise labels by drawing from crowdGPPL
with $20$ users, $5$ latent components, and 
$s=0.001$.
We vary the precision of the consensus function, $\sigma$, to control the noise in 
the consensus function. 
%, and
%train our models on all pairs involving only points in the training set.
%We then predict the consensus function
% and personal utilities for all users 
%for the points in the test set.
%We repeat this process, varying the value of $s$ in the generation step, 
%which controls the precision of a latent preference function: as $s$ is increased, the latent
%function values have a smaller amplitude and the pairwise labels become noisier.
The complete experiment was repeated $25$ times, including generating new data 
for each value of $\sigma$.
%The results of the first test in Figure \ref{fig:simA} show that increasing
%the noise rate in the pairwise labels causes a near-linear decrease in the
%rank correlation between the predicted and true preference function values. 
%Nonetheless, GPPL is able to recover the ranking of points with $\tau > 0.5$ when
%more than $1/3$ pairwise labels are incorrect.
%we recover a consensus function
%from preference labels produced by multiple simulated users with varying differences
%in their individual preferences. We use the same process as the first simulation, except
%we now draw the pairwise labels from a crowdGPPL model  latent factors,
%instead of a single-user model. 
%We use three methods to recover the consensus function, GPPL-per-user, pooled-%GPPL, 
%and crowdGPPL with $C=25$ so that there is one factor per user. 
Figure \ref{fig:simB} shows that the crowdGPPL model is better able to recover the latent consensus
function than the other methods, even when noise levels are high. 
GPPL's predictions may be worsened by biased users whose preferences deviate
 consistently from the consensus. GPPL-per-user relies on separate instances of GPPL, so 
 does not benefit from sharing information between users when training.

The second simulation modifies the previous setup by fixing $\sigma = 10$ and varying $s$
to evaluate the methods'
ability to to recover the personal preferences of simulated users.
Results in Figure \ref{fig:simC} show that crowdGPPL is able to make better 
predictions when noise is below $0.3$ but its benefit disappears when 
the noise level increases further. 

\begin{figure}[t]
%\subfloat[Inferring preferences for a single user]{
%\label{fig:simA}
%\includegraphics[width=.35\columnwidth]{../../results/synth_3/single_user/tau_test}
%}
\subfloat[Consensus]{
\label{fig:simB}
\includegraphics[width=.322\columnwidth,clip=true,trim=11 0 13 0]{tau_test}
}
\subfloat[Personal preferences]{
\label{fig:simC}
\includegraphics[width=.316\columnwidth,clip=true,trim=20 0 13 0]{tau_test_personal}
}
\subfloat[Latent factors]{
\label{fig:simD}
\includegraphics[width=.325\columnwidth,clip=true,trim=11 0 12 0]{num_pairs_r}
}
\caption{Simulations: Rank correlation between true and inferred preference values for different inference tasks.  (a) \& (b) varying level of noise in pairwise training labels, (c) varying number of pairwise training labels. 
}
\end{figure}
%In the final simulation, %we evaluate the effect of the quantity of training data 
%with different numbers of latent factors in the generating model.
We hypothesize that learning a model with a larger number of latent components
requires more training data.
%scenario that would require more training data. 
In the third simulation, we generate data using the same setup as before, but fix $s=0.2$ and $\sigma=1$
and vary the number of pairwise training labels 
and the number of true components through
$C_{true} \in \{ 1, 3, 10, 20\}$.
To evaluate the correlation between inferred and true user components, 
%we match inferred factors to true factors, 
we compute Pearson correlations between each
true component and each inferred component, then repeatedly select pairs of components with the highest correlations
 until every true component is matched to an inferred component. 
In Figure \ref{fig:simD} we plot the mean of the correlations between matched pairs of components.
For all values of $C_{true}$, increasing the
number of training labels beyond $1000$ leads to only minor increases in correlation at best. 
Performance is highest when $C_{true} = 20$,
possibly because the predictive model has $C = 25$
and hence is a closer match to the generating model.
However, for all values of $C_{true}$, performance with $>500$ labels remains above 0.5,
 showing the model is reasonably robust
to mismatches between $C$ and $C_{true}$.
% Does this pose a question: does the mismatch affect the performance? If the correlations
% decrease, it may mean that the model is decomposing a factor into a sum of multiple factors,
% or it may just be unable to learn it. This would need a new experiment: for 700 training pairs,
% how does the accuracy of personalised predictions vary with the number of latent factors?
% I think this needs us to vary C and keep C_true=3, otherwise we don't know whether the 
% performance differences are due to the mismatched no. factors or due to different underlying dataset, i.e. we need to keep the data the same to compare!

\subsection{Argument Convincingness}\label{sec:exp_scale}

% TODO: split the personalised results by no. training examples per worker
% and by model confidence. Does filtering predictions with low confidence estimates
% help? Do workers with more data get better predictions?

\begin{table}
\small
\begin{tabularx}{\columnwidth}{ | l | X | X | X | X | X | X | X | X | X |}
\hline
 & \multicolumn{3}{l|}{Consensus}&\multicolumn{3}{l|}{Personal} &\multicolumn{3}{p{3.1cm}|}{Personal, workers with $>$40 pairs in training set} \\ \hline
 Method & Acc & CEE & $\tau$ & Acc & CEE & $\tau$ & Acc & CEE & $\tau$ \\ \hline
 %SVM & .70 & .58 & .31 & .63 & .66 & .31 \\
 %Bi-LSTM & .73 &  .55 & .21 & .64 & .64 & .21 \\
 GPPL %medi.
  & .77 & .50 & .40 & .71 & .56 & .32 & .72 & .55 & .26 \\ % .71 & .56 & .32 \\ % now showing with > 40 training examples
 %GPPL opt. & .76 & .51 & .47 & .70 & .58 &  .30 \\
 crowdGPPL %medi. 
 & .78 & .48 & .51 & .73 & .61 & .33 & .74 & .59 & .29  %.71 & .59 & .32 \\ % now showing workers with > 40 training examples
 %crowdGPPL opt. & .78 & .48 & .50 & .69 & .59 & .29
 %PL+ SVR & .75 & .55 & \textbf{.40} & .75 & & .40 \\
 %GPC & .73 & .53 & - & .68 & .59 & - \\
 \\\hline
\end{tabularx}
\caption{Performance comparison on UKPConvArgCrowdSample using ling+GloVe features. \emph{Acc} and \emph{CEE} show classification accuracy and cross entropy error (or log-loss) for pairwise predictions, 
while $\tau$ is Kendall's $\tau$ for the predicted preference function. }
% wilcoxon signed-rank test: crowdGPPL vs. GPPL --> medi. p = 
\label{tab:convarg}
\end{table}
We evaluate consensus learning, personal preference learning and scalability
on an NLP task, namely identifying convincing arguments. 
The dataset, \emph{UKPConvArgCrowdSample}, was subsampled by \citet{simpson2018finding}
from the crowdsourced data provided by \citet{habernal2016argument}, and
contains arguments written by users
of online debating forums,
along with crowdsourced judgments of pairs of arguments
 indicating the most convincing argument.
%The task is to quantify how convincing each argument is
%by learning a model from pairwise preference labels obtained from crowdworkers
%on Amazon Mechanical Turk. 
Each argument is represented by $32,310$ numerical features and the
dataset is divided into $32$ folds ($16$ topics, each of which has two opposing stances). For each fold, we train on $31$ folds and test on the remaining fold.
We extend
the task described in \citet{simpson2018finding} to predict not just the consensus,
but also the personal preferences of individual crowd workers.
%thereby performing a cross-topic evaluation.
%test the ability of the preference learning methods to predict the consensus
 %by training on raw crowdsourced pairwise labels
%for $31$ topics, and testing against the gold pairwise labels and rankings for the
%remaining topic. This process is repeated for all $32$ topics.
GPPL was previously shown to outperform SVM, Bi-LSTM and 
Gaussian process classifier methods at consensus prediction for \emph{UKPConvArgCrowdSample}~\citep{simpson2018finding}. 
%We compare GPPL with crowdGPPL and also test each method's 
%ability to predict the raw crowdsourced labels, i.e. the individual preference labels
%supplied by each worker.
We hypothesize that a worker's view of convincingness 
depends on their prior beliefs and understanding of the subject 
discussed, and that crowdGPPL may therefore
%If this is the case, then provided that the data is sufficiently informative,
predict unseen 
pairwise labels or rankings for individual workers or the consensus more accurately than GPPL,
by accounting 
for the biases of individual workers.

Table \ref{tab:convarg} shows performance for GPPL and crowdGPPL. 
%with the median
%heuristic (\emph{medi.}) and with gradient-based optimization (\emph{opt.}).
The hyperparameters were kept the same as in Section \ref{sec:exp_synth} 
except for: GPPL uses $\alpha_0 = 2$, $\beta_0 = 200$ and
crowdGPPL uses $\alpha_0=2$, $\beta_0=2000$, set by comparing
training set accuracy against $\alpha_0=2, \beta_0 = 20000$ and $\alpha_0 = 2, \beta_0 = 2$;
$C=50$ and $\epsilon=2$, which were not optimized.
CrowdGPPL outperforms GPPL at predicting the consensus pairwise labels
shown by classification accuracy (\emph{acc}) and cross entropy error (\emph{CEE}),
and the consensus ranking (significant with $p<0.05$, Wilcoxon signed-rank test), 
shown by Kendall's $\tau$ rank correlation.
For the personal preference predictions, crowdGPPL also outperforms 
GPPL at ranking pairwise label accuracy, suggesting that there is a benefit
to modeling individual workers when predicting the consensus. 
The benefits of crowdGPPL on this task may be restricted
because the pairwise comparisons in the test folds are noisy and contain
numerous contradictions~\citep{habernal2016argument}.
For workers with more training data, the more complex crowdGPPL model is able to improve further over GPPL.
Since the CEE scores of crowdGPPL are lower than those of GPPL, while accuracy is higher, 
crowdGPPPL may be slightly under-confident.
%Length-scale optimization improves performance over the median heuristic,
%although the difference is small and required approximately $5$ times longer to run.
% PERFORMANCE ----------------------------------------------------
% TODO: exclude tau from the personal column -- not enough data to compute the gold standard correctly, only pairwise labels are real gold!
% Things we could do:
% forget about trying to predict the crowd consensus, focus on scalability
% This means we don't need to show performance improvements against 
% Houlsby or for crowdGPPL for the consensus prediction.
% however, if we want to show that the optimization procedure has improved performance, we should find out why the consensus results for crowdGPPL are so bad. 
% It may be a data matchup error.
% Change to a different dataset instead of convincingness, or make sure to exclude workers with < 10 labels.
% For sushi, we can test on unseen users. This wasn't done by Khan or Houlsby. I guess
% that performance will be close to the pooled model, but it may help. This would
% demonstrate a benefit of the GP method.
%TODO check why the results now show no difference between the methods --
% the performance appears to be more extreme, i.e. either much worse or much
% better with crowdGPPL. Folds where the transfer between domains works well -->
%crowdGPPL will work well. So how can we show this? Is the confidence higher
% on the better performing folds? Consider showing performance on more confident points only.
%TODO check whether evaluating accuracy on all pairs would be better than doing it
% per user -- currently the long tail of workers with few labels are getting too much
% weight. Problem would be if the workers with more labels on one fold have fewer
% on the training folds...
% TODO comment on opt versus non opt -- significant? 
% TODO explain that tau is computed per person then averaged. 
% TODO exclude the personalised metrics for conv if they don't work. But why don't they if the consensus is good?

We examine the scalability of our SVI implementation by evaluating GPPL and crowdGPPL with
different numbers of inducing points, $M$. Here, we fix $C=5$ and keep other model hyperparameters 
and experimental setup the same as for Table \ref{tab:convarg}. 
Figure \ref{fig:M} shows the trade-off between
runtime and accuracy as an effect of choosing $M$. Accuracy peaks
using $M=200$ while the runtime continues to increase rapidly in a polynomial fashion.
Since there are $33,210$ features, the runtime includes large overheads due to the computation of
the covariance matrices, which is linear in the number of features. 
%The plots show that the SVI method provides a substantial cut
%in runtimes while maintaining good prediction accuracy.

Figures \ref{fig:Ntr} and \ref{fig:Npairs} show runtimes as a
function of the number of items in the training set, $N_{tr}$,
and the number of pairwise training labels, $P$, respectively (all other settings remain as in Figure \ref{fig:M}).
The runtime increase with $N_{tr}$ for GPPL is almost imperceptible,
while for crowdGPPL it increases almost linearly as more
computations over the set of items are required than for GPPL, and the method takes more iterations to converge with more items. 
However, many of the additional computations can be parallelized
in future implementations.
Increasing the number of pairwise labels, $P$, above 1000, however, does not visibly affect the runtimes.
%The methods labeled "no SVI" show runtimes
%for GPPL and crowdGPPL with variational inference but no stochastic updates 
%or inducing points. 
%When using SVI, runtimes increase very little 
%with $N_{tr}$ or $P$. 
%These are compared with Bi-LSTM and SVM classifiers trained to 
%output probabilities of pairwise labels. 
%The plots clearly show the rapid increases
%in runtimes for these alternative methods. 
%For GPPL and crowdGPPL, 
%the cost of kernel computations becomes visible only with $33,210$ features, 
%indicating the benefits of more compact representations. BiLSTM appears unaffected by
%additional input dimensions, while the SVM runtimes increase noticably from $30$ to $300$ and $3000$ features.
% List of experiments to include -- need new plots for the crowd model:
% \begin{enumerate}
% \item Performance, computation time vs. no. inducing points
% \item Computation time vs. dataset size, no. features
% %\item not done: memory vs no. inducing points, update size
% %\item not done: Performance, computation time, vs update size
% %%\item not done: performance, computation time vs different initialisation methods for the inducing points; include different initialisations of K-means
% \end{enumerate}
% SCALABILITY ----------------------------------------------------------
\begin{figure}
\captionsetup{justification=centering}
\subfloat[Accuracy and runtime with varying $M$]{
\label{fig:M}
 \includegraphics[clip=true,trim=10 0 0 0,width=.38\columnwidth]{num_inducing_32310_features}
}
\subfloat[Varying number of items, $N$.]{
\label{fig:Ntr}
\includegraphics[clip=true,trim=10 0 10 0,width=.32\columnwidth]{num_arguments}
}
\subfloat[Varying no. pairwise training labels, $P$. ]{
\label{fig:Npairs}
\includegraphics[clip=true,trim=50 0 10 0,width=.27\columnwidth]{num_pairs}
}
\caption{
    Runtimes for training+prediction on UKPConvArgCrowdSample. 300 GloVe features. Means over 15 runs. CrowdGPPL with 5 components.
}
\end{figure}

\subsection{Sushi Preferences}\label{sec:sushi}

%TODO sushi-A small needs describing or cutting out.
We use Sushi preference data (shown in Table \ref{tab:datasets}),
to benchmark the classification and ranking performance of GPPL and crowdGPPL, 
against previous approaches 
and investigate the use of user features for predicting preferences.
The datasets contain, for each user, a gold standard preference ranking 
of $10$ types of sushi,
from which we generate gold-standard pairwise labels. 
%These labels can be considered noise-free, since
%they are derived directly from the gold standard ranking.  
To test performance with very few training pairs, we obtain \emph{Sushi-A-small}
by selecting $100$ users at random from the complete \emph{Sushi-A} dataset,
then selecting $5$ pairs for training and $25$ for testing per user.
For \emph{Sushi-A}, we select a subset of $100$ users at random from the complete dataset, then 
split the data into training and test sets by randomly
selecting $20$ pairs for each user for training and $25$ for testing. 
For \emph{Sushi-B}, we use all $5000$ workers, and subsample $10$ training pairs and $1$ test pair per user.
%These subsampling steps are repeated $25$ times.
Beside GPPL, crowdGPPL and GPPL-per-user, we 
% but this could not be applied to larger datasets as the 
%computation costs were too high.
introduce four further baselines: 
\emph{crowdGPPL$\mathbf{\backslash \bs u}$}, which ignores the user features;
 \emph{crowdGPPL$\mathbf{\backslash \bs u \backslash \bs x}$}, which ignores both user and item features;
 \emph{crowdGPPL$\mathbf{\backslash \bs u \backslash \bs t}$}, which 
excludes the consensus function $\bs t$ from the model as well as the user
features;
and  \emph{crowdGPPL$\backslash$induc}, which uses all features but does 
not use inducing points for a sparse approximation.
For $\backslash\bs u$ methods, the user covariance matrix, $\bs K_w$, in the crowdGPPL model is replaced by the identity matrix, and for crowdGPPL$\mathbf{\backslash \bs u \backslash \bs x}$, the item covariance matrices, $\bs K_v$ and $\bs K_t$ are also replaced by the identity matrix.
%We evaluate the quality of pairwise predictions using classification accuracy and cross entropy 
%error (also known as log loss) to gauge the quality of the probabilities that each method outputs.
%We also evaluate each method's inferred personal preference values
%against the gold standard rankings by computing Kendall's $\tau$ rank correlation
%coefficient between the negative rank and the inferred preference function values,
%for all the items contained in each subsampled user's ranking.
We set hyperparameters $C=20$ without optimization,
$\alpha_0=1, \beta_0=100$ using a grid search over values 
$10^{\{-1,...,3\}}$ on withheld user data from the \emph{Sushi-A} dataset,
$\epsilon=5$, $|P_i|=200$ for \emph{Sushi-A} and $|P_i|=2000$ for \emph{Sushi-B}. All other hyperparameters are the same 
as for Section \ref{sec:exp_synth}.
The complete process of subsampling, training and testing, was repeated $25$ times
for each dataset.

% Houlsby results: for comparison against a different inference technique on small data, include
% the test error results from their paper.
% Guo results: not directly comparable. We will rerun a similar approach but with our inference method.
% Khan results: for a different model that separates the latent features from the item/user features.
% Abbasnejad results (community-based preference learning): sushi data with 10 items; 60/40 train/test split of each user's preference pairs. This means the result is based on 27 pairs training.
% don't worry about this though, it doesn't seem to work very well in their results.

% Number of users affects value of including user features.
% Plot results on increasing dataset size.

% [10] T. Kamishima. Nantonac collaborative filtering:
% Recommendation based on order responses. In
% ACM SIGKDD 9th Int. Conf. Knowledge Discov-
% ery and Data Mining, 2003.

% Run 25 repeats of random train/test splits with:
% Exclude these two as we focus on bigger datasets in this paper. Cutting these creates the space 
% for including three metrics.
% Downside: cannot test whether increasing no. users imroves performance through better collaborative learning. 
% 
% 100 (a la Houlsby 20 pairs per user), 
% 200 (a la Khan, 3 training, 1 test, P=600, P_test=200), 
% For small datasets, we may want to test on the argumentation data so we can assess the crowd
% consensus results with small data. That dataset is more interesting here because lots of items,
% small data is a new scenario.
% % % % [a] 1000 (a la Houlsby 15 training, 5 test pairs per user, $P=15000,P_{test}=5000$), Sushi-A (10 items), 
% % % % and [b] 5000 users (a la Khan, 10 training, 1 test pairs per user, $P=50000, P_{test}=5000$), \emph{Sushi-B} (100 items).
% % % % Evaluate on parwise labelling error, pairwise label logloss, spearman rank correlation,
% % % % and runtime.
% could use normalized mean loss from guo, but not sure where the utilities come from -- ranking?

% 25 random train/test splits on all 5000 users with varying no. (1, 5, 10, 20, 40) pairs per user.

% Can put in the Houlsby (100/1000 users, 20 pairs each, labelling error) and Khan (200 users, 3 pairs each, logloss) results.
% Khan also provide code, so could be rerun to get classification error.

\begin{table}
\small
\begin{tabularx}{\textwidth}{| p{1.1cm} | X | X | X | p{0.55cm} | X | X | X | p{0.7cm} | X | X | X | p{0.7cm} |}
\hline
& \multicolumn{4}{c|}{\textbf{Sushi-A-small}} & \multicolumn{4}{c|}{\textbf{Sushi-A}} & \multicolumn{4}{c|}{\textbf{Sushi-B}} \\ 
Method & Acc & CE E & $\tau$ & Run-time & Acc & CE E & $\tau$ & Run-time & Acc & CE E & $\tau$ & Run-time \\
\hline\hline
%\multicolumn{13}{|l|}{\textit{crowdGPPL}} \\
\multicolumn{1}{|b{1.1cm}|}{crowd-GPPL} & .67 & .63 & .39 & 38%76.91 %(27.23) %& .63 (.02) & .69 (.00) & .25 (.02)
& .79 & .51 & .66 & 60 %149.6 %(20.86)
& \textbf{.79} & \textbf{.57} & .50 & 393 \\
%old results: & .69 & 2.23 & .45 & 9163 \\
%full, opt. & & & & & .80 & .48 & .68 & 3718 
%& - & - & - & - \\%(2193.72) \\
$\backslash $induc & .70 & \textbf{.57} & .46 & 51 %(14.81) %& .63 (.02) & .68 (.00) & .24 (.03)
& .84 & \textbf{.33} & .79 & 646 %(50.17) 
& - & - & - & -
\\
%$\backslash t$ & .67 & .80 & .39 & 7.45 (29.00) %& .61 (.02) & .69 (.00) & .19 (.03)
%& .79 & .57 & .65 & 158.51 (25.06)
%\\
$\backslash u$ & .70 & .58 & .46 & 9 %(24.62) %& .65 (.02) & .64 (.03) & .29 (.03)
& .84 & \textbf{.33} & \textbf{.80} & 95 %(4.93)
& .78 &\textbf{.57} & .49 & 303 \\
%old results & \textbf{.76} & \textbf{.47} & \textbf{.60} & 21052\\
%$\backslash u$, opt. & & & & & \textbf{.85} & \textbf{.33} &  \textbf{.80} & 5016 %(1978.82) \\
%& - & - & - & - \\
$\backslash u \backslash x$ & \textbf{.71} & \textbf{.57} & \textbf{.49} & 5 %(3.74) %& .65 (.02) & .63 (.02) & .29 (.03)
& \textbf{.85} & \textbf{.33} & \textbf{.80} & 89 %(12.12)
& .77 & .58 & .48 & 269
%old results & \textbf{.76} & .48 & \textbf{.60} & 15478
\\
%$\backslash u$, FITC & .70 & .58 & .46 & 333.68 (57.36) %& .65 (.01) & .64 (.02) & .29 (.02)
%& \textbf{.85} & \textbf{.33} & \textbf{.80} & 551.56 (16.82)
%\\
$\backslash u,\backslash t$ % this is also FITC to emulate Houlsby -- mention it in the text, not here
& .68 & .60 & .43 & 291 %(48.62) %& .50 (.00) & .69 (.00) & nan (nan)
& .84 & \textbf{.33} & .79 & 204 %(12.15) 
& .76 & .61 & .54 & 1001
% old results & .75 & .49 & .59 & 30484
\\ 
%$\backslash u$$\backslash t$, FITC, opt. & & & &  & .84 & \textbf{.33} & \textbf{.80} & 7413.53 (2849.63) \\
\hline 
GPPL & .65 & .62 & .31 & \textbf{2} %(.26) %& .65 (.02) & .63 (.02) & .30 (.03)
& .66 & .62 & .32 & \textbf{6} %(.38) & 
& .65 & .62 & .31 & \textbf{51}
\\
GPPL-per-user & .67 & .64 & .42 & 183 %(1.12) %& .50 (.00) & .69 (.00) & nan (nan)
& .83 & .40 & .79 & 1106 & .75 & .60 & \textbf{.60} & 3172 %(.44)
\\
\hline
%\citet{houlsby2012collaborative} CP & & & & & .84 \\
%\citet{houlsby2012collaborative} CPU & & & & & .83 \\ 
%\citet{khan2014scalable} & &&& & & & & & .69 & &
%\\ \hline
\end{tabularx}
\caption{Predicting personal preferences: performance on \emph{Sushi-A} dataset and \emph{Sushi-B} datasets.
Runtimes given in seconds, with standard deviation between repeats in brackets.
For accuracy, all standard deviations are $\leq 0.02$, for CEE $\leq 0.08$, for Kend. $\leq 0.03$.
 }
\label{tab:sushi}
\end{table}
%  -- the stds were removed because they're not very informative for acc, CEE, tau
% The commented results are predictions on new users -- for sushiAsmall, the only interesting 
% point is that excluding the consensus decreases accuracy. How does it make predictions without a t?
The results in Table \ref{tab:sushi} 
%we re-state the previous performance metrics and did not re-implement these methods. 
illustrate the performance benefit of crowd models over single-user GPPL, and
the runtimes 
show the speedup of the sparse approximation of crowdGPPL over crowdGPPL$\backslash$induc.
GPPL is substantially quicker to train than crowd-GPPL, and GPPL-per-user does not scale well to larger numbers
of users.
% including feature data for items and users leads to faster convergence than crowdGPPL$\backslash \bs u$ and crowd$\backslash \bs u \backslash \bs x$.
%Likewise, the use of matrix factorization leads to only a small increase in
% runtimes of these methods over joint-GPPL. 
 %However, the runtimes for crowdGPPL are higher than those of GPPL.
 When using inducing points, the user features decrease the performance of crowdGPPL on \emph{Sushi-A} and \emph{Sushi-A-small}: crowdGPPL$\backslash$induc
 and crowdGPPL$\backslash\bs u$ both outperform the full crowdGPPL. 
 To use inducing points,
 there must be a strong relationship between neighbouring points, which
 may not to be the case given the features in this dataset.
 However, on \emph{Sushi-B}, the user features provide a small improvement, so may be weakly informative
 given enough users when we have only $10$ pairwise labels per user, rather than $15$ as for \emph{Sushi-A}.
 Comparing crowdGPPL$\backslash\bs u$ with crowdGPPL$\backslash\bs u\bs t$, including the consensus function appears to improve performance by a modest amount.
 GPPL-per-user performs well on \emph{Sushi-A}, but does not perform as well as other methods on \emph{Sushi-B}, as
there are only $10$ pairwise training labels per user and no collaborative learning.
 %Again, we see small improvements in performance after length-scale optimization,
 %but this takes approximately 22 times as long to train.
% as are the performance metrics. 

CrowdGPPL produces similar classification scores to the earlier method of 
 \citet{houlsby2012collaborative} (0.83 on \emph{Sushi-A} with user features, and 0.84 without), 
 while using a more scalable inference method.
Performance is also improved over \citet{khan2014scalable} , 
whose model comprised a GP for each user and matrix factorization (CEE=0.69 on \emph{Sushi-B}),
 as well as \citet{salimans2012collaborative}, using matrix factorization with no user or item features (Kendall's $\tau\approx0.39$ on \emph{Sushi-B}). 

%Figure \ref{fig:latent_factor_variance} again shows that the inferred crowdGPPL model relies
%heavily on a subset of the latent factors.
% We should bring the scalability experiments forward so that we don't need the runtimes here?
% Or so we can at least avoid comparing with Houlsby and Khan on runtimes. 
% The Khan runtime should be GPPL-per-user + BMF. However, the interconnection of the two might 
% make them take more or less time?

\subsection{Posterior Variance of Item Components}
\label{sec:components}

We investigate how many latent components were actively used by 
 crowdGPPL on the \emph{UKPConvArgCrowdSample} and \emph{Sushi-A} datasets 
 using the median heuristic.
Figure \ref{fig:latent_factor_variance}
plots the posterior expectations of the inferred scales, $1/s_c$, for the latent item 
 components. 
 The plots show
that many factors have a very small variance and therefore do not contribute strongly 
to many of the model's predictions. This indicates that our Bayesian approach, in which the priors
of the latent factors have mean zero, has inferred a simpler model than the
number of latent components would permit.
\begin{figure}
\centering
\subfloat[\emph{UKPConvArgCrowdSample}]{
\includegraphics[trim=10 10 0 0,clip=true,width=.34\textwidth]{conv_factor_scales}
}
\subfloat[\emph{Sushi-A}]{
\includegraphics[trim=20 5 0 0,clip=true,width=.32\textwidth]{sushi_factor_scales}
}
\caption{
Distribution of latent factor variances, $1/s_c$, for crowdGPPL on UKPConvArgCrowdSample, \emph{Sushi-A} and Sushi-B, averaged over all runs.
}
\label{fig:latent_factor_variance}
\end{figure}
