\section{Background and Related Work}
\label{sec:rw}

% Don't think the references really show this, or should necessarily be added: Pairwise comparisons have low cognitive load:
% doesn't show this -- [1] Urszula Chajewska, Daphne Koller, and Ronald Parr. Making rational decisions using adaptive
% utility elicitation. In Proceedings of the Seventeenth National Conference on Artificial Intel-
% ligence and Twelfth Conference on Innovative Applications of Artificial Intelligence, pages
% 363–369. AAAI Press / The MIT Press, 2000.
% [2] Vincent Conitzer. Eliciting single-peaked preferences using comparison queries. Journal of
% Artificial Intelligence Research, 35:161–191, 2009. --> provides algorithms for aggregating rankings but does not consider uncertainty,
%user features...

% Done -- citation is in the intro. work from Fürnkranz should be cited (he may be a reviewer).
% \cite{hullermeier2008label} -- cite this in the intro. Preference learning through
% pairwise comparison but used to rank labels in a multiclass situation, rather than ranking items.
% \cite{furnkranz2009binary},\cite{furnkranz2010preference} -- a simple function for combining preferences generated by comparing pairs of labels assigned to items. E.g. if I can give movies scores of 1 to 5, this method combines the scores of pairs of items.
% For object ranking approaches, this idea has first been formalized by Tesauro [58] under the name comparison training.
% He proposed a symmetric neural-network architecture that can be trained with representations of two states and a training
% signal that indicates which of the two states is preferable. The elegance of this approach comes from the property that
% one can replace the two symmetric components of the network with a single network, which can subsequently provide a
% real-valued evaluation of single states.

% TODO: [18] T. Salimans, U. Paquet, and T. Graepel. Collabo-
% rative learning of preference rankings. In Proceed-
% ings of the 6th ACM Conference on Recommender
% Systems, 2012.

% Note that Khan's method does not need factorization assumptions in the approximate posterior.
% Instead, they have no prior over v (item features).
% They have a diagonal covariance for the user features -- cheap. 
% They need a separate GP per user but it seems like this is not a problem in practice -- I guess
% the method scales linearly with no. users. In our case, we model covariance between users, so
% scaling is poor unless you can use inducing points or diagonal covariance.

% Preference learning with multiple users
\subsection{Pairwise Preference Learning} % Learning a Consensus by ...

% TODO: do we really care about multiple rankings in this paragraph?
%Given a set of items and a set of parwise labels, the goal of preference learning is to 
%either infer a 
To obtain a ranking over items given
a set of pairwise labels, many preference learning methods model
the user's choices as a random function of the latent utility of the items~\citep{thurstone1927law}.
\todo{random utility model background from GPPRL paper? Or from later in this paper.
Go back to intro and see if we can define the task more precisely and clearly. 
See GPPRL and ACL paper for suggestions on how to introduce GPPL and BWS.
Have the goals of PL been introduced? They should be in the intro.
We need to state that it is necessary to account for individual user bias - define the term clearly.
}
%Not sure what this stuff really adds, it's not really related enough?
%Recent work on this type of approach has analysed bounds on error rates ~\citep{chen2015spectral}, 
%sample complexity~\citep{shah2015estimation}, and joint models for ranking and clustering 
%items given a set of pairwise comparisons~\citep{li2018simultaneous}.

%METHODS THAT PRODUCE A SINGLE RANKING
...In contrast, Gaussian processes preference learning (\emph{GPPL})
uses item features to make predictions for unseen items and
share information between similar items~\citep{chu2005preference}.
GPPL was used by \citet{simpson2018finding} to aggregate crowdsourced pairwise labels,
but assumes the same level of noise for all annotators and ignores the effect of divergent opinions.
\todo{
Moreover, please note that the call for papers for ECMLPKDD journal
track explicitly states the following: "Consequently, journal versions
of previously published conference papers, or survey papers will not
be considered for the special issue." This is in contrast to "regular"
journal submissions, which can be extended versions of previous
conference papers. The earlier publication you have on this topic was
presented at ACL 2018. We would thus like you to make sure that the
revision puts an even stronger emphasises on the new results, to make
sure that the manuscript solidly goes beyond an "extended conference
paper".
}
To crowdsource sentiment annotations, 
\citet{kiritchenko2016capturing} propose to use an extension of pairwise comparison
known as \emph{best-worst scaling}, in which the annotator selects best and worst items from a set.
They apply a simple counting technique to infer a ranking over the items, which requires 
each item to have a sufficient number of comparisons.
A popular method for predicting pairwise labels of new items given their features is 
\emph{SVM-rank}~\citep{joachims2002optimizing}.
...however, these approaches assume a single utility function over items,
so are not suitable for modelling personal preferences 
and cannot account for the biases of individual users.


Pairwise labels may be provided by multiple users who have differing preferences or give labels 
that conflict with one another. 
Previous work for handling these differences falls broadly into two camps: (1)
methods that treat disagreement between users as annotation errors and attempt to infer the true utilities
and (2) personalised preference models that infer the utilities specific to each user.

(1)

\todo{
Another claim from the authors is the SVI for large-scale crowdGPPL, which is due to limitation of GP. Indeed, the online strategy has been well-studied in crowdBT,  crowdPL,  the previous works and the following work. 
Li et al,. Hybrid-MST: A Hybrid Active Sampling Strategy for Pairwise Preference Aggregation, NIPS 2018 --
doesn't consider different workers at all, uses only ML learning 
- The active learning strategy they devise based on minimum spanning trees
could also be used with crowdGPPL or crowdBT, but using a Bayesian method gives us a distribution over utilities, which means we can do BALD to estimate EIG much
quicker. 
- Their method aims at reducing the complexity of selecting pairs for active learning, it doesn't address complexity of learning a feature-based model from pairs.
}
The problem of disagreement between annotators in a crowd was addressed by \citet{chen2013pairwise}
and \citet{wang2016blind},
using Bayesian approaches that learn the individual labelling accuracy of each worker.
Recently, \citet{pan2018stagewise,han2018robust}
introduced scalable methods for learning from noisy \textit{k}-ary preferences,
which are totally--ordered subsets of $k$ items.
However, these methods treat disagreements as labelling errors 
rather than subjective disagreements, and assume that there is a single true utility for each item,
which we call the \emph{consensus}.
Therefore, they cannot reflect the different preferences of individuals in the crowd in subjective tasks
such as recommendation or natural language interpretation.
%\citet{wang2016blind} improved performance 
%by modeling the level of noise in the latent utility function for each annotator in a given domain, rather than
%in the pairwise labels. 
Such differences of opinion can be seen as labelling \emph{bias}, rather than noise,
as a labeller will consistently express preferences that are different from others,
and this difference is not random but depends directly on the items considered.
With small numbers of labels, such biases may reduce performance when estimating 
the consensus.
Furthermore, previous aggregation methods for noisy preferences
do not consider the input features of items,
hence cannot make predictions for new test items without obtaining comparisons for those items~\citep{chen2013pairwise,wang2016blind,han2018robust,pan2018stagewise,li2018hybrid}.
Instead, it would be necessary to use the outputs of the aggregation methods to train 
another model for prediction. 
%Including models such as Gaussian process to account for input features brings scalability challenges that are not addressed by these existing methods. --> think this is in the wrong place. First we need to motivate using GPs or nonlinear models in general. Then we can state that this brings scalability problems. In the SVI section, first mention online learning with crowdBT, Han, Pan, but then state that introducing GPs brings new scalability problems. Then go on to SVI as a solution.

%predicts only pairwise labels rather than the underlying utilities.
For crowdsourced data, \citet{fu2016robust} show that performance
of SVM-rank is improved by identifying outliers in crowdsourced data
that correspond to probable errors.
\citet{uchida2017entity} extend SVM-rank to account for different levels of confidence in each pairwise annotation expressed
by the annotators.
% A number of studies consider actively selecting pairs of items for
% comparison to minimize the number of pairwise labels required~\citep{radlinski2007active,qian2015learning,maystre2017just,cai2017pairwise}.
However, besides modelling labelling noise, 
the previous work on crowdsourced preference learning does not 
account for the effect of divergence of opinion on the inferred preferences.

\subsection{Inferring Individual Preferences for Members of a Crowd}

% \citet{tian2012learning} consider crowdsourcing tasks where there may be more than one correct answer.
% They use a nonparametric Dirichlet process model to infer a variable number of clusters of answers for each task,
% and also infer annotator reliability. 
% However, they do not apply the approach to ranking using pairwise labels.
% actively selecting pairs for annotation as a multi-armed bandit problem~\citep{busa2018preference}.
%  do not study the process of learning from an oracle or user that we can query.
% Rather, we develop a model for aggregating pairwise labels from multiple sources, 
% which can be used as the basis of active learning methods that exploit the model uncertainty 
% estimates provided by this Bayesian approach.


(2) \todo{this should support a Bayesian approach}

An example is the method of ~\citet{herbrich2007trueskill}, which learns the skill of chess players from 
match outcomes by treating them as noisy pairwise labels.

As well as aggregating preferences from a crowd to identify a consensus,
we also wish to predict the preferences of individual users.
\citet{yi_inferring_2013} and \citet{kim2014latent} address this task by learning
 multiple latent rankings and inferring
the preference of each user toward those rankings, while 
\citet{salimans2012collaborative} use Bayesian matrix factorization to identify multiple
latent rankings.
A probabilistic matrix factorisation method using GPs to
was previously applied to  applied to collaborative filtering~\citep{lawrence2009non} and.
A GP is used to make predictions of a user's ratings for an item that they have not seen before, given the
item's latent features, but the $\mathcal{O}(N^3)$ time complexity of GP inference was not addressed.
None of these approaches exploit item or user 
input features to make predictions over new test items or users. % , nor remedy labelling errors or generalize .
Input features can be extracted from the content or metadata of each item or user, for example, in text 
it is common to count occurrences of each unique token or n-gram (sequence of n tokens),
or more recently, to obtain an \emph{embedding} of a word, sentence or document,
which represents its content numerically~\citep{mikolov2013distributed,devlin2018bert}.
Features can be represented as a vector of numerical values corresponding to each item or user
and can be used to predict the outputs for a test item or user based on observations of others with similar feature values.
Features can also help to remedy labelling errors when learning from noisy, crowdsourced data~\citep{felt2016semantic,simpson2015language}.
%crowdranking \citet{yi_inferring_2013} uses the crowd to make up for the fact that a target user has small data -- it's a form of collaborative filtering with pairwise labels using a non-Bayesian inference algorithm.
% Several other works learn multiple rankings from crowdsourced pairwise labels
% rather than a single gold-standard ranking, 
% but do not consider the item or user features so cannot extrapolate to new users or 
% items~\citep{yi_inferring_2013,kim2014latent}. 
% Both \citet{yi_inferring_2013} and learn a small number of
% latent ranking functions that can be combined to construct personalized preferences, 
% although neither provide a Bayesian treatment to handle data sparsity.
%include the work on collaborative GPPL
%Several extensions of BMF use Gaussian process priors over latent factors 
%to model correlations between 
%items given side information or observed item features~\citep{adams2010incorporating,zhou2012kernelized,houlsby2012collaborative,bolgar2016bayesian}. 
%However, these techniques are not directly applicable to 
%learning from pairwise comparisons 
%as they assume that the observations are Gaussian-distributed numerical ratings~\citep{shi2017survey}. 


In contrast, \citet{guo2010gaussian} propose a joint Gaussian process over the
space of users and features. Since this scales cubically
in the number of users, \citet{abbasnejad2013learning} 
propose to cluster the users into behavioural groups.
However, distinct clusters do not
allow for collaborative learning between users with partially overlapping preferences, e.g. two users may both like one genre of music, 
while having different preferences over other genres. 
\citet{khan2014scalable} instead learn a GP for each user,
and combine them with matrix factorization to perform collaborative filtering.
However, this approach does not model the relationship between
 input features and the latent factors, does not place a prior over item factors,
 and does not scale to very large sets of users.
An alternative is \emph{Collaborative GP}~\citep{houlsby2012collaborative},
which uses a latent factor model, where each latent factor has a Gaussian process prior. 
This allows the model to take advantage of the input features of
users and items when learning the latent factors. 
Each individual's preferences are then represented 
by a mixture of latent functions.
%Pairwise labels from users with common interests help to predict each other's 
%preference function, hence 
%this can be seen as a collaborative learning method, as used in recommender systems.
Using matrix factorization in combination with GP priors is therefore an effective
way to model the individual preferences of users while
making use of input features to generalize to test cases. However,
none of these approaches considers a consensus preference function, and
more scalable inference is needed for datasets containing thousands of items or users.
%other Bayesian recommender systems that deal with noisy preferences
%To combine Bayesian matrix factorization with a pairwise likelihood,
%\citet{houlsby2012collaborative} propose
%However, their proposed method does not scale sufficiently to the numbers of 
%items, users or pairwise labels found in many important application domains. 
% previous work on how to make BMF more scalable with larger datasets or Latent factor analysis etc.
% who has used GPs for BMF?

 % PCA: Gaussian noise. "The classical PCA converts a set of samples with possibly correlated variables into another   
 % set of samples with linearly uncorrelated variables via an orthogonal transformation [1]. Based on this, PCA
 % is an effective technique widely used in performing dimensionality reduction and extracting features." -- Shi et al 2017. shi2017survey
 % SVD: like PCA with the mean vector set to zeroes.
 % variations of PCA: for handling outliers or large sparse errors
 % most matrix factorizations are special cases of PCA and in practice do not consider the mean vector.
 % probabilistic PCA: latent variables are unit isotropic Gaussians --> all have 0 covariance and 1 variance.
 % Bayesian PCA: places priors on all latent variables.
 % Probabilistic factor analysis: assumes different variances on each of the latent factors.
 % Probabilistic matrix factorisation: ignores the mean. --> I.e. can be done with SVD
 % I think this means our method is a form of PFA? But extended to consider correlations in the weights.
 % NMF: as matrix factorisation but the low-rank matrices are non-negative.

% of their hybrid inference method expectation propagation and variational Bayes limits its application in many domains.
% They use FITC to provide a sparse approximation. This is still not as scalable as SVI and doesn't work as well --
% see the Hensman papers?
%with Gaussian processes to improve performance with sparse data. 
%Their model can be learned using pairwise labels, numerical ratings, or other likelihoods.

%user features nor model dependencies between item features 
%and the low-dimensional latent features, so it cannot exploit the latent features to predict preference scores for new items and relies instead on a user-specific GP.
%In contrast, our approach does not require learning a separate GP per user, but instead
%places GPs on both the latent factors. This means that the item and user
%features assist in learning the latent factors as we can exploit their similarities and
%correlations.
%relying instead on a purely individual GP with no shared information,
%(this would be a problem if the user fits the latent features exactly as the GP will end up modelling the user's individual devaiation from the common preferences modeled by the latent features) 
%To achieve scalability using a variational EM algorithm, \citet{khan2014scalable}
%sub-sample the pairwise labels meaning that 
%some training data must be discarded. In this paper, we
%applies stochastic variational inference to learn from all data 
%while limiting memory and computational requirements.
% how do we compare to them or do we get out of it? --> compare results on the same datasets
%Our approach is similar to Khan et al. 2014. "Scalable Collaborative Bayesian Preference Learning" but differs in that
%we also place priors over the weights and model correlations between different items and between different people.
%Our use of priors also encourage sparseness in the features. 
%TODOs:
% what is meant by 'factorization assumptions' exactly and do we make them? I think we do but don't fully
% understand why they're so bad. See [18,11] from Khan for examples of bad factorization. 

\subsection{Scalable Approximate Bayesian Inference}


Many of the approaches for modelling individual user preferences use matrix factorization 
to share information between users and items,
which benefits from a Bayesian treatment to reduce the effects of overfitting or noisy data~\citep{saha2015scalable}.
Recent work on scalable Bayesian matrix factorization focuses on distributing and parallelizing 
 inference but 
 these approaches do not make use of input features 
 and are not directly applicable when Gaussian processes are used to integrate input features
 ~\citep{ahn2015large,saha2015scalable,vander2017distributed,chen2018large}. 
%This paper focuses instead on reducing computational and memory costs, 
%although the method we propose is amenable to parallelization.
Models that combine Gaussian processes with non-Gaussian likelihoods 
require approximate inference methods that often scale poorly with 
the amount of training data available. 
Established methods such as the Laplace approximation 
and expectation propagation~\citep{rasmussen_gaussian_2006} have
computational complexity $\mathcal{O}(N^3)$ with $N$ data points
 and memory complexity $\mathcal{O}(N^2)$. 
For collaborative GPPL, \citet{houlsby2012collaborative}
propose a  kernel for pairwise 
preference learning and use a sparse
\emph{generalized fully independent training conditional} (GFITC) 
approximation~\citep{snelson2006sparse} to reduce the computational complexity to $\mathcal{O}(PM^2 + UM^2)$ and 
memory complexity to $\mathcal{O}(PM + UM)$,
where $P$ is the number of pairwise labels, $M \ll P$ is a fixed number of inducing points, and $U$ is the number of users.
However, this is not sufficiently scalable
 for very large numbers of items, users or pairs, as there is no clear way to parallelize it or to limit memory consumption.
%which assumes that
%the observations at each training data point are independent of one another given the 
%latent values at the inducing points.
%Since $P$ is typically larger than $N$, the number of inducing points, $M$ may also need to be increased.
The GP over pairs also makes it difficult to extract posteriors for latent function values for individual items,
and prevents mixing pairwise training labels
with observed ratings in future data aggregation settings.

\citet{li2018hybrid} provide computationally efficient active learning for pairwise labels, but
do not handle annotator disagreements nor consider input features of items.
%remember that they are not doing Bayesian inference but ML.


To handle large numbers of pairwise labels, \citet{khan2014scalable}
develop a variational EM algorithm and sub-sample pairwise data rather than learning from the complete training set.
An alternative is \emph{Stochastic variational inference (SVI)}~\citep{hoffman2013stochastic}, which updates an approximation to the posterior using 
 a different random sample of the training data at each iteration. 
 This allows the approximation to make use of all training data over a number of 
 iterations, while limiting training costs per iteration.
SVI has been successfully applied to Gaussian process regression~\citep{hensman2013gaussian} and classification~\citep{hensman2015scalable},
further improving scalability over existing sparse approximations.
For multi-output GPs, ~\citet{nguyen2014collaborative} introduce an SVI approach where 
each output is a weighted combination of shared latent functions plus a latent function specific to that output. 
They apply their method to capture dependencies between regression tasks,
treating the weights for the shared latent functions as hyperparameters. 
In this paper, we also use shared latent functions to capture dependencies between different users' preferences,
but introduce a Bayesian treatment of the weights using a GP over user features,
and integrate a non-Gaussian likelihood into the SVI framework to enable learning from pairwise labels.

 An SVI method for preference learning,
 that places a GP over items rather than pairs, was previously applied by ~\citet{simpson2018finding}.
 However, the details of the inference approach were not provided and the model does not consider
 differences between the preferences of individual users.
In this paper, we provide the first complete derivation of SVI for Gaussian process preference learning,
then extend the approach to a new multi-user model, 
incorporating the first application of SVI to Bayesian matrix factorization.

