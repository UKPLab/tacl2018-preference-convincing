\section{Related Work}
\label{sec:rw}

% Don't think the references really show this, or should necessarily be added: Pairwise comparisons have low cognitive load:
% doesn't show this -- [1] Urszula Chajewska, Daphne Koller, and Ronald Parr. Making rational decisions using adaptive
% utility elicitation. In Proceedings of the Seventeenth National Conference on Artificial Intel-
% ligence and Twelfth Conference on Innovative Applications of Artificial Intelligence, pages
% 363–369. AAAI Press / The MIT Press, 2000.
% [2] Vincent Conitzer. Eliciting single-peaked preferences using comparison queries. Journal of
% Artificial Intelligence Research, 35:161–191, 2009. --> provides algorithms for aggregating rankings but does not consider uncertainty,
%user features...

% Done -- citation is in the intro. work from Fürnkranz should be cited (he may be a reviewer).
% \cite{hullermeier2008label} -- cite this in the intro. Preference learning through
% pairwise comparison but used to rank labels in a multiclass situation, rather than ranking items.
% \cite{furnkranz2009binary},\cite{furnkranz2010preference} -- a simple function for combining preferences generated by comparing pairs of labels assigned to items. E.g. if I can give movies scores of 1 to 5, this method combines the scores of pairs of items.
% For object ranking approaches, this idea has first been formalized by Tesauro [58] under the name comparison training.
% He proposed a symmetric neural-network architecture that can be trained with representations of two states and a training
% signal that indicates which of the two states is preferable. The elegance of this approach comes from the property that
% one can replace the two symmetric components of the network with a single network, which can subsequently provide a
% real-valued evaluation of single states.

% TODO: [18] T. Salimans, U. Paquet, and T. Graepel. Collabo-
% rative learning of preference rankings. In Proceed-
% ings of the 6th ACM Conference on Recommender
% Systems, 2012.

% Note that Khan's method does not need factorization assumptions in the approximate posterior.
% Instead, they have no prior over v (item features).
% They have a diagonal covariance for the user features -- cheap. 
% They need a separate GP per user but it seems like this is not a problem in practice -- I guess
% the method scales linearly with no. users. In our case, we model covariance between users, so
% scaling is poor unless you can use inducing points or diagonal covariance.

\subsection{Aggregating Pairwise Labels from a Crowd} % Learning a Consensus by ...

% TODO: do we really care about multiple rankings in this paragraph?

To obtain a ranking from pairwise labels, many preference learning methods model
the user's choices as a random function of the latent utility of the items~\citep{thurstone1927law}.
An example is the method of ~\citet{herbrich2007trueskill}, which learns the skill of chess players from 
match outcomes by treating them as noisy pairwise labels.
Recent work on this type of approach has analyzed bounds on error rates ~\citep{chen2015spectral}, 
sample complexity~\citep{shah2015estimation}, and joint models for ranking and clustering from pairwise comparisons~\citep{li2018simultaneous}.
% When all items have a sufficient number of labels, it is possible to estimate the underlying utility using
% simple counting procedures~\citep{kiritchenko2016capturing}.
%, but do not propose methods
%for learning multiple rankings from crowds of users.
% Most approaches use Bradley-Terry or Thurstone. However some also try Mallows models ~\cite
% {busa2014preference,raman2015bayesian} to get the uncertainty over the ranking instead of over a 
% latent score.
% Other approaches use graph-based ranking measures, e.g. based on Kleinberg's HITS ~\citep{sunahase2017pairwise} or PageRank.
% extension of chenc2013 to k-ary preferences. han2018robust
% wang2016blind/Thurstonian Pairwise Preference (TPP): Chen et al. lacks the mechanism to model multiple query domains, (what does this mean? --> better at labeling certain types of items)
%thus incapable to characterize workers’ domain-dependent expertise and truthfulness. 
% CROWD BT does not take query difficulty into account either. (do they have a feature-dependent model?)
% Furthermore, unlike TPP, CROWD BT does not model the generation of rankings (it models generation of pairwise labels directly)
% Therefore, it simplifies the generation of inconsistent annotations as solely a result from worker accuracy
% (it treats differences between workers as pure noise).

The problem of disagreement between annotators in a crowd was addressed by \citet{chen2013pairwise}
\todo{
How do we make it clear that this is not the case? --
This paper extended the pairwise ranking model in Xi Chen's work, Pairwise Ranking Aggregation in a Crowdsourced Setting with Gaussian Process.
The preference learning has well-studied. Particularly, the pairwise label using BT model has been published by Xi Chen's work. Recently, the model has been extended to more general PL model, such as:
Han et al., Robust Plackett-Luce model for k-ary crowdsourced preferences. Machine Learning 107(4): 675-702 (2018);
Pan et al., Stagewise learning for noisy k-ary preferences. Machine Learning 107(8-10): 1333-1361 (2018).
[these two seem to extend the crowd-BT model, so are mainly about de-noising.]
Hybrid-MST: A Hybrid Active Sampling Strategy forPairwise Preference Aggregation --
doesn't consider different workers at all, uses only ML learning [make better case
for Bayesian?]. The active learning strategy they devise based on minimum spanning trees
could also be used with crowdGPPL or crowdBT. Using a Bayesian method gives
us a distribution over utilities, which means we can do BALD to estimate EIG much
quicker. Their method aims at reducing the complexity of selecting pairs for active learning, it doesn't address complexity of learning a feature-based model from pairs.
Use sushi-A-small to show that crowd-BT and others don't deal with small data well
if they don't use feature information.
We should also discuss the learning strategies used in these methods. They are not aimed
at GPs and matrix factorisation, unlike our model, but could they still be applied here?
If so, this should be discussed in future work.
Finally, there may be a nice dataset from one of these that we could use? 
Important distinction is that we look for subjective tasks, not just denoising the
pairwise labels. This might need to be justified better by describing tasks
such as argument mining. Consensus in a subjective task is helpful to learn the personal 
preferences of new users -- do we show this or describe it? Consensus can also be useful
if we need to extract a gold standard or objectively correct rating, but annotators
have different biases, rather than just noise levels. What about settlement rating for
planet hunters? -- turn into pairs. Some users may ignore certain types of building,
so images with those features would be biased to low ratings for those people. 
Biases like this could harm performance if there aren't enough preferences from 
different users to cancel them out.
Can we demonstrate this, or just argue it in the intro?
Yes, find three argument pairs where the crowdGPPL and ground truth agree, but the 
annotators do not; show the annotator bias according to crowdGPPL, i.e. weights for the 
latent features.  
]
Those models are more general and robust to the noisy preferences in crowdsourcing settings.
In terms of robustness of noise modeling, I do not find the proposed model can be significantly better than the previous works.  There is no experiments comparing with any baselines in crowdsourcing, even Chen's work. 
}
and \citet{wang2016blind},
using Bayesian approaches that learn the individual accuracy of each worker.
%These methods do not learn the relationship between the workers' individual preferences
%and the ground truth. 
%\citet{wang2016blind} improved performance 
%by modeling the level of noise in the latent utility function for each annotator in a given domain, rather than
%in the pairwise labels. 
However, they do not %\citet{chen2013pairwise} nor \citet{wang2016blind}
exploit item features to mitigate data sparsity.
In contrast, Gaussian processes preference learning (\emph{GPPL})
uses item features to make predictions for unseen items and
share information between similar items~\citep{chu2005preference}.
GPPL was used by \citet{simpson2018finding} to aggregate crowdsourced pairwise labels,
but assumes the same level of noise for all annotators and ignores the effect of divergent opinions.
\todo{
Moreover, please note that the call for papers for ECMLPKDD journal
track explicitly states the following: "Consequently, journal versions
of previously published conference papers, or survey papers will not
be considered for the special issue." This is in contrast to "regular"
journal submissions, which can be extended versions of previous
conference papers. The earlier publication you have on this topic was
presented at ACL 2018. We would thus like you to make sure that the
revision puts an even stronger emphasises on the new results, to make
sure that the manuscript solidly goes beyond an "extended conference
paper".
}
To crowdsource sentiment annotations, 
\citet{kiritchenko2016capturing} propose to use an extension of pairwise comparison
known as \emph{best-worst scaling}, in which the annotator selects best and worst items from a set.
They apply a simple counting technique to infer a ranking over the items, which requires 
each item to have a sufficient number of comparisons.
% account for the varying quality of pairwise labels obtained from a crowd
%by learning an individual model of agreement with the true pairwise labels for each worker.  

%  consider the case where different rankings correspond to lists of items
% provided in response to search queries. 
% While they model the dependence of annotator accuracy on the domain of a query,
% their approach was not applied to personal or subjective rankings.

%Say we want to learn a 'ground truth' preference function that may be one user's preference function.
% One set of pairwise labels may be informative for one subset of items.
% E.g. music recommendation, two users may have similar jazz preferences but all other genres are different
% E.g. learning user preferences from webpage clicks, selecting items from a list may be informative in one context and meaningless in another
A popular method for predicting pairwise labels of new items given their features is 
\emph{SVM-rank}~\citep{joachims2002optimizing}.
%predicts only pairwise labels rather than the underlying utilities.
For crowdsourced data, \citet{fu2016robust} show that performance is improved by identifying outliers in crowdsourced data
that correspond to probable errors.
\citet{uchida2017entity} extend SVM-rank to account for different levels of confidence in each pairwise annotation expressed
by the annotators.
%However, these approaches do not model divergence of opinion between annotators
%and do not provide a Bayesian solution.
%Related works have also investigated budget constraints for crowdsourcing pairwise labels~\citep{cai2017pairwise}.
% Also consider mentioning relevant work on active learning
% -- finding the most preferred item with minimal labels
% -- learning a preference function using AL
% -- learning within a budget constraint
% Strategy: look at recent works from ML/AI/DM conferences. Must consider annotators with different preferences.
% A number of studies consider actively selecting pairs of items for
% comparison to minimize the number of pairwise labels required~\citep{radlinski2007active,qian2015learning,maystre2017just,cai2017pairwise}.
However, besides modeling labeling noise, 
the previous work on crowdsourced preference learning does not 
account for the effect of divergence of opinion on the inferred preferences.
% does not 
% provide a Bayesian approach for aggregating pairwise labels from crowds
% that can make predictions for new items
% and model the divergence of opinions between annotators.

\subsection{Bayesian Methods for Inferring Individual Preferences}

% \citet{tian2012learning} consider crowdsourcing tasks where there may be more than one correct answer.
% They use a nonparametric Dirichlet process model to infer a variable number of clusters of answers for each task,
% and also infer annotator reliability. 
% However, they do not apply the approach to ranking using pairwise labels.
% actively selecting pairs for annotation as a multi-armed bandit problem~\citep{busa2018preference}.
%  do not study the process of learning from an oracle or user that we can query.
% Rather, we develop a model for aggregating pairwise labels from multiple sources, 
% which can be used as the basis of active learning methods that exploit the model uncertainty 
% estimates provided by this Bayesian approach.
%
%queries belong to 'domains'. Annotators have different accuracy on each domain. 
% we don't do that because we assume each annotator has their own ranking and so different noise levels are introduced through the personalized preference function having larger values where the annotator is confident. 
\todo{ Section 2.2: Define or give an example of 'input features' when introduced.  }
As well as aggregating preferences from a crowd to identify a consensus,
we also wish to predict the preferences of individual users.
\citet{yi_inferring_2013} and \citet{kim2014latent} address this task by learning
 multiple latent rankings and inferring
the preference of each user toward those rankings, while 
\citet{salimans2012collaborative} use Bayesian matrix factorization to identify multiple
latent rankings.
However, none of these approaches exploit item features to remedy labeling errors or generalize to new test items.
%crowdranking \citet{yi_inferring_2013} uses the crowd to make up for the fact that a target user has small data -- it's a form of collaborative filtering with pairwise labels using a non-Bayesian inference algorithm.
% Several other works learn multiple rankings from crowdsourced pairwise labels
% rather than a single gold-standard ranking, 
% but do not consider the item or user features so cannot extrapolate to new users or 
% items~\citep{yi_inferring_2013,kim2014latent}. 
% Both \citet{yi_inferring_2013} and learn a small number of
% latent ranking functions that can be combined to construct personalized preferences, 
% although neither provide a Bayesian treatment to handle data sparsity.
%include the work on collaborative GPPL
%Several extensions of BMF use Gaussian process priors over latent factors 
%to model correlations between 
%items given side information or observed item features~\citep{adams2010incorporating,zhou2012kernelized,houlsby2012collaborative,bolgar2016bayesian}. 
%However, these techniques are not directly applicable to 
%learning from pairwise comparisons 
%as they assume that the observations are Gaussian-distributed numerical ratings~\citep{shi2017survey}. 
In contrast, \citet{guo2010gaussian} propose a joint Gaussian process over the
space of users and features. Since this scales cubically
in the number of users, \citet{abbasnejad2013learning} 
propose to cluster the users into behavioural groups.
However, distinct clusters do not
allow for collaborative learning between users with partially overlapping preferences, e.g. two users may both like one genre of music, 
while having different preferences over other genres. 
\citet{khan2014scalable} instead learn a GP for each user,
and combine them with matrix factorization to perform collaborative filtering.
However, this approach does not model the relationship between
 input features and the latent factors, does not place a prior over item factors,
 and does not scale to very large sets of users.
An alternative is \emph{Collaborative GPPL}~\citep{houlsby2012collaborative},
which uses a latent factor model, where each latent factor has a Gaussian process prior. 
This allows the model to take advantage of the input features of
users and items when learning the latent factors. 
Each individual's preferences are then represented 
by a mixture of latent functions.
%Pairwise labels from users with common interests help to predict each other's 
%preference function, hence 
%this can be seen as a collaborative learning method, as used in recommender systems.
Using matrix factorization in combination with GP priors is therefore an effective
way to model the individual preferences of users while
making use of input features to generalize to test cases. However,
none of these approaches considers a consensus preference function, and
more scalable inference is needed for datasets containing thousands of items or users.
%other Bayesian recommender systems that deal with noisy preferences
%To combine Bayesian matrix factorization with a pairwise likelihood,
%\citet{houlsby2012collaborative} propose
%However, their proposed method does not scale sufficiently to the numbers of 
%items, users or pairwise labels found in many important application domains. 
% previous work on how to make BMF more scalable with larger datasets or Latent factor analysis etc.
% who has used GPs for BMF?

 % PCA: Gaussian noise. "The classical PCA converts a set of samples with possibly correlated variables into another   
 % set of samples with linearly uncorrelated variables via an orthogonal transformation [1]. Based on this, PCA
 % is an effective technique widely used in performing dimensionality reduction and extracting features." -- Shi et al 2017. shi2017survey
 % SVD: like PCA with the mean vector set to zeroes.
 % variations of PCA: for handling outliers or large sparse errors
 % most matrix factorizations are special cases of PCA and in practice do not consider the mean vector.
 % probabilistic PCA: latent variables are unit isotropic Gaussians --> all have 0 covariance and 1 variance.
 % Bayesian PCA: places priors on all latent variables.
 % Probabilistic factor analysis: assumes different variances on each of the latent factors.
 % Probabilistic matrix factorisation: ignores the mean. --> I.e. can be done with SVD
 % I think this means our method is a form of PFA? But extended to consider correlations in the weights.
 % NMF: as matrix factorisation but the low-rank matrices are non-negative.

% of their hybrid inference method expectation propagation and variational Bayes limits its application in many domains.
% They use FITC to provide a sparse approximation. This is still not as scalable as SVI and doesn't work as well --
% see the Hensman papers?
%with Gaussian processes to improve performance with sparse data. 
%Their model can be learned using pairwise labels, numerical ratings, or other likelihoods.

%user features nor model dependencies between item features 
%and the low-dimensional latent features, so it cannot exploit the latent features to predict preference scores for new items and relies instead on a user-specific GP.
%In contrast, our approach does not require learning a separate GP per user, but instead
%places GPs on both the latent factors. This means that the item and user
%features assist in learning the latent factors as we can exploit their similarities and
%correlations.
%relying instead on a purely individual GP with no shared information,
%(this would be a problem if the user fits the latent features exactly as the GP will end up modelling the user's individual devaiation from the common preferences modeled by the latent features) 
%To achieve scalability using a variational EM algorithm, \citet{khan2014scalable}
%sub-sample the pairwise labels meaning that 
%some training data must be discarded. In this paper, we
%applies stochastic variational inference to learn from all data 
%while limiting memory and computational requirements.
% how do we compare to them or do we get out of it? --> compare results on the same datasets
%Our approach is similar to Khan et al. 2014. "Scalable Collaborative Bayesian Preference Learning" but differs in that
%we also place priors over the weights and model correlations between different items and between different people.
%Our use of priors also encourage sparseness in the features. 
%TODOs:
% what is meant by 'factorization assumptions' exactly and do we make them? I think we do but don't fully
% understand why they're so bad. See [18,11] from Khan for examples of bad factorization. 

\subsection{Scalable Approximate Bayesian Inference}

\todo{
Another claim from the authors is the SVI for large-scale crowdGPPL, which is due to limitation of GP. Indeed, the online strategy has been well-studied in crowdBT,  crowdPL,  the previous works and the following work. 
Li et al,. Hybrid-MST: A Hybrid Active Sampling Strategy for Pairwise Preference Aggregation, NIPS 2018
Therefore, I don't find any new insight of SVI here.
}
Recent work on scalable Bayesian matrix factorization focuses on distributing and parallelizing 
 inference %rather than reducing total costs, 
 but is not directly applicable to Gaussian processes~\citep{ahn2015large,vander2017distributed,chen2018large}. 
%This paper focuses instead on reducing computational and memory costs, 
%although the method we propose is amenable to parallelization.
Models that combine Gaussian processes with non-Gaussian likelihoods 
require approximate inference methods that often scale poorly with 
the amount of training data available. 
Established methods such as the Laplace approximation 
and expectation propagation~\citep{rasmussen_gaussian_2006} have
computational complexity $\mathcal{O}(N^3)$ with $N$ data points
 and memory complexity $\mathcal{O}(N^2)$. 
For collaborative GPPL, \citet{houlsby2012collaborative}
propose a  kernel for pairwise 
preference learning and use a sparse
\emph{generalized fully independent training conditional} (GFITC) 
approximation~\citep{snelson2006sparse} to reduce the computational complexity to $\mathcal{O}(PM^2 + UM^2)$ and 
memory complexity to $\mathcal{O}(PM + UM)$,
where $P$ is the number of pairwise labels, $M \ll P$ is a fixed number of inducing points, and $U$ is the number of users.
However, this is not sufficiently scalable
 for very large numbers of items, users or pairs, as there is no clear way to parallelize it or to limit memory consumption.
%which assumes that
%the observations at each training data point are independent of one another given the 
%latent values at the inducing points.
%Since $P$ is typically larger than $N$, the number of inducing points, $M$ may also need to be increased.
The GP over pairs also makes it difficult to extract posteriors for latent function values for individual items,
and prevents mixing pairwise training labels
with observed ratings in future data aggregation settings.

To handle large numbers of pairwise labels, \citet{khan2014scalable}
develop a variational EM algorithm and sub-sample pairwise data rather than learning from the complete training set.
An alternative is \emph{Stochastic variational inference (SVI)}~\citep{hoffman2013stochastic}, which updates an approximation using 
 a different random sample at each iteration. 
 This allows the approximation to make use of all training data over a number of 
 iterations, while limiting training costs per iteration.
SVI has been successfully applied to Gaussian process regression~\citep{hensman2013gaussian} and classification~\citep{hensman2015scalable},
and provides a convenient framework for sparse approximation.
 An SVI method was also developed for preference learning,
 which places a GP over items rather than pairs, but does not model multiple users' preferences~\citep{simpson2018finding}.
This paper provides the first full derivation of this approach, %including showing how to learn the observation noise
%as part of the variational approach. 
as well as providing the first application of SVI to Bayesian matrix factorization
as part of a new model that accounts for individual user preferences.
%developing the first application of SVI to a matrix factorization approach.
%As our method includes Bayesian matrix factorization (BMF) as part of the model, 
%we believe this paper is the first to apply SVI to BMF.
