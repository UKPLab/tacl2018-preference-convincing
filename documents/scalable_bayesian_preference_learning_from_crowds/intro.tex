\section{Introduction}\label{sec:intro}

%Things we assume are discussed in the intro:
% -- recommendation -- item-item similarity, user-user similarity (CF), user-item matching
% -- learning from implicit preferences/user actions
% -- need to handle sparsity i.e. most pairs of users/items not labelled
% -- noisiness of pairwise labels? (btw can we use our model to combine implicit labels for a single user --> ground truth for this user --> not all sources of data agree with true orderings). 
% -- benefits of BMF
% -- use of GPs for BMF?
% -- Older learning-to-rank algorithms: 
%Learning to rank from pairwise comparison data has been studied for document retrieval systems
% 2. R. Herbrich, T. Graepel, K. Obermayer, "Large margin rank boundaries for ordinal regression" in , MIT Press, 2000.
% Show Context

% a learning algorithm was developed by extending AdaBoost in [3]
% 3. Y. Freund, R. D. Iyer, R. E. Schapire, Y. Singer, "An efficient boosting algorithm for combining preferences", Journal of Machine Learning Research, vol. 4, pp. 933-969, 2003.
% Show Context

% A simple probabilistic model based on a neural network, called RankNet, was introduced in [4] 
% 4. C. J. C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. N. Hullender, "Learning to rank using gradient descent", ICML, pp. 89-96, 2005.
% T. Joachims, "Optimizing search engines using clickthrough data", KDD, 2002.

%an SVM algorithm was generalized in [6] to learn a linear function for ranking
% 6. T. Joachims, "Optimizing search engines using clickthrough data", KDD, 2002.


%uchida2017entity -- learning preferences from user reviews
%APRIL paper from Alex? As a motivating example of why we need to learn preference function.

% A crowd may not necessarily consist of individual human annotators working towards a common goal of annotating a dataset. 
% It may be separate users of a system, whose individiual preferences we wish to predict,
% or we may treat the same user carrying out different tasks as a separate individual.  
% Each user may also generate labels for different types of user interaction.
% Hence, we can go beyond simple user-item matrices to treat each type of annotation  
% as a different 'user', and thereby combine different sources of information.
% Like the workers in a crowd, each source of annotations has a particular correlation with a ground truth
% that we wish to predict, i.e. the preferences for a particular user in a specific context.
% We must learn this correlation by identifying the ground truth from either explicit user ratings
% or consensus between relevant sources of information, defined a priori.

%The experiments section notes the three main benefits of our method:
% a) recovering an underlying consensus from noisy pairwise labels; --> 1
% b) modeling personal preferences from pairwise labels;  --> 1
% c) the scalability of our proposed Bayesian preference learning methods, GPPL and crowd-GPPL using SVI. --> 2
% d) ? test the hyperparameter learning method to see if it improves cost of learning
% over gradient-free methods.
% Is this the clear benefit or are we missing something else?
% Are these mentioned below?

%for instance, the best way to summarise a text document may depend on the readerâ€™s 
%prior knowledge of the topic. 
%However, state-of-the-art methods rely heavily on large quantities of labelled training data, 
%making adaptation to user or context difficult. This leads to the following questions:
%How can users be empowered to adapt existing models to their individual changing needs?
%Can we train NLP models without acquiring large numbers of labelled examples?
%How can decisions made using machine learning be explained users to build their trust?

% TODO: starts off too general. This is about preference learning, not annotation!
% TODO: what is the purpose of the consensus? Is it a de-biased ground truth for some objective task or task for which a consensus is desirable? If so, does this fall under the category of subjective tasks? Perhaps a better skew on this is 
% to think about preference learning foremost as the topic: ranking with pairwise labels. If this is desirable, then we 
% encounter two problems: 1, the biased labellers, and 2, subjective tasks.

%Comparing the values of items according to a particular quality is a 

% learing -> infers a function for comparing
%
\emph{Preference learning} is the task of learning to compare the values of a set of alternatives
according to a particular quality~\citep{furnkranz2010preference}. The goal may be to 
rank items by preference, or given a pair of items, to select the item with the highest utility.
For many preference learning tasks, annotators have divergent opinions on the correct rankings or utilities,
which impedes the acquisition of training data.
%for any given data point.
%For many learning tasks, the desired output is inherently subjective or context-dependent, 
%yet supervised learning learning relies on gold-standard training labels. 
%yet for many tasks, 
%which can be hard to obtain in sufficient quantities 
%for subjective tasks or 
%when annotators fail to agree due to their individual viewpoints or preferences.
For example, in the field of argument mining, one goal is to 
identify convincing arguments from a corpus of documents~\citep{habernal2016argument}. 
Whether a particular argument is convincing or not depends on the reader's point of view and prior knowledge~\citep{lukin2017argument}.
Similarly, recommender systems can perform better if they make recommendations tailored
to a specific user~\citep{resnick1997recommender}.
%However, while large amounts of data may be available from
%many different users, the amount of data for any particular user
%may be very small~\citep{?}.
%When large amounts of labeled data are required for machine learning,
Crowdsourcing is frequently used as a cost-effective source of labeled data, 
yet disagreements between annotators must be resolved to obtain a gold-stanard
training set, typically requiring redundant labeling and increased annotation costs~\citep{snow2008cheap,banerji2010galaxy,gaunt2016training}.
% TODO should be saying 'instance' not 'item' because preferences could be over abstract instances, such as states a robot could enter
%Current methods for preference learning
%are either unable to model differences of opinion\citep{G?},
%do not scale to realistic dataset sizes\citep{chu2005preference},
%or cannot generalize to new instances or users\citep{BMF?best worst scaling?}.
Therefore, we require solutions for modeling individual preferences given
limited data per user and producing a gold standard from crowds of annotators with different opinions.

% not sure I like this because previous paragraph starts of super general,
% then here we seem to focus arbitrarily on ranking and rating?
% TODO Bigger issue: we seem to move on from previous problem of subjectivity to the problem of 
% assigning numerical ratings.
% We put pairwise stuff here because it narrows the scope early. One could also talk about collaborative filtering here instead.
% Here we are searching for the right form of annotation...
A preference model can be trained using numerical ratings, yet 
each annotator may interpret the values differently and may label inconsistently depending on the order in which they annotate items~\citep{ovadia2004ratings,yannakakis2011ranking}: 
a score of -1, say, from one annotator may be equivalent to a -20 from another. 
An alternative is \emph{pairwise labeling}, 
in which the annotator compares pairs of items and selects the preferred one in each pair.
Making pairwise choices places lower cognitive load on annotators than numerical ratings~\citep{yang2011ranking},
and facilitates the total sorting of items, since it avoids cases where two items have the same value~\citep{kendall1948rank,kingsley2010preference}.
Besides explicit annotations, pairwise preference labels can be extracted from
user behavior logs, such as when a user selects one item from a list in preference
to others~\citep{joachims2002optimizing}. 
However, such \emph{implicit annotations} can be noisy, as are crowdsourced pairwise labels~\citep{habernal2016argument}.
Therefore, while pairwise labels provide a valuable
form of training data, preference learning methods must be able to 
aggregate noisy sources of pairwise data from crowds 
or from different types of implicit annotation.
We henceforth refer to the annotators, users or implicit data sources simply as \emph{users}
whose preferences we wish to model.
Likewise, the term \emph{items} refers to any type of instance that users may express preferences over,
and could be states or actions as well as physical objects.
%individual preferences and reducing 
%the incompatibility of labels provided by different annotators.

% TODO could consider swapping the order of BMF and Bayesian paragraphs? Or better linking the pairwise and BMF paragraphs?
In recommender systems, information from different users is aggregated using \emph{collaborative filtering},  
which predicts a user's preferences for an item they have not annotated using the observed preferences of 
similar users for that item~\citep{resnick1997recommender}.
A typical approach is to represent observed ratings in a user-item matrix,
then apply \emph{matrix factorization}~\citep{koren2009matrix}
to decompose a user-item matrix into two low-dimensional matrices.
Users and items with similar observed ratings have similar row vectors in the low-dimensional
matrices. Multiplying the low-dimensional representations predicts ratings for unseen
user-item pairs. 
However, traditional approaches are not able to handle pairwise labels,
nor extrapolate effectively to new users or items.
%nor perform well with very small numbers of observations per user\citep{mnih2008probabilistic}. 
%They do not take advantage of the item or user features
%~\cite{?} or require a hierarchical model that incorporates them outside the matrix factorization step~\cite{?}. 
%Collaborative filtering is therefore a way to aggregate preference data from multiple users 
%to make up for a shortfall in labeled data for a target user whose preferences we 
%wish to predict.

The limited amount of noisy data for each user,
as well as the desire to aggregate data from multiple users,
motivates a Bayesian approach that can account for uncertainty in the model.
Matrix factorization, has been shown to benefit from a Bayesian treatment when data is sparse~\citep{salakhutdinov2008bayesian}. 
Previous work has also introduced a Bayesian approach for combining crowdsourced preferences, but this models only ground truth rather than individual preferences and cannot make predictions for new users or items~\citep{chen2013pairwise}.
For crowdsourced classification tasks, \citet{simpson2017bayesian} show that modeling item features using a Gaussian process can improve performance, but this has not yet been adapted for preference learning.
Gaussian processes have also been used in previous work to provide a Bayesian framework for preference learning
~\citep{chu2005preference,houlsby2012collaborative,khan2014scalable}, 
but these methods do not scale to large numbers of items, users, or
pairwise labels as it is unclear how to parallelize them or constrain their memory requirements.
% to computational and memory complexity is at best linear in the number of items, users and labels.
%these methods also do not 

In this paper, we propose a scalable Bayesian approach to pairwise preference learning with crowds,
whose members may be annotators, users or sources of implicit annotations.
Our approach is designed for preference learning over items, rather than over labels (see discussion in \citet{furnkranz2010preference}),
and jointly models %for aggregating pairwise  labels from multiple sources that
personal preferences and the consensus of a crowd by combining 
matrix factorization and Gaussian processes. % preference learning~\citep{chu2005preference}.
To enable inference at the scale of large, real-world datasets,
we derive a stochastic variational inference scheme~\citep{hoffman2013stochastic} for our approach.
%Our method is able to model both the individual preferences of members of the crowd,
%and to extract a gold-standard consensus from crowdsourced annotations.
% Structure of related work should also follow these questions:
% preference learning from crowds (okay); 
% Bayesian preference learning (where does it fit in these points? Small data/extrapolation to new examples);
% Bayesian matrix factorization (again, Bayesian part needs to be linked in better; this is about personal preference learning);
% SVI (okay, fits scalability part; but maybe talk about scalable Bayesian methods in general?);
% Nothing about hyperparameters (Nickisch, maybe don't separate this point out but make it part of the scalable Bayesian inference method?).
% % structure below is a little confusing? I think only a list of contributions + one or two questions is correct.
%  develop methodology to solve the following questions:
% \begin{enumerate}
%   \item How can we learn  user preference functions over large sets of items given a large number of pairwise comparisons?
%   \item How do we account for the different personal preferences of annotators when inferring a consensus preference function?
%   \item Can we provide a Bayesian approach to account for uncertainty in our learned model and confidence in its predictions? 
% \end{enumerate}
% Our technical contributions can be summarized as follows:
% \begin{enumerate}
%  \item We propose a model for aggregating pairwise preference labels from multiple sources that
%  models both personal preferences and the consensus of a crowd;
%   % using a model of the noise and biases of individual annotators. % say why this is not possible with Dawid and Skene
%   \item %To enable collaborative preference learning % collaborative preference learning needs to be defined above
%    %To enable inference on our model at the scale of large, real-world datasets, 
%    We derive a stochastic variational inference method for Bayesian matrix factorization and Gaussian process preference learning.
% %  \item To expedite hyper-parameter tuning, we introduce a technique for gradient-based length-scale optimization of Gaussian processes.
% \end{enumerate}
By providing a scalable Bayesian approach we open preference learning up to novel applications
for which annotations are sparse, noisy and biased, and where the number of users, items and pairwise labels is large.
Our empirical evaluation demonstrates the scalability of our approach,
and its ability to predict personal preferences as well as an objective gold-standard given crowdsourced data.
% TODO how much do we really test data sparsity?

The next section of the paper discusses related work.
We then we develop our model for preference learning from crowds in Section \ref{sec:model},
followed by our proposed inference method in Section \ref{sec:inf}.
%and hyper-parameter optimisation technique in Section \ref{sec:ls}.
Then, in Section \ref{sec:expts}, 
we evaluate our approach empirically, showing its behaviour on synthetic data, 
its scalability and its predictive performance on several real-world datasets.
