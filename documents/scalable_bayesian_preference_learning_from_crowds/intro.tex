\section{Introduction}\label{sec:intro}

%Things we assume are discussed in the intro:
% -- recommendation -- item-item similarity, user-user similarity (CF), user-item matching
% -- learning from implicit preferences/user actions
% -- need to handle sparsity i.e. most pairs of users/items not labelled
% -- noisiness of pairwise labels? (btw can we use our model to combine implicit labels for a single user --> ground truth for this user --> not all sources of data agree with true orderings). 
% -- benefits of BMF
% -- use of GPs for BMF?
% -- Older learning-to-rank algorithms: 
%Learning to rank from pairwise comparison data has been studied for document retrieval systems
% 2. R. Herbrich, T. Graepel, K. Obermayer, "Large margin rank boundaries for ordinal regression" in , MIT Press, 2000.
% Show Context

% a learning algorithm was developed by extending AdaBoost in [3]
% 3. Y. Freund, R. D. Iyer, R. E. Schapire, Y. Singer, "An efficient boosting algorithm for combining preferences", Journal of Machine Learning Research, vol. 4, pp. 933-969, 2003.
% Show Context

% A simple probabilistic model based on a neural network, called RankNet, was introduced in [4] 
% 4. C. J. C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. N. Hullender, "Learning to rank using gradient descent", ICML, pp. 89-96, 2005.
% T. Joachims, "Optimizing search engines using clickthrough data", KDD, 2002.

%an SVM algorithm was generalized in [6] to learn a linear function for ranking
% 6. T. Joachims, "Optimizing search engines using clickthrough data", KDD, 2002.


%uchida2017entity -- learning preferences from user reviews
%APRIL paper from Alex? As a motivating example of why we need to learn preference function.

% A crowd may not necessarily consist of individual human annotators working towards a common goal of annotating a dataset. 
% It may be separate users of a system, whose individiual preferences we wish to predict,
% or we may treat the same user carrying out different tasks as a separate individual.  
% Each user may also generate labels for different types of user interaction.
% Hence, we can go beyond simple user-item matrices to treat each type of annotation  
% as a different 'user', and thereby combine different sources of information.
% Like the workers in a crowd, each source of annotations has a particular correlation with a ground truth
% that we wish to predict, i.e. the preferences for a particular user in a specific context.
% We must learn this correlation by identifying the ground truth from either explicit user ratings
% or consensus between relevant sources of information, defined a priori.

%The experiments section notes the three main benefits of our method:
% a) recovering an underlying consensus from noisy pairwise labels; --> 1
% b) modeling personal preferences from pairwise labels;  --> 1
% c) the scalability of our proposed Bayesian preference learning methods, GPPL and crowd-GPPL using SVI. --> 2
% d) ? test the hyperparameter learning method to see if it improves cost of learning
% over gradient-free methods.
% Is this the clear benefit or are we missing something else?
% Are these mentioned below?

%for instance, the best way to summarise a text document may depend on the readerâ€™s 
%prior knowledge of the topic. 
%However, state-of-the-art methods rely heavily on large quantities of labelled training data, 
%making adaptation to user or context difficult. This leads to the following questions:
%How can users be empowered to adapt existing models to their individual changing needs?
%Can we train NLP models without acquiring large numbers of labelled examples?
%How can decisions made using machine learning be explained users to build their trust?

% TODO: starts off too general. This is about preference learning, not annotation!
% TODO: what is the purpose of the consensus? Is it a de-biased ground truth for some objective task or task for which a consensus is desirable? If so, does this fall under the category of subjective tasks? Perhaps a better skew on this is 
% to think about preference learning foremost as the topic: ranking with pairwise labels. If this is desirable, then we 
% encounter two problems: 1, the biased labellers, and 2, subjective tasks.

%Comparing the values of items according to a particular quality is a 

% learing -> infers a function for comparing
%
\emph{Preference learning} is the task of learning to compare the values of a set of alternatives
according to a particular quality~\citep{furnkranz2010preference}. 
Here, we focus on ranking items by preference 
and selecting the preferred item from a pair.
% or \emph{utility} --> leave this until we introduce RUM
For many preference learning tasks, people have divergent opinions on the correct rankings, % or utilities,
which means that data from different annotators may not agree.
For example, in argument mining, 
a sub-field of natural language processing (NLP),
one goal is to rank arguments by their \emph{convincingness}~\citep{habernal2016argument}. 
Whether a particular argument is convincing or not depends on the reader's point of view and prior knowledge~\citep{lukin2017argument}.
Similarly, personal preferences affect recommender systems,
which often perform better if they tailor recommendations
to a specific user~\citep{resnick1997recommender}.
Disagreements also occur when training data is acquired from multiple annotators,
for example, using crowdsourcing,
and are often mitigated by redundant labelling, 
which increases costs~\citep{snow2008cheap,banerji2010galaxy}.
% TODO: Say something about the size of dataset requiring scalable solutions? Maybe covered below.
%Crowdsourcing is frequently used as a cost-effective source of labelled data, 
%yet disagreements between annotators must be resolved to obtain a gold-standard
%training set, typically requiring redundant labelling and increased annotation costs~.
%should the bias stuff go here to motivate the next line?
% or should the crowdsourcing stuff go later on? This is just motivation, the above considers solutions already.
Therefore, we require preference learning methods that can account for differences of opinion
%when predicting individual preferences or 
%producing a gold standard from crowdsourced preference annotations.
to solve two tasks:
(1) predicting \emph{personal} preferences for members of a crowd
and
(2) inferring a \emph{consensus} given observations from multiple, disparate users.

As data from a single user is often sparse,
recommender systems often %combine information from different users 
predict predict a user's preferences by 
via \emph{collaborative filtering},  
which combines the observed preferences of similar users~\citep{resnick1997recommender}.
A typical approach is to represent observed ratings in a user-item matrix,
then apply \emph{matrix factorization}~\citep{koren2009matrix}
to decompose a user-item matrix into two low-dimensional matrices.
Users and items with similar observed ratings have similar row vectors in the low-dimensional
matrices. By multiplying the low-dimensional representations, we can predict ratings for unseen
user-item pairs. 
However, traditional approaches 
are not able to extrapolate to new users or items,
as they do not exploit the \emph{input features} of items or users
that can be extracted from their content or metadata.
For example, in text it is common to count occurrences of each unique token
or, more recently, to obtain an \emph{embedding} of a word or document that
represents its content as a numerical vector~\citep{mikolov2013distributed,devlin2018bert}.
%Features can be represented as a vector of numerical values corresponding to each item or user
%and can be used to 
Input features enable predictions for a test item or user 
based on observations of others with similar feature values,
and can help to remedy labelling errors when learning from noisy, crowdsourced data~\citep{felt2016semantic,simpson2015language}.
Furthermore,
many established methods for matrix factorisation 
require that training data is provided in the form of numerical ratings.

A drawback of numerical ratings is that 
each annotator may interpret the values differently and may label inconsistently depending on the order in which they annotate items~\citep{ovadia2004ratings,yannakakis2011ranking}.
A score of 4/5, say, from one annotator may be equivalent to 3/5 from another. 
The problem is avoided by \emph{pairwise labelling}, 
in which the annotator selects their preferred item from a pair.
Pairwise labelling can also be quicker for annotators than numerical rating,
and facilitates the total sorting of items, since it avoids two items having the same value~\citep{kendall1948rank,kingsley2010preference,yang2011ranking},
and can be result in more accurate rankings\citep{kiritchenko2017best}.
Besides explicit annotations, pairwise labels can be extracted from
user behaviour logs, such as when a user selects one item from a list in preference
to others~\citep{joachims2002optimizing}. 
We therefore focus on preference learning from pairwise labels,
and henceforth refer to the annotators, users or implicit data sources simply as \emph{users}.
Likewise, \emph{items} are any type of instance that users may express preferences over,
and could be states or actions as well as objects.

Pairwise labels provided by a crowd are often noisy~\citep{chen2013pairwise}, 
as are labels extracted from user logs~\citep{hu2008collaborative}.
The need to be robust to noise despite limited data per user
and to aggregate data from multiple users
motivates a Bayesian treatment.
Bayesian approaches account for uncertainty in the model,
which has been shown to benefit matrix factorization with sparse data~\citep{salakhutdinov2008bayesian}
and preference learning with noisy pairwise labels~\citep{chen2013pairwise},
although this previous work does not model individual preferences or make predictions for new users or items.
A powerful Bayesian approach for prediction is the Gaussian process (GP), 
which models nonlinear functions of input features
and has previously been applied to preference learning
~\citep{chu2005preference,houlsby2012collaborative,khan2014scalable}.
However, existing GP-based methods do not scale to very large numbers of items, users, or
pairwise labels as their computational and memory requirements grow with the size of the dataset.

In this paper, we propose a scalable Bayesian approach to pairwise preference learning with 
large numbers of users or annotators. % or sources of implicit annotations.
Our method, \emph{crowdGPPL},
 jointly models personal preferences and the consensus of a crowd through a combination of
matrix factorization and Gaussian processes. 
We propose a \emph{stochastic variational inference (SVI)} scheme~\citep{hoffman2013stochastic}
that scales to extremely large datasets, as its
memory complexity and the time complexity of each iteration are 
fixed independently of size of the dataset.
Our new approach opens the door to novel applications involving very large numbers of users, items and pairwise labels,
that would previously have exceeded computational or memory resources and were difficult to parallelise.
We evaluate the method empirically on two real-world datasets to demonstrate 
the scalability of our approach,
and its ability to predict both personal preferences and an objective gold-standard given 
preferences from thousands of users.
Our results improve performance over the previous state-of-the-art \citep{simpson2018finding} on a crowdsourced argumentation dataset, illustrating the potential of the method for NLP applications.

The next section of the paper provides background on preference learning
and discusses related work.
We then develop our model for preference learning from crowds in Section \ref{sec:model},
followed by our proposed inference method in Section \ref{sec:inf}.
%and hyper-parameter optimisation technique in Section \ref{sec:ls}.
Then, in Section \ref{sec:expts}, 
we evaluate our approach empirically, showing its behaviour on synthetic data, 
its scalability and its predictive performance on real-world datasets.
