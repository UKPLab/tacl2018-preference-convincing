\section{Introduction}\label{sec:intro}

Many tasks are more suited to pairwise comparisons than classification etc. 
Crowds of non-expert annotators may label more accurately if presented with pairs. 
Implicit feedback may be taken from user actions in an application that can be represented as a preference, such as choosing
an option over other options.

There are several works for learning from noisy pairwise comparisons so far (Horvitz et al. 2013 or something like that?). 
However, these do not provide a way to take account of item features or to model different but valid subjective viewpoints. 
They assume there is a single ground truth and can therefore model only one task and one user's (or a consensus of all users) preferences at once. 

Work by Felt et al. 2015, Simpson et al. 2015 etc. shows that item features are particularly useful when combining crowdsourced data. A Gaussian process has not been tested for this purpose before?

GP preference learning presents a way to learn from noisy preferences but assumes constant noise and a single underlying preference function. 
The collaborative Gaussian process (Houlsby et al. 2012) learns multiple users' preferences. 
However existing implementations do not scale and do not identify ground truth. 

We show how to scale it using SVI and how to use the model to identify ground truth from subjective preferences. 

%Things we assume are discussed in the intro:
% -- recommendation -- item-item similarity, user-user similarity (CF), user-item matching
% -- learning from implicit preferences/user actions
% -- need to handle sparsity i.e. most pairs of users/items not labelled
% -- noisiness of pairwise labels? (btw can we use our model to combine implicit labels for a single user --> ground truth for this user --> not all sources of data agree with true orderings). 
% -- benefits of BMF
% -- use of GPs for BMF?
% -- Older learning-to-rank algorithms: 
%Learning to rank from pairwise comparison data has been studied for document retrieval systems
% 2. R. Herbrich, T. Graepel, K. Obermayer, "Large margin rank boundaries for ordinal regression" in , MIT Press, 2000.
% Show Context

% a learning algorithm was developed by extending AdaBoost in [3]
% 3. Y. Freund, R. D. Iyer, R. E. Schapire, Y. Singer, "An efficient boosting algorithm for combining preferences", Journal of Machine Learning Research, vol. 4, pp. 933-969, 2003.
% Show Context

% A simple probabilistic model based on a neural network, called RankNet, was introduced in [4] 
% 4. C. J. C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. N. Hullender, "Learning to rank using gradient descent", ICML, pp. 89-96, 2005.
% T. Joachims, "Optimizing search engines using clickthrough data", KDD, 2002.

%an SVM algorithm was generalized in [6] to learn a linear function for ranking
% 6. T. Joachims, "Optimizing search engines using clickthrough data", KDD, 2002.


%uchida2017entity -- learning preferences from user reviews
%APRIL paper from Alex? As a motivating example of why we need to learn preference function.

% A crowd may not necessarily consist of individual human annotators working towards a common goal of annotating a dataset. 
% It may be separate users of a system, whose individiual preferences we wish to predict,
% or we may treat the same user carrying out different tasks as a separate individual.  
% Each user may also generate labels for different types of user interaction.
% Hence, we can go beyond simple user-item matrices to treat each type of annotation  
% as a different 'user', and thereby combine different sources of information.
% Like the workers in a crowd, each source of annotations has a particular correlation with a ground truth
% that we wish to predict, i.e. the preferences for a particular user in a specific context.
% We must learn this correlation by identifying the ground truth from either explicit user ratings
% or consensus between relevant sources of information, defined a priori.

%The experiments section notes the three main benefits of our method:
% a) recovering an underlying consensus from noisy pairwise labels; --> 1
% b) modeling personal preferences from pairwise labels;  --> 1
% c) the scalability of our proposed Bayesian preference learning methods, GPPL and crowd-GPPL using SVI. --> 2
% d) ? test the hyperparameter learning method to see if it improves cost of learning
% over gradient-free methods.
% Is this the clear benefit or are we missing something else?
% Are these mentioned below?

\subsubsection*{New Intro}

Supervised learning relies on gold-standard labels for training, 
yet for many tasks, annotators have divergent opinions on the correct label
for any given data point.
%which can be hard to obtain in sufficient quantities 
%for subjective tasks or 
%when annotators fail to agree due to their individual viewpoints or preferences.
In the field of argument mining, for example, one goal may be to train a model to 
identify convincing arguments from a corpus of documents~\citep{habernal2016argument}. 
Whether a particular argument is convincing or not depends on the reader's point of view and prior knowledge~\citep{lukin2017argument}.
Similarly, recommender systems typically perform better if they make recommendations tailored
to a specific user~\citep{??}.
While large amounts of data may be available from
many different users, the amount of gold-standard training data for any particular user
may be very small~\citep{?}.
Disagreements between annotators also present a problem when using crowdsourcing to 
obtain a single set of gold labels~\citep{?}.
Subjective tasks therefore require methods for learning individual preferences given 
limited data and obtaining gold standard labels from crowds of biased annotators.

% not sure I like this because previous paragraph starts of super general,
% then here we seem to focus arbitrarily on ranking and rating?
% TODO Bigger issue: we seem to move on from previous problem of subjectivity to the problem of 
% assigning numerical ratings.
% We put pairwise stuff here because it narrows the scope early. One could also talk about collaborative filtering here instead.
% Here we are searching for the right form of annotation...
Many subjective tasks can be cast as \emph{preference learning} problems,
in which the aim is to learn rankings or numerical ratings of items that reflect their value.
While preferences can be expressed as numerical ratings,
different annotators may interpret values differently: 
a score of -1, say, from one annotator may be equivalent to a -20 from another~\cite{?}. 
The annotators may also assign ratings inconsistently
over time~\cite{?}. 
An alternative is \emph{pairwise preference labeling}, 
in which the annotator compares pairs of items and selects the preferred item.
Making pairwise choices places lower cognitive load on annotators than numerical ratings ~\citep{kendall1948rank,kingsley2010},
and facilitates the total sorting of items, since it avoids cases where items have the same values.
Besides explicit annotations, pairwise preference labels can be extracted from
user behavior logs, such as when a user selects one item from a list in preference
to others~\cite{}. 
Pairwise preference labels therefore provide a potentially valuable
source of information for learning individual preferences and reducing 
the incompatibility of labels provided by different annotators.
% TODO How do pairwise labels related to the previous problem of subjective tasks?

In recommender systems, the lack of labeled data from a specific user is addressed through \emph{collaborative filtering}, whereby predictions of a user's preferences are
informed by the observed preferences of similar users~\citep{?}.
By representing the preferences as a user-item matrix
This is often achieved using \emph{matrix factorization}~\cite{?},
in which a  records the rating given by each user to each item; 
this matrix is factorized approximately into two low-dimensional matrices;
users and items with similar ratings have similar row vectors in the low-dimensional
matrices; 
multiplying the low-dimensional representations predicts ratings for unseen
user-item pairs. 
Traditional approaches are not able to extrapolate to new users or items,
and require sufficient data for each to factorize them correctly. They do not take advantage of the item or user features
~\cite{?} or require a hierarchical model that incorporates them outside the matrix factorization step~\cite{?}. 
Collaborative filtering is therefore a way to aggregate preference data from multiple users to make up for a shortfall in labeled data for a target user whose preferences we 
wish to predict.

Whether provided explicitly or extracted from user behavior logs,
pairwise preference labels may be noisy, and the amount of 
data for any given user may be small.
This motivates a Bayesian approach that can account for uncertainty in the model 
and its predictions that results from noise, 
small datasets and the aggregation of data from multiple sources.
While Bayesian methods for pairwise preference learning have been proposed
in previous work~\cite{chu2009preference,houlsby2012collaborative,khan2014scalable}, they do not scale to large numbers of items, users, or
pairwise labels as computational and memory complexity is polynomial in the number
of items and pairs and linear or worse in the number of users.



% Structure of related work should also follow these questions:
% preference learning from crowds (okay); 
% Bayesian preference learning (where does it fit in these points? Small data/extrapolation to new examples);
% Bayesian matrix factorization (again, Bayesian part needs to be linked in better; this is about personal preference learning);
% SVI (okay, fits scalability part; but maybe talk about scalable Bayesian methods in general?);
% Nothing about hyperparameters (Nickisch, maybe don't separate this point out but make it part of the scalable Bayesian inference method?).

% structure below is a little confusing? I think only a list of contributions + one or two questions is correct.
In this paper, we develop methodology to solve the following questions:
\begin{enumerate}
  \item How can we learn  user preference functions over large sets of items given a large number of pairwise comparisons?
  \item How do we account for the different personal preferences of annotators when inferring a consensus preference function?
  \item Can we provide a Bayesian approach to account for uncertainty in our learned model and confidence in its predictions? 
\end{enumerate}
To answer these questions we make the following technical contributions:
\begin{enumerate}
 \item We propose a model of both personal preferences and the consensus of a crowd 
  that aggregates pairwise preference labels
  % using a model of the noise and biases of individual annotators. % say why this is not possible with Dawid and Skene
  \item %To enable collaborative preference learning % collaborative preference learning needs to be defined above
   To enable inference on our model at the scale of large, real-world datasets, we develop
   stochastic variational inference for Bayesian matrix factorization and Gaussian process preference learning.
  \item To expedite hyper-parameter tuning, we introduce a technique for gradient-based length-scale optimization of Gaussian processes.
\end{enumerate}

The next section of the paper discusses related work.
We then we develop our model for preference learning from crowds in Section \ref{sec:model},
followed by our proposed inference method in Section \ref{sec:inf} and
hyper-parameter optimisation technique in Section \ref{sec:ls}.
Then, in Section \ref{sec:expts}, 
we evaluate our approach empirically, showing first its behaviour on synthetic data, 
then its scalability and predictive performance on several real-world datasets.
