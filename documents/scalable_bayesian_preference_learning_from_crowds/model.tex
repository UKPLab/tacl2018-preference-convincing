\section{Bayesian Preference Learning for Crowds}\label{sec:model}

We assume that a pair of items, $a$ and $b$, have utilities
$f(\bs x_a)$ and $f(\bs x_b)$, which represent their value to a user,
and that $f: \mathbb{R}^D \mapsto \mathbb{R}$ 
is a function of item features, where $\bs x_a$ and $\bs x_b$ are vectors 
of length $D$ containing the features of items $a$ and $b$, respectively.
If $f(\bs x_a) > f(\bs x_b)$, then $a$ is preferred to $b$ (written $a \succ b$).
The outcome of a comparison between $a$ and $b$ is 
a pairwise label, $y(a, b)$.
Assuming that pairwise labels never contain errors,
then $y(a, b)=1$ if $a \succ b$ and $0$ otherwise.
Given knowledge of $f$, we can compute the utilities 
of items in a test set given their features, and the outcomes of pairwise comparisons.

\citet{thurstone1927law} proposed the random utility model,
which relaxes the assumption that pairwise labels, $y(a, b)$,
are always consistent with the ordering of $f(\bs x_a)$ and $f(\bs x_b)$.
Under the random utility model, the likelihood $p(y(a,b)=1)$ 
increases as $f_a - f_b$ increases, i.e.,
as the utility of item $a$ increases
relative to the utility of item $b$.
This reflects the greater consistency in a user's choices
when their preferences are stronger,
while accommodating
%However, since $0 < p(y(a,b)=1) < 1$, the model 
%is uncertain about the value of $y(a,b)$,
labelling errors or variations in a user's choices over time.
%The uncertainty is lower if the values $f_a$ and $f_b$ are further apart, 
%which 
%The random utility model is defined by a likelihood function that
%maps the utilities to $p(y(a,b))$.
%or the Thurstone-Mosteller model.
%\begin{align}
%p(y(a, b) | f) & = \frac{1}{1 + \exp( f(\bs x_a) - f(\bs x_b) ) }
%\end{align}
In the Thurstone-Mosteller model, % case V model, 
noise in the observations is explained by a Gaussian-distributed noise term, $\delta \sim \mathcal{N}(0, \sigma^2)$:
\begin{flalign}
 p(y(a, b) | f(\bs x_a) + \delta_{a}, f(\bs x_b) + \delta_{b} )  
 \hspace{0.9cm} & = \begin{cases}
 1 & \text{if }f(\bs x_a) + \delta_{a} \geq f(b) + \delta_{b} \\
 0 & \text{otherwise,}
 \end{cases} &
 \label{eq:thurstone}
\end{flalign}
Integrating out the unknown values of $\delta_a$ and $\delta_b$ gives:
\begin{flalign}
& p( y(a, b) | f(\bs x_a), f(\bs x_b) )  & \label{eq:plphi}\\
& = \!\! \int\!\!\!\! \int \!\! p( y(a, b) | f(\bs x_a) + \delta_{a}, f(\bs x_b) + \delta_{b} ) \mathcal{N}\left(\delta_{a}; 0, \sigma^2\right)\mathcal{N}\left(\delta_{b}; 0, \sigma^2\right) d\delta_{a} d\delta_{b} 
%= \Phi\left(\frac{f(\bs x_a) - f(\bs x_b)}{\sqrt{2\sigma^2}}\right) 
= \Phi\left( z \right), & \nonumber
\end{flalign}
where $z = \frac{f(\bs x_a) - f(\bs x_b)}{\sqrt{2\sigma^2}}$,
and $\Phi$ is the cumulative distribution function of the standard normal distribution,
meaning that $\Phi(z)$ is a 
probit likelihood.\footnote{Please note that a full list of symbols is provided for reference in Appendix $\ref{sec:not}$}
This likelihood is also used by
\citet{chu2005preference} for Gaussian process preference learning (GPPL), but here 
we simplify the formulation by assuming that $\sigma^2 = 0.5$,
which leads to $z$ having a denominator of $\sqrt{2 \times 0.5}=1$,
hence $z = f(\bs x_a) - f(\bs x_b)$.
Instead, we model varying degrees of noise in the pairwise labels
by scaling $f$ itself, as we describe in the next section.

In practice, $f(\bs x_a)$ and $f(\bs x_b)$ must be inferred from
pairwise training labels, $\bs y$,
to obtain a posterior distribution over their values.
If this posterior is a multivariate Gaussian distribution,
then the probit likelihood allows us to analytically marginalise 
$f(\bs x_a)$ and $f(\bs x_b)$
to obtain the probability of a pairwise label:
\begin{flalign}
p(y(a,b)| \bs y) 
= \Phi(\hat{z}),& & \hat{z} = \frac{\hat{f}_a - \hat{f}_b}{\sqrt{1 + C_{a,a} + C_{b,b} 
- 2C_{a,b}} }, \label{eq:predict_z} &&
\end{flalign}
where $\hat{f}_a$ and $\hat{f}_b$ are the means and
$\bs C$ is the posterior covariance matrix of the multivariate Gaussian over
$f(\bs x_a)$ and $f(\bs x_b)$.
Unlike other choices for the likelihood, such as a sigmoid,
the probit allows us to compute the posterior over a pairwise label
without further approximation,
% given this posterior over $f(\bs x_a)$ and $f(\bs x_b)$,
%numerical integration
hence we assume this pairwise label likelihood for our proposed preference learning model.
%Obtaining the posterior over $f$ is itself challenging, however, 
%and therefore in Section $\ref{sec:inf}$ we propose 
%an approximate inference method to address this problem.


\subsection{GPPL for Single User Preference Learning}

We can model the preferences of a single user by assuming
a Gaussian process prior over the user's utility function, 
%is a function of item features and 
$f \sim \mathcal{GP}(0, k_{\theta}/s)$, where $k_{\theta}$ is a kernel function with hyperparameters $\theta$
and $s$ is an inverse scale parameter.
The kernel function takes numerical item features as inputs and determines the covariance between values of $f$ for different items. 
The choice of kernel function and its hyperparameters controls the shape and smoothness of the function 
across the feature space and is often treated as a model selection problem.
Kernel functions suitable for a wide range of tasks include the \emph{squared exponential} 
and the \emph{Mat\'ern}~\citep{rasmussen_gaussian_2006},
which both make minimal assumptions but 
assign higher covariance to items with similar feature values.
We use $k_{\theta}$ to compute a covariance matrix $\bs K_{\theta}$,
between a set of $N$ observed items with features $\bs X = \{ \bs x_1, ..., \bs x_N \}$.

Here we extend the original definition of GPPL~\citep{chu2005preference},
by introducing the inverse scale, $s$,
which is drawn from a gamma prior, 
$s \sim \mathcal{G}(\alpha_0, \beta_0)$, with shape $\alpha_0$ and scale $\beta_0$.
The value of $1/s$ determines the variance of $f$,
and therefore 
the magnitude of differences between $f(\bs x_a)$ and $f(\bs x_b)$ for
items $a$ and $b$. This in turn affects the level of certainty
in the pairwise label likelihood as per Equation \ref{eq:plphi}.

Given a set of $P$ pairwise labels, %for a single user, 
$\bs y=\{y_1,...,y_P\}$,
where %the $p$th label, 
$y_p=y(a_p, b_p)$ is the preference label for items $a_p$ and $b_p$, % refers to items $\{ a_p, b_p \}$.
we can write the joint distribution over all variables as follows:
\begin{flalign}
p\left( \bs{y}, \bs f, s | k_{\theta}, \bs X, \alpha_0, \beta_0 \right) 
=  \prod_{p=1}^P p( y_p | \bs f ) 
\mathcal{N}(\bs f; \bs 0, \bs K_{\theta}/s) \mathcal{G}(s; \alpha_0, \beta_0) %\nonumber \\
%=  \prod_{p=1}^P \Phi\left( z_p \right) 
%\mathcal{N}(\bs f; \bs 0, \bs K_{\theta}/s) \mathcal{G}(s; \alpha_0, \beta_0), &
\label{eq:joint_single}
\end{flalign}
where 
$\bs f = \{f(\bs {x}_1),...,f(\bs {x}_N)\}$
is a vector containing the utilities of the $N$ items referred to by $\bs y$,
and $p( y_p | \bs f ) = \Phi\left( z_p \right)$ is the pairwise likelihood (Equation \ref{eq:plphi}). 
%We henceforth refer to this model simply as \emph{GPPL}.

\subsection{Crowd Preference Learning} \label{sec:crowd_model}

To predict the preferences of individuals in a crowd,
we could use an independent GPPL model for each user.
However, by modelling all users jointly, we can
exploit correlations between their interests
to improve predictions when preference data is sparse,
and reduce the memory cost of storing separate models.
Correlations between users 
can arise from common interests over certain subsets of items,
such as in one particular genre in a book recommendation task.
Identifying such correlations helps to predict 
 preferences from  fewer observations and is the core idea of collaborative filtering~\citep{resnick1997recommender} and matrix factorisation~\citep{koren2009matrix}.

As well as individual preferences, 
we wish to predict the consensus by aggregating
preference labels from multiple users. 
Individual biases of different users may affect consensus predictions,
particularly when data for certain items comes from a small subset of users.
The consensus could also help
predict preferences of users with little or no data
 by favouring popular items
and avoiding generally poor items.
We therefore propose 
%address this problem by proposing
 \emph{crowdGPPL}, which jointly models 
the preferences of individual users as well as the underlying consensus of the crowd.
Unlike previous methods for inferring the consensus, 
such as \emph{CrowdBT}~\citep{chen2013pairwise}, we do not treat differences between users as simply the result of labelling errors, 
but also account for their subjective biases
towards particular items. 
 
% is there a better word than 'label sources' for the different sources of implicit feedback or other types of labeling?
%In a scenario with multiple users or label sources, 
For crowdGPPL, 
we represent utilities in a matrix, $\bs{F} \in \mathbb{R}^{N \times U}$,
with %$N$ rows corresponding to items and 
$U$ columns corresponding to users. 
Within $\bs F$, each entry $F_{a,j} = f(\bs x_a, \bs u_j)$ is the 
utility for item $n$ for user $j$ with user features $\bs u_j$.
We assume that $\bs{F} = \bs{V}^T \bs{W} + \bs{t}\bs{1^T}$
 is the product of two low-rank matrices
plus a vector of consensus utilities, $\bs{t} \in \mathbb{R}^N$, 
where $\bs{W} \in \mathbb{R}^{C \times U}$ is a latent representation
of the users,
$\bs{V} \in \mathbb{R}^{C \times N}$ is a latent representation of the items,
 $C$ is the number of latent \emph{components}, i.e., the dimension
of the latent representations,
and $\bs 1$ is a column vector of ones of length $U$. 
The column $\bs v_{.,a}$ of $\bs V$, and the column $\bs w_{.,j}$ of $\bs W$,
 are latent vector representations of item $a$ and user $j$,
 respectively.
%Users with similar values for a certain feature will have similar preferences for 
%the subset of items with corresponding feature values. 
Each row of $\bs V$, $\bs v_c=\{ 
v_c(\bs{x}_1),...,v_c(\bs{x}_N)\}$,  
contains evaluations of a latent function, 
$v_c\sim \mathcal{GP}(\bs 0, k_{\theta} /s^{(v)}_c)$,
of item features, $\bs x_a$,
where $k$ is a kernel function, $s^{(v)}_c$ is an inverse function scale,
and $\theta$ are kernel hyperparameters.
%Since our goal is to infer a consensus from a crowd as well as to model individual 
%users' preferences, 
The consensus utilities, $\bs t = \{t(\bs {x}_1),...,t(\bs {x}_N)\}$,
are values of a consensus utility function over item features,
$t\sim \mathcal{GP}(\bs 0, k_{\theta} /s^{(t)})$, which is shared across all users,
with inverse scale $s^{(t)}$.
Similarly, each row of $\bs W$, 
$\bs w_c=\{w_c(\bs u_1),...,w_c(\bs u_U)\}$,
 contains evaluations of a latent function,
$w_c \sim \mathcal{GP}(\bs 0, k_{\eta}/s_c^{(w)})$,
of user features, $\bs u_j$, 
with inverse scale $s_c^{(w)}$
and kernel hyperparameters $\eta$.
Therefore, each utility in $\bs F$ can be written as
a weighted sum over the latent components:
\begin{flalign}
  f(\bs x_a, \bs u_j) = \sum_{c=1}^C  v_c(\bs x_a) w_c(\bs u_j) + t(\bs x_a),
  \label{eq:vw_plus_t}
\end{flalign}
where $\bs u_j$ are the features of user $j$ and $\bs x_a$ are the features of item $a$.
Each latent component corresponds to a utility function 
for certain items, which is shared by a subset of users to differing degrees.
For example, in the case of book recommendation,
$c$ could relate to science fiction novels, 
$v_c$ to a ranking over them,
and $w_c$ to the degree of agreement of users with that ranking.
%CrowdGPPL therefore combines latent features of items and
%users -- represented by the latent components -- with the
%utilities of the items according to an underlying consensus across users.
%Given the consensus, $t$,
%utility for item $a$, $t(\bs x_a)$,
The individual preferences of each user $j$ deviate from a consensus across users, $t$, according
to $\sum_{c=1}^C  v_c(\bs x_a) w_c(\bs u_j)$. 
This allows us to subtract the effect of individual biases when inferring the consensus utilities. 
The consensus can also help 
when inferring personal preferences for %new users, 
%new items or 
new combinations of users and items that are
very different to those in the training data by
 accounting for any objective or widespread appeal that an item may have.
%We provide a Bayesian treatment to matrix factorization by placing Gaussian process priors over the latent functions.
%differently for each user and item. For example, the observed user feature 'age' may correlate with some latent interests of users, but certain users will deviate from their peer group. 
% what happens if two users have identical features (say, the feature representation
% has only simple values, such as age in years)? They have 1-1 covariance, but there 
% is variance in the GP at one location, so both can be drawn separately from the prior.
%It is not necessary to learn a separate scale for $w_c$, since $v_c$ and $w_c$ are multiplied with each other, making a single $s^{(v)}_c$ equivalent to the product of two separate scales. 
%The choice of $C$ can be treated as a hyperparameter, or modeled using a non-parametric prior, such as 
%the Indian Buffet Process, which assumes an infinite number of latent components ~\citep{ding2010nonparametric}.
%This section described a Bayesian matrix factorization model, 
%which we will subsequently extend to a preference learning model for crowds of users and label sources. 
% joint distribution
% notes about problems with inference.

Although the model assumes a fixed number of components, $C$,
the GP priors over $\bs w_c$ and $\bs v_c$ act as \emph{shrinkage}
or \emph{ARD priors} that favour values close to zero~\citep{mackay1995probable,psorakis2011overlapping}. 
Components that are not required to explain the data will have posterior
expectations and scales $1/s^{(v)}$ and $1/s^{(w)}$ approaching zero.
Therefore, %due to our choice of prior, 
it is not necessary to optimise the value of $C$ by hand, 
providing a sufficiently large number is chosen. 

Equation \ref{eq:vw_plus_t} is similar to
\emph{cross-task crowdsourcing}~\citep{mo2013cross}, which 
uses matrix factorisation to model annotator performance in different tasks,
where $\bs t$ corresponds to the objective difficulty of a task.
However, unlike crowdGPPL, they do not use GPs to model the factors, 
nor apply
the approach to preference learning.
For preference learning, collabGP~\citep{houlsby2012collaborative}
is a related model that 
excludes the consensus and uses values in $\bs v_c$ to represent pairs
 rather than individual items, so does not infer item ratings.
It also omits scale parameters for the GPs that 
encourage shrinkage when $C$ is larger than required.
 
We combine the matrix factorisation method with the preference likelihood of Equation \ref{eq:plphi}
to obtain the joint preference model for multiple users, \emph{crowdGPPL}:
%represent a consensus between users,
%if present, while allowing individual users' preferences to deviate from this value through $\bs V^T \bs W$. 
%Hence, $\bs t$ can model the underlying ground truth or consensus in crowdsourcing scenarios, or when using
%multiple label sources to learn preferences for one individual.
\begin{flalign}
&p\left( \bs{y}, \bs V, \bs W, \bs t, s^{(v)}_1 \!\!, .., s^{(v)}_C\!\!, s^{(w)}_1\!\!, .., s^{(w)}_C\!\!, s^{(t)} 
| k_{\theta}, \bs X, k_{\eta}, \bs U, \alpha_0^{(t)}\!\!, \beta_0^{(t)}\!\!,
\alpha_0^{(v)}\!\!, \beta_0^{(v)}\!\!, \alpha_0^{(w)}\!\!, \beta_0^{(w)} \right) 
 & \nonumber \\ 
& = \prod_{p=1}^P \Phi\left( z_p \right) 
\mathcal{N}(\bs t; \bs 0, \bs K_{\theta} /s^{(t)})
\mathcal{G}({s^{(t)}}; \alpha_0^{(t)}, \beta_0^{(t)})
\prod_{c=1}^C \left\{
\mathcal{N}(\bs v_c; \bs 0, \bs K_{\theta} /s^{(v)}_c)
\right.
 & \nonumber \\  
&\left.
\mathcal{N}(\bs w_c; \bs 0, \bs L_{\eta}/s^{(w)}_c) \mathcal{G}(s^{(v)}_c; \alpha_0^{(v)}, \beta_0^{(v)})\mathcal{G}(s^{(w)}_c; \alpha_0^{(w)}, \beta_0^{(w)}) \right\}, &
\label{eq:joint_crowd}
\end{flalign}
where 
$z_p = \bs v_{.,a_p}^T \bs{w}_{.,u_p} + t_{a_p} - \bs v_{.,b_p}^T \bs{w}_{.,u_p} - t_{b_p}$,
index $p$ refers to a user and a pair of items, $\{u_p, a_p, b_p \}$,
$\bs U$ is the set of feature vectors for all users,
$\bs K_{\theta}$ is the prior covariance for the items as in GPPL,
and $\bs L_{\eta}$ is the prior covariance for the users computed
using $k_{\eta}$.

