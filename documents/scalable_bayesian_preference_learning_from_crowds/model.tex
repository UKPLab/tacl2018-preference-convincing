\section{Bayesian Preference Learning for Crowds}\label{sec:model}

%%%% Notes

% Title or name of the model: 
% -- cannot decide this until we get most of the paper complete: will emphasis be on crowds? distilling ground truth
% from noisy sources (Bayesian preference learning for fusing unreliable sources)? user preferences/collaborative filtering?
% -- need a new name to differentiate from Houlsby et al. and Khan et al.?
% -- what are the differences in the model? Let's get the model written up.
% -- should also be some buzzword or word to generate interest: 
%    -- 'variational' is on the up, could be used in paper title
%    -- 'stochastic variational' is also on the up
%    -- 'crowdsourcing' on the way down as at 2010 level
%    -- 'gaussian process' on the way up
%    -- 'matrix factorization' kind of on the way up
%    -- 'scalable' on the way up
%    -- 'interactive learning' on the way down
%    -- 'preference learning' flattish, may be on way up slowly
% -- aimed at crowdsourcing problems (uses a common mean function as consensus?)
% -- other parameters for importance of features?
% -- combined preference learning? Preference aggregation? Collaborative crowdsourced preference learning? Bayesian preference learning for crowds? another word for 'multi-user' or 'many users and many items' vs. crowds?

% TO ADD: Why does the variance in f cancel out when predicting the probability of a pairwise label?
% TO ADD: Why does \sigma disappear if we learn the output scale.

% We also estimate the output scale of the GPs, the latent components, and item bias as part of the 
% variational approximation allowing us to estimate these parameters in a Bayesian manner without 
% resorting to maximum likelihood approaches.

% mention how the noise model deals with inconsistencies in preferences

% \begin{enumerate}
% \item What are the benefits of Bayesian methods and Gaussian processes in particular?
% \item The proposal by \citep{chu2005preference} shows how the advantages of a Bayesian
% approach can be exploited for preference learning by modifying the observation model

% Extensions:
% -- how do we replace the GP with a NN?
% -- would this move us from a Bayesian to an ML solution?
% -- maybe save this for future work? Or add a few lines if we can make it fit with the theme of the paper.
% -- another is to replace the fixed number of clusters with a CRP, then the whole thing can be nonparameteric preference learning with crowds.

We assume that a pair of items, $a$ and $b$, have utilities
$f(\bs x_a)$ and $f(\bs x_b)$, which represent their value to a user,
and that $f: \mathbb{R}^D \mapsto \mathbb{R}$ 
is a function of item features, where $\bs x_a$ and $\bs x_b$ are vectors 
of length $D$ containing the features of items $a$ and $b$, respectively.
If $f(\bs x_a) > f(\bs x_b)$, then $a$ is preferred to $b$ (written $a \succ b$).
The outcome of a comparison between $a$ and $b$ is 
a pairwise label, $y(a, b)$.
Assuming that pairwise labels never contain errors,
then $y(a, b)=1$ if $a \succ b$ and $0$ otherwise.
Given knowledge of $f$, we can compute the utilities 
of items in a test set given their features, and the outcomes of pairwise comparisons.

\citet{thurstone1927law} proposed the random utility model,
which relaxes the assumption that pairwise labels, $y(a, b)$,
are always consistent with the ordering of $f(\bs x_a)$ and $f(\bs x_b)$.
Under the random utility model, the likelihood $p(y(a,b)=1)$ 
increases as $f_a - f_b$ increases, i.e.,
as the utility of item $a$ increases
relative to the utility of item $b$.
This reflects the greater consistency in a user's choices
when their preferences are stronger,
while accommodating
%However, since $0 < p(y(a,b)=1) < 1$, the model 
%is uncertain about the value of $y(a,b)$,
labelling errors or variations in a user's choices over time.
%The uncertainty is lower if the values $f_a$ and $f_b$ are further apart, 
%which 
%The random utility model is defined by a likelihood function that
%maps the utilities to $p(y(a,b))$.
%or the Thurstone-Mosteller model.
%\begin{align}
%p(y(a, b) | f) & = \frac{1}{1 + \exp( f(\bs x_a) - f(\bs x_b) ) }
%\end{align}
In the Thurstone-Mosteller model, % case V model, 
noise in the observations is explained by a Gaussian-distributed noise term, $\delta \sim \mathcal{N}(0, \sigma^2)$:
\begin{flalign}
 p(y(a, b) | f(\bs x_a) + \delta_{a}, f(\bs x_b) + \delta_{b} )  
 \hspace{0.9cm} & = \begin{cases}
 1 & \text{if }f(\bs x_a) + \delta_{a} \geq f(b) + \delta_{b} \\
 0 & \text{otherwise,}
 \end{cases} &
 \label{eq:thurstone}
\end{flalign}
Integrating out the unknown values of $\delta_a$ and $\delta_b$ gives:
\begin{flalign}
& p( y(a, b) | f(\bs x_a), f(\bs x_b) )  & \label{eq:plphi}\\
& = \!\! \int\!\!\!\! \int \!\! p( y(a, b) | f(\bs x_a) + \delta_{a}, f(\bs x_b) + \delta_{b} ) \mathcal{N}(\delta_{a}; 0, \sigma^2)\mathcal{N}(\delta_{b}; 0, \sigma^2) d\delta_{a} d\delta_{b} 
%= \Phi\left(\frac{f(\bs x_a) - f(\bs x_b)}{\sqrt{2\sigma^2}}\right) 
= \Phi\left( z \right), & \nonumber
\end{flalign}
where $z = \frac{f(\bs x_a) - f(\bs x_b)}{\sqrt{2\sigma^2}}$,
and $\Phi$ is the cumulative distribution function of the standard normal distribution,
meaning that $\Phi(z)$ is a 
probit likelihood.\footnote{Please note that a full list of symbols is provided for reference in Appendix $\ref{sec:not}$}

In practice, $f(\bs x_a)$ and $f(\bs x_b)$ must be inferred from
pairwise training labels, $\bs y$,
to obtain a posterior distribution over their values.
If this posterior is a multivariate Gaussian distribution,
then the probit likelihood allows us to analytically marginalise 
$f(\bs x_a)$ and $f(\bs x_b)$
to obtain the probability of a pairwise label:
\begin{align}
p(y(a,b)| \bs y) 
= \Phi(\hat{z}),& & \hat{z} = \frac{\hat{f}_a - \hat{f}_b}{\sqrt{2\sigma^2 + C_{a,a} + C_{b,b} 
- 2C_{a,b}} }, \label{eq:predict_z}
\end{align}
where $\hat{f}_a$ and $\hat{f}_b$ are the means and
$\bs C$ is the posterior covariance matrix of the multivariate Gaussian over
$f(\bs x_a)$ and $f(\bs x_b)$.
Unlike other choices for the likelihood, such as a sigmoid,
the probit allows us to compute the posterior over a pairwise label
without further approximation, %numerical integration
hence we assume this pairwise label likelihood for our proposed preference learning model.
This likelihood is also used by
~\citet{chu2005preference} for Gaussian process preference learning (GPPL), but here 
we simplify the formulation by assuming that $\sigma^2 = 0.5$,
which leads to $z$ having a denominator of $\sqrt{2 \times 0.5}=1$.
Instead, we model varying degrees of noise in the pairwise labels
by scaling $f$ itself, as we describe in the next section.
%Obtaining the posterior over $f$ is itself challenging, however, 
%and therefore in Section $\ref{sec:inf}$ we propose 
%an approximate inference method to address this problem.


\subsection{GPPL for Single User Preference Learning}

We can model the preferences of a single user by assuming
a Gaussian process prior over the user's utility function, 
%is a function of item features and 
$f \sim \mathcal{GP}(0, k_{\theta}/s)$, where $k_{\theta}$ is a kernel function with hyperparameters $\theta$
and $s$ is an inverse scale parameter.
The kernel function takes numerical item features as inputs and determines the covariance between values of $f$ for different items. 
The choice of kernel function and its hyperparameters controls the shape and smoothness of the function 
across the feature space and is often treated as a model selection problem.
Kernel functions suitable for a wide range of tasks include the \emph{squared exponential} 
and the \emph{Mat\'ern}~\citep{rasmussen_gaussian_2006},
which both make minimal assumptions but 
assign higher covariance to items with similar feature values.
We use $k_{\theta}$ to compute a covariance matrix $\bs K_{\theta}$,
between a set of $N$ observed items with features $\bs X = \{ \bs x_1, ..., \bs x_N \}$.

Here we extend the original definition of GPPL~\citep{chu2005preference},
by introducing the inverse scale, $s$,
which is drawn from a gamma prior, 
$s \sim \mathcal{G}(\alpha_0, \beta_0)$, with shape $\alpha_0$ and scale $\beta_0$.
The value of $1/s$ determines the variance of $f$,
and therefore 
the magnitude of differences between $f(\bs x_a)$ and $f(\bs x_b)$ for
items $a$ and $b$. This in turn affects the level of certainty
in the pairwise label likelihood as per Equation \ref{eq:plphi}.

Given a set of $P$ pairwise labels, %for a single user, 
$\bs y=\{y_1,...,y_P\}$,
where %the $p$th label, 
$y_p=y(a_p, b_p)$ is the preference label for items $a_p$ and $b_p$, % refers to items $\{ a_p, b_p \}$.
we can write the joint distribution over all variables as follows:
\begin{flalign}
p\left( \bs{y}, \bs f, s | k_{\theta}, \bs X, \alpha_0, \beta_0 \right) 
=  \prod_{p=1}^P p( y_p | \bs f ) 
\mathcal{N}(\bs f; \bs 0, \bs K_{\theta}/s) \mathcal{G}(s; \alpha_0, \beta_0) %\nonumber \\
%=  \prod_{p=1}^P \Phi\left( z_p \right) 
%\mathcal{N}(\bs f; \bs 0, \bs K_{\theta}/s) \mathcal{G}(s; \alpha_0, \beta_0), &
\label{eq:joint_single}
\end{flalign}
where 
$\bs f = \{f(\bs {x}_1),...,f(\bs {x}_N)\}$
is a vector containing the utilities of the $N$ items referred to by $\bs y$,
and $p( y_p | \bs f ) = \Phi\left( z_p \right)$ is the pairwise likelihood (Equation \ref{eq:plphi}). 
%We henceforth refer to this model simply as \emph{GPPL}.

\subsection{Crowd Preference Learning} \label{sec:crowd_model}

To predict the preferences of individuals in a crowd,
we could use an independent GPPL model for each user.
However, by modelling all users jointly, we can
exploit correlations between their interests
to improve predictions when preference data is sparse,
and reduce the memory cost of storing separate models.
Correlations between users 
can arise from common interests over certain subsets of items,
such as in one particular genre in a book recommendation task.
Identifying such correlations helps to predict 
 preferences from  fewer observations and is the core idea of collaborative filtering~\citep{resnick1997recommender} and matrix factorisation~\citep{koren2009matrix}.

As well as individual preferences, 
we wish to predict the consensus by aggregating
preference labels from multiple users. 
Individual biases of different users may affect consensus predictions,
particularly when data for certain items comes from a small subset of users.
The consensus could also help
predict preferences of users with little or no data
 by favouring items popular items
and avoiding generally poor items.
We therefore propose 
%address this problem by proposing
 \emph{crowdGPPL}, which jointly models 
the preferences of individual users as well as the underlying consensus of the crowd.
Unlike previous methods for inferring the consensus, 
such as \emph{CrowdBT}~\citep{chen2013pairwise}, we do not treat differences between users as simply the result of labelling errors, 
but also account for their subjective biases
towards particular items. 
 
% is there a better word than 'label sources' for the different sources of implicit feedback or other types of labeling?
%In a scenario with multiple users or label sources, 
For crowdGPPL, 
we represent utilities in a matrix, $\bs{F} \in \mathbb{R}^{N \times U}$,
with %$N$ rows corresponding to items and 
$U$ columns corresponding to users. %, and entries are preference values.
We assume that $\bs{F} = \bs{V}^T \bs{W} + \bs{t}$
 is the product of two low-rank matrices
plus a vector of consensus utilities, $\bs{t} \in \mathbb{R}^N$, 
where $\bs{W} \in \mathbb{R}^{C \times U}$ is a latent representation
of the users,
$\bs{V} \in \mathbb{R}^{C \times N}$ is a latent representation of the items,
and $C$ is the number of latent \emph{components}, i.e., the dimension
of the latent representations.
The column $\bs v_{.,a}$ of $\bs V$, and the column $\bs w_{.,j}$ of $\bs W$,
 are latent vector representations of item $a$ and user $j$,
 respectively.
%Users with similar values for a certain feature will have similar preferences for 
%the subset of items with corresponding feature values. 
Each row of $\bs V$, $\bs v_c=\{ 
v_c(\bs{x}_1),...,v_c(\bs{x}_N)\}$,  
contains evaluations of a latent function, 
$v_c\sim \mathcal{GP}(\bs 0, k_{\theta} /s^{(v)}_c)$,
of item features, $\bs x_a$,
where $k$ is a kernel function, $s^{(v)}_c$ is an inverse function scale,
and $\theta$ are kernel hyperparameters.
%Since our goal is to infer a consensus from a crowd as well as to model individual 
%users' preferences, 
The consensus utilities, $\bs t = \{t(\bs {x}_1),...,t(\bs {x}_N)\}$,
are values of a consensus utility function over item features,
$t\sim \mathcal{GP}(\bs 0, k_{\theta} /s^{(t)})$, which is shared across all users,
with inverse scale $s^{(t)}$.
Similarly, each row of $\bs W$, 
$\bs w_c=\{w_c(\bs u_1),...,w_c(\bs u_U)\}$,
 contains evaluations of a latent function,
$w_c \sim \mathcal{GP}(\bs 0, k_{\eta}/s_c^{(w)})$,
of user features, $\bs u_j$, 
with inverse scale $s_c^{(w)}$
and kernel hyperparameters $\eta$.
Therefore, each utility in $\bs F$ can be written as
a weighted sum over the latent components:
\begin{flalign}
  f(\bs x_a, \bs u_j) = \sum_{c=1}^C  v_c(\bs x_a) w_c(\bs u_j) + t(\bs x_a),
  \label{eq:vw_plus_t}
\end{flalign}
where $\bs u_j$ are the features of user $j$ and $\bs x_a$ are the features of item $a$.
Each latent component corresponds to a utility function 
for certain items, which is shared by a subset of users to differing degrees.
For example, in the case of book recommendation,
$c$ could relate to science fiction novels, 
$v_c$ to a ranking over them,
and $w_c$ to the degree of agreement of users with that ranking.
%CrowdGPPL therefore combines latent features of items and
%users -- represented by the latent components -- with the
%utilities of the items according to an underlying consensus across users.
%Given the consensus, $t$,
%utility for item $a$, $t(\bs x_a)$,
The individual preferences of each user $j$ deviate from a consensus across users, $t$, according
to $\sum_{c=1}^C  v_c(\bs x_a) w_c(\bs u_j)$. 
This allows us to subtract the effect of individual biases when inferring the consensus utilities. 
The consensus can also help 
when inferring personal preferences for %new users, 
%new items or 
new combinations of users and items that are
very different to those in the training data by
 accounting for any objective or widespread appeal that an item may have.
%We provide a Bayesian treatment to matrix factorization by placing Gaussian process priors over the latent functions.
%differently for each user and item. For example, the observed user feature 'age' may correlate with some latent interests of users, but certain users will deviate from their peer group. 
% what happens if two users have identical features (say, the feature representation
% has only simple values, such as age in years)? They have 1-1 covariance, but there 
% is variance in the GP at one location, so both can be drawn separately from the prior.
%It is not necessary to learn a separate scale for $w_c$, since $v_c$ and $w_c$ are multiplied with each other, making a single $s^{(v)}_c$ equivalent to the product of two separate scales. 
%The choice of $C$ can be treated as a hyperparameter, or modeled using a non-parametric prior, such as 
%the Indian Buffet Process, which assumes an infinite number of latent components ~\citep{ding2010nonparametric}.
%This section described a Bayesian matrix factorization model, 
%which we will subsequently extend to a preference learning model for crowds of users and label sources. 
% joint distribution
% notes about problems with inference.

Although the model assumes a fixed number of components, $C$,
the GP priors over $\bs w_c$ and $\bs v_c$ act as \emph{shrinkage}
or \emph{ARD priors} that favour values close to zero~\citep{mackay1995probable,psorakis2011overlapping}. 
Components that are not required to explain the data will have posterior
expectations and scales $1/s^{(v)}$ and $1/s^{(w)}$ approaching zero.
Therefore, %due to our choice of prior, 
it is not necessary to optimise the value of $C$, providing a sufficiently large number is chosen. 

Equation \ref{eq:vw_plus_t} is similar to
\emph{cross-task crowdsourcing}~\citep{mo2013cross}, which 
uses matrix factorisation to model annotator performance in different tasks,
where $\bs t$ corresponds to the objective difficulty of a task.
However, unlike crowdGPPL, they do not use GPs to model the factors, 
nor apply
the approach to preference learning.
For preference learning, collabGP~\citep{houlsby2012collaborative}
is a related model that 
excludes the consensus and uses values in $\bs v_c$ to represent pairs
 rather than individual items, so does not infer item ratings.
It also omits scale parameters for the GPs that 
encourage shrinkage when $C$ is larger than required.
 
We combine the matrix factorisation method with the preference likelihood of Equation \ref{eq:plphi}
to obtain the joint preference model for multiple users, \emph{crowdGPPL}:
%represent a consensus between users,
%if present, while allowing individual users' preferences to deviate from this value through $\bs V^T \bs W$. 
%Hence, $\bs t$ can model the underlying ground truth or consensus in crowdsourcing scenarios, or when using
%multiple label sources to learn preferences for one individual.
\begin{flalign}
&p\left( \bs{y}, \bs V, \bs W, \bs t, s^{(v)}_1 \!\!, .., s^{(v)}_C\!\!, s^{(w)}_1\!\!, .., s^{(w)}_C\!\!, s^{(t)} 
| k_{\theta}, \bs X, k_{\eta}, \bs U, \alpha_0^{(t)}\!\!, \beta_0^{(t)}\!\!,
\alpha_0^{(v)}\!\!, \beta_0^{(v)}\!\!, \alpha_0^{(w)}\!\!, \beta_0^{(w)} \right) 
 & \nonumber \\ 
& = \prod_{p=1}^P \Phi\left( z_p \right) 
\mathcal{N}(\bs t; \bs 0, \bs K_{\theta} /s^{(t)})
\mathcal{G}({s^{(t)}}; \alpha_0^{(t)}, \beta_0^{(t)})
\prod_{c=1}^C \left\{
\mathcal{N}(\bs v_c; \bs 0, \bs K_{\theta} /s^{(v)}_c)
\right.
 & \nonumber \\  
&\left.
\mathcal{N}(\bs w_c; \bs 0, \bs L_{\eta}/s^{(w)}_c) \mathcal{G}(s^{(v)}_c; \alpha_0^{(v)}, \beta_0^{(v)})\mathcal{G}(s^{(w)}_c; \alpha_0^{(w)}, \beta_0^{(w)}) \right\}, &
\label{eq:joint_crowd}
\end{flalign}
where 
$z_p = \bs v_{.,a_p}^T \bs{w}_{.,u_p} + t_{a_p} - \bs v_{.,b_p}^T \bs{w}_{.,u_p} - t_{b_p}$,
index $p$ refers to a tuple $\{u_p, a_p, b_p \}$ that identifies a user and a pair of items,
$\bs U$ is the set of feature vectors for all users,
and $\bs L_{\eta}$ is the prior covariance matrix for the users computed
using $k_{\eta}$.

