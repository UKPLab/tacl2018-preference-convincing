\RequirePackage[fleqn]{amsmath}
\RequirePackage{fix-cm}
%
\documentclass[smallcondensed,natbib]{svjour3}     % onecolumn (ditto)
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}

%\usepackage{mathptmx} % this causes problems with bold font
\usepackage{newtxtext,newtxmath}

\usepackage{url}
\makeatletter
\makeatother
%\usepackage[hidelinks]{hyperref}
\DeclareMathOperator*{\argmax}{arg\,max}

\usepackage[fleqn]{amsmath}
%\usepackage{amssymb}
% \usepackage{amstext}
% \usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{algorithm2e}
\usepackage{array}
\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{url}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{numprint}
\usepackage{multirow}
\usepackage{xcolor}
%\usepackage{todonotes}

\newcommand{\bs}{\boldsymbol}
\newcommand{\wrtd}{\mathrm{d}}

\makeatletter
\makeatother %some sort of hack related to the symbol @

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{ 
%Predicting Preferences for Crowds of Users: a Scalable, Bayesian Approach
Scalable Bayesian Preference Learning for Crowds
% title change to emphasise that this is a personalised model, not all about aggregation
%Scalable Bayesian Methods for Personalised Preference Learning
%One source of confusion seems to be the inclusion of features, which is a big distinction.
% predictive feature-based models.
% ... with item and user features
% predictive modelling with input features
%Learning predictive preference models from crowds of users: a scalable Bayesian method
% Predicting preferences of crowds of users: a scalable, Bayesian approach
}

\author{Edwin Simpson 
\and Iryna Gurevych
}

\institute{
Edwin Simpson \and Iryna Gurevych \at
Ubiquitous Knowledge Processing Lab, Dept. of Computer Science, Technische Universit\"at Darmstadt.\\
              \email{\{simpson,gurevych\}@ukp.informatik.tu-darmstadt.de}
}

\date{Received: date}

\begin{document}

\titlerunning{Scalable Bayesian Preference Learning}
\authorrunning{Simpson, E and Gurevych, I}

% do not exceed 20 pages including references

\maketitle

\begin{abstract}
We propose a scalable Bayesian preference learning method 
for jointly predicting the preferences of individuals as well as the consensus of a crowd
 from pairwise labels.
Peoples' opinions often differ greatly,
making it difficult to predict their preferences from small amounts of personal data.
Individual biases also make it harder to infer the consensus of a crowd
when there are few labels per item.
%In applications such as recommendation it is also necessary to predict the preferences of individual annotators or users. 
%and identify common preferences between users. % use of the latent factors 
%We address these challenges by combining matrix factorization to model individual preferences with 
%Gaussian processes to integrate user and item features. By taking a Bayesian approach, our model
We address these challenges by combining matrix factorisation with 
Gaussian processes,
using a Bayesian approach to account for uncertainty arising from noisy and sparse data.
Our method exploits input features, such as text embeddings and user metadata,
to predict preferences for new items and users that are not in the training set.
As previous solutions based on Gaussian processes do not scale to 
large numbers of users, items or pairwise labels, 
we propose a stochastic variational inference approach that limits computational and memory costs.
Our experiments on a recommendation task show that
our method is competitive with previous approaches despite our scalable inference approximation.
We demonstrate the method's scalability on a natural language processing task 
with thousands of users and items, and show 
improvements over the state of the art on this task.
%by modelling the preferences of individual members of the crowd.
%We also show how to  %that robustness to %able to learn the effective number of components required to model the data and
%choosing more latent components than required,
%apply gradient-based optimization to length-scale hyper-parameters to improve performance.
We make our software publicly available for future 
work~\footnote{\url{https://github.com/UKPLab/tacl2018-preference-convincing/tree/crowdGPPL}}.
%We show how to make collaborative preference learning work at scale and how it can be used to learn
%a target preference function from crowdsourced data or other noisy preference labels. 
%The collaborative model captures the reliability of each worker or data source and models their biases and error rates. 
%It uses latent factors to share information between similar workers and a target preference function.
%We devise an SVI inference schema to enable the model to scale to real-world datasets.
%Experiments compare results using standard variational inference, laplace approximation and SVI.
%On real-world data we show the benefit of the personalised model over a GP preference learning approach 
%that treats all labels as coming from the same source,
%as well as established alternative methods and classifier baselines.
%We show that the model is able to identify a number of latent features for the workers and for textual arguments.
\end{abstract}

% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
%\IEEEpeerreviewmaketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{intro}
\input{related_work}
\input{model}
\input{inference}
%\input{hyperparameters}
\input{experiments}
\input{discussion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% use section* for acknowledgment
\section*{Acknowledgments}

This work was supported by the German Federal Ministry of Education and Research (BMBF) 
under promotional references 01UG1416B (CEDIFOR), 16DHL1040 (FAMULUS) and 03VP02540 (Ar-
gumenText),
and by the German Research Foundation (DFG) as part of 
the QA-EduInf project (grant GU 798/18-1 and grant RI 803/12-1), 
the German-Israeli Project Cooperation (DIP, grant DA1600/1-1 and grant GU 798/17-1), 
and the EVIDENCE project (grant GU 798/27-1). 
It also received funding
from the European Unions Horizon 2020 research
and innovation programme (H2020-EINFRA-2014-
2) under grant agreement No. 654021 (Open-
MinTeD). It reflects only the authors views and the
EU is not liable for any use that may be made of
the information contained therein. We would like to
thank the journal editors and reviewers for their valuable feedback.

\bibliographystyle{spbasic}
\bibliography{simpson_scalable_bayesian_pref_learning_from_crowds}

\appendix
\input{appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
