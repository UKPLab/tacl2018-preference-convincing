\section{Posterior Inference}

Due to the non-Gaussian likelihood, Equation \ref{eq:plphi},
the posterior distribution over $\bs f$ contains intractable integrals:
\begin{flalign}
p(\bs f | \bs y, k_{\theta}, \alpha_0, \beta_0) = 
\frac{\int \prod_{p=1}^P \Phi(z_p) \mathcal{N}(\bs f; \bs 0, \bs K_{\theta}/s) 
\mathcal{G}(s; \alpha_0, \beta_0) d s}{\int \int \prod_{p=1}^P \Phi(z_p) \mathcal{N}(\bs f'; \bs 0, \bs K_{\theta}/s) 
\mathcal{G}(s; \alpha_0, \beta_0) d s d f' }.
\label{eq:post_single}
\end{flalign}
To simplify the integral in the denominator,
 we approximate the preference likelihood with a Gaussian:
\begin{flalign}
\prod_{p=1}^P \Phi(z_p) \approx \mathcal{N}(\bs y; \Phi(\bs z), \bs Q),
\label{eq:likelihood_approx}
\end{flalign}
where $\bs z=\{z_1, ..., z_P\}$
and $\bs Q$ is a diagonal noise covariance matrix.

\section{Variational Inference}
\label{sec:vb_eqns}

\begin{flalign}
\log q(\bs f_m) &= \log \mathcal{N}\left(\bs y; \tilde{\Phi}(\bs z), \bs Q\right)]
+ \log\mathcal{N}\left(\bs f_m; \bs 0, \bs K_{mm}/\mathbb{E}\left[s\right]\right)  + \textrm{const}, \nonumber \\
%&= \log \int \mathcal{N}(\bs y - 0.5; \bs G \bs f, \bs Q) 
%\mathcal{N}(\bs f; \bs A \bs f_m, \bs K - \bs A \bs K_{nm}^T) & \nonumber\\
%& \hspace{3.2cm} \mathcal{N}(\bs f_m; \bs 0, \bs K_{mm}\mathbb{E}[1/s]) \textrm{d} \bs f + \textrm{const} & \nonumber\\
 & = \log \mathcal{N}(\bs f_m; \hat{\bs f}_m, \bs S ), \\
\bs S^{-1} &= \bs K^{-1}_{mm}/\mathbb{E}[s] + \bs A^T \bs G^T \bs Q^{-1} \bs G \bs A, \label{eq:S}\\
\hat{\bs f}_m &= \bs S \bs A^T \bs G^T \bs Q^{-1} (\bs y - \Phi(\mathbb{E}[\bs z]) + \bs G \mathbb{E}[\bs f] ), \label{eq:fhat_m}
\end{flalign}

We derive the variational lower bound as follows:
\begin{flalign}
& \mathcal{L}(q) = \sum_{i=1}^{L} \mathbb{E}_{q}\left[ \log p\left( v_i \succ u_i | f(v_i), f(u_i) \right) \right] \nonumber&&\\
& + \mathbb{E}_{q}\left[ \log \frac{p\left( \bs f | \bs\mu, \bs K/s \right)}{q\left(\bs f\right)} \right]
+ \mathbb{E}_{q}\left[ \log \frac{p\left( s | a_0, b_0\right)}{q\left(s \right)} \right] 
\label{eq:vblb}
\end{flalign}
Substituting the forms of the distributions with their variational parameters, we get:
\begin{flalign}
\mathcal{L}(q) = & \mathbb{E}_{q}\Bigg[ \sum_{i=1}^{L} [v_i \succ u_i]\log\Phi(z_i) && \nonumber\\
& + [v_i \prec u_i]\left(1-\log\Phi(z_i)\right) \Bigg] \nonumber&&\\
& + \log \mathcal{N}\left(\hat{\bs f}; \bs\mu, \bs K/\hat{s} \right) 
- \log\mathcal{N}\left(\hat{\bs f}; \hat{\bs f}, \bs C \right) \nonumber&&\\
& + \mathbb{E}_{q}\left[ \log\mathcal{G}\left( s; a_0, b_0\right) - \log\mathcal{G}\left(s; a, b \right) \right]  &&
\end{flalign}
We now replace the likelihood with a Gaussian approximation:
\begin{flalign}
\mathcal{L}(q) & \approx  \mathbb{E}_{q}\left[ \mathcal{N}( \bs y | \Phi(\bs z), \bs Q) \right]
&\nonumber\\
& + \log \mathcal{N}\left(\bs f; \bs\mu, \bs K/\hat{s} \right) - \log\mathcal{N}\left(\bs f; \hat{\bs f}, \bs C \right) 
&\nonumber\\
& + \mathbb{E}_q\left[ \log\mathcal{G}\left( s; a_0, b_0\right) - \log\mathcal{G}\left(s; a, b \right) \right] \nonumber&\\
& \approx  - \frac{1}{2} \left\{ L \log 2\pi + \log |\bs Q| - \log|\bs C| \right. \nonumber&\\
& \left. + \log|\bs K/s| + (\hat{\bs f} - \bs\mu)\hat{s}\bs K^{-1}
(\hat{\bs f} - \bs\mu) \right. \nonumber&\\
& \left. + \mathbb{E}_q\left[ (\bs y - \Phi(\bs z))^T \bs Q^{-1} (\bs y - \Phi(\bs z)) \right] \right\} \nonumber&\\
& - \Gamma(a_0) + a_0(\log b_0) + (a_0-a)\mathbb{E}[\log s] \nonumber&\\
& + \Gamma(a) + (b-b_0) \hat{s} - a \log b  &
\end{flalign}
Finally, we use a Taylor-series linearisation to make the remaining expectation tractable:
\begin{flalign}
\mathcal{L}(q) & \approx - \frac{1}{2} \left\{ L \log 2\pi + \log |\bs Q| - \log|\bs C| \right. \nonumber&&\\
& \left. + \log|\bs K/\hat{s}| + (\hat{\bs f} - \bs\mu)\hat{s}\bs K^{-1}(\hat{\bs f} - \bs\mu) \right. \nonumber&&\\
& \left. + (\bs y - \Phi(\hat{\bs z}))^T \bs Q^{-1} (\bs y - \Phi(\hat{\bs z}))\right\} \nonumber&&\\
& - \Gamma(a_0) + a_0(\log b_0) + (a_0-a)\mathbb{E}[\log s] \nonumber&&\\
& + \Gamma(a) + (b-b_0) \hat{s} - a \log b, &&
\label{eq:vblb_terms}
\end{flalign}
where $\Gamma()$ is the gamma function, 
$\mathbb{E}[\log s] = \Psi(a) - \log(b)$, and $\Psi()$ is the digamma function.

\begin{flalign}
\mathcal{L} =\; & \mathbb{E}_{q(\bs f, \bs f_m, s)}[\log p(\bs y | \bs f) + \log p(\bs f_m, s | \bs K, 
\alpha_0, \beta_0) -\log q(\bs f_m) - \log q(s) ] & \nonumber \\ \label{eq:lowerbound}
=\; & \sum_{p=1}^P \mathbb{E}_{q(\bs f)}[\log p(y_p | f_{a_p}, f_{b_p})] - \frac{1}{2} \bigg\{ \log|\bs K_{mm}| - \mathbb{E}[\log s] - \log|\bs S| - M
\nonumber &\\
& + \hat{\bs f}_m^T\mathbb{E}[s] \bs K_{mm}^{-1}\hat{\bs f}_m + 
\textrm{tr}(\mathbb{E}[s] \bs K_{mm}^{-1} \bs S) \bigg\}  + \log\Gamma(\alpha) - \log\Gamma(\alpha_0)  + \alpha_0(\log \beta_0) \nonumber\\
& + (\alpha_0-\alpha)\mathbb{E}[\log s]+ (\beta-\beta_0) \mathbb{E}[s] - \alpha \log \beta, &
\label{eq:full_L_singleuser}
\end{flalign}
where the terms relating to $\mathbb{E}[p(\bs f | \bs f_m) - q(\bs f)]$ cancel and we compute as follows:
\begin{flalign}
\mathbb{E}[s] & = \frac{2\alpha_0 + M}{2\beta} \label{eq:Es}\\
\mathbb{E}[\log s] & = \Psi(2\alpha_0 + M) - \log(2\beta), \label{eq:Elogs}
\end{flalign}
where $\Psi$ is the digamma function.

The gradient of $\mathcal{L}(q)$ with respect to the lengthscale, $l_d$, is as follows:
\begin{flalign}
\nabla_{l_d} \mathcal{L}(q) & =  - \frac{1}{2} \left\lbrace 
\frac{\partial \log|\bs K/\hat{s}|}{\partial l_d} - \frac{\partial \log|\bs C|}{\partial l_d} 
\nonumber \right. \\
& \left.  - (\hat{\bs f}-\bs\mu)\hat{s} \frac{\partial K^{-1}}{\partial l_d} (\hat{\bs f}-\bs\mu)
\right\rbrace \nonumber & \\
%& = \frac{1}{2} \hat{s} \left\lbrace \frac{\partial \log |\bs C \bs K^{-1}|}{\partial l_d}
%\right. \\
%& \left.  - (\hat{\bs f}-\bs\mu) \bs K^{-1} \frac{\partial \bs K}{\partial l_d} \bs K^{-1} (\hat{\bs f}-\bs\mu)
%\right\rbrace  \nonumber \\
& =  -\frac{1}{2} \left\lbrace  \frac{\partial \log | \frac{1}{\hat{s}}\bs K \bs C^{-1} |}{\partial l_d} \right. \nonumber & \\
& \left.  + \hat{s} (\hat{\bs f}-\bs\mu) \bs K^{-1} \frac{\partial \bs K}{\partial l_d} \bs K^{-1} (\hat{\bs f}-\bs\mu)
\right\rbrace   &
%& =  - \frac{1}{2} \left\lbrace \frac{\partial \log|\bs K/s| }{\partial l_d} + \frac{\partial \log |\bs K^{-1}s + \bs G\bs Q^{-1}\bs G^T|}{\partial l_d}
%\right. \\
%& \left.  - \hat{s} (\hat{\bs f}-\bs\mu) \bs K^{-1} \frac{\partial \bs K}{\partial l_d} \bs K^{-1} (\hat{\bs f}-\bs\mu)
%\right\rbrace  \nonumber\\
%& =  -\frac{1}{2} \left\lbrace \frac{\partial \log |\bs I + \bs K/s\bs G\bs Q^{-1}\bs G^T|}{\partial l_d}
%\right. \\
%& \left.  - \hat{s} (\hat{\bs f}-\bs\mu) \bs K^{-1} \frac{\partial \bs K}{\partial l_d} \bs K^{-1} (\hat{\bs f}-\bs\mu)
%\right\rbrace  \nonumber
\end{flalign}
Using the fact that $\log | A | = \mathrm{tr}(\log A)$, $\bs C = \left[\bs K^{-1} - \bs G \bs Q^{-1} \bs G^T \right]^{-1}$, and $\bs C = \bs C^{T}$, we obtain:
\begin{flalign}
& =  -\frac{1}{2} \mathrm{tr}\left(\left(\hat{s}\bs K^{-1}\bs C\right) \bs G\bs Q^{-1}\bs G^T \frac{\partial \bs K}{\partial l_d}
\right) \nonumber \\
& + \frac{1}{2}\hat{s} (\hat{\bs f}-\bs\mu) \bs K^{-1} \frac{\partial \bs K}{\partial l_d} \bs K^{-1} (\hat{\bs f}-\bs\mu)  \nonumber\\ 
& =  -\frac{1}{2} \mathrm{tr}\left(\left(\hat{s}\bs K^{-1}\bs C\right)
\left(\bs C^{-1} - \bs K^{-1}/\hat{s}\right) \frac{\partial \bs K}{\partial l_d}
\right) \nonumber \\
& + \frac{1}{2}\hat{s} (\hat{\bs f}-\bs\mu) \bs K^{-1} \frac{\partial \bs K}{\partial l_d} \bs K^{-1} (\hat{\bs f}-\bs\mu).  \label{eq:gradient_ls}
\end{flalign}
Assuming a product over kernels for each feature, $\bs K=\prod_{d=1}^{D} \bs K_d$, we can compute the kernel gradient 
as follows for the Mat\'ern $\frac{3}{2}$ kernel function:
\begin{flalign}
\frac{\partial \bs K}{\partial l_d} & = \prod_{d'=1,d'\neq d}^D K_{d} \frac{\partial K_{l_d}}{\partial l_d} \\
\frac{\partial K_{l_d}}{\partial l_d} & = \frac{3\bs |\bs x_d - \bs x_d'|^2}{l_d^3} \exp\left( - \frac{\sqrt{3} \bs |\bs x_d - \bs x_d'|}{l_d} \right)
\label{eq:kernel_der}
\end{flalign}
where $|\bs x_d - \bs x_d'|$ is the distance between input points.
