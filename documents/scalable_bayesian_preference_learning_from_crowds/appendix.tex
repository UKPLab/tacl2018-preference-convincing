
\section{Variational Lower Bound for GPPL}
\label{sec:vb_eqns}

Due to the non-Gaussian likelihood, Equation \ref{eq:plphi},
the posterior distribution over $\bs f$ contains intractable integrals:
\begin{flalign}
p(\bs f | \bs y, k_{\theta}, \alpha_0, \alpha_0) = 
\frac{\int \prod_{p=1}^P \Phi(z_p) \mathcal{N}(\bs f; \bs 0, \bs K_{\theta}/s) 
\mathcal{G}(s; \alpha_0, \beta_0) d s}{\int \int \prod_{p=1}^P \Phi(z_p) \mathcal{N}(\bs f'; \bs 0, \bs K_{\theta}/s) 
\mathcal{G}(s; \alpha_0, \beta_0) d s d f' }.
\label{eq:post_single}
\end{flalign}
We can derive a variational lower bound as follows, beginning with an approximation that does not use inducing points:
\begin{flalign}
\mathcal{L} = \sum_{p=1}^{P} \mathbb{E}_{q(\bs f)}\!\left[ \ln p\left( y_p| f(\bs x_{a_p}), f(\bs x_{b_p}) \right) \right]
\!+ \mathbb{E}_{q(\bs f),q(s)}\!\left[ \ln \frac{p\left( \bs f | \bs 0, \frac{\bs K}{s} \right)}
{q\left(\bs f\right)} \right] 
\!+ \mathbb{E}_{q(s)}\!\left[ \ln \frac{p\left( s | \alpha_0, \beta_0\right)}{q\left(s \right)} \right] &&
\label{eq:vblb}
\end{flalign}
Writing out the expectations in terms of the variational parameters, we get:
\begin{flalign}
\mathcal{L} = &\; \mathbb{E}_{q(\bs f)}\Bigg[ \sum_{p=1}^{P} y_p \ln\Phi(z_p) + (1-y_p) \left(1-\ln\Phi(z_p)\right) \Bigg] 
+ \mathbb{E}_{q(\bs f)}\left[\ln \mathcal{N}\left(\hat{\bs f}; \bs\mu, \bs K/\mathbb{E}[s] \right) \right]
\nonumber\\
& 
- \mathbb{E}_{q(\bs f}\left[\ln\mathcal{N}\left(\bs f; \hat{\bs f}, \bs C \right) \right]
 + \mathbb{E}_{q(s)}\left[ \ln\mathcal{G}\left( s; \alpha_0, \beta_0\right) - \ln\mathcal{G}\left(s; \alpha, \beta \right) \right]  
  \nonumber \\
 =&\;  \sum_{p=1}^{P} y_p \mathbb{E}_{q(\bs f)}[
\ln\Phi(z_p) ]+ (1-y_p) \left(1-\mathbb{E}_{q(\bs f)}[\ln\Phi(z_p)] \right) \Bigg]  \nonumber\\
 & -\frac{1}{2}\left\{
 %N \ln 2\pi + 
 \ln | \bs K | - \mathbb{E}[\ln s] + \mathrm{tr}\left( \left(\hat{\bs f}^T \hat{\bs f} + \bs C\right)\bs K^{-1} \right)
% - N \ln 2\pi 
- \ln |\bs C| - N
 \right\}  \nonumber \\
 & - \Gamma(\alpha_0) + \alpha_0(\ln \beta_0) + (\alpha_0-\alpha)\mathbb{E}[\ln s] + \Gamma(\alpha) + (\beta-\beta_0) \mathbb{E}[s] - \alpha \ln \beta. 
\end{flalign}
The expectation over the likelihood can be computed using numerical integration. 
%I can no longer follow this... I think that the expectation containing Phi(z) looks dodgy as there should be a term
%relating to the variance of z. 
%We compute this by observing that the probit likelihood can be written as 
%a product of two terms:
%\begin{flalign}
%\mathcal{L} 
% =&\; \mathbb{E}_{q(\bs f)}\Bigg[ \sum_{p=1}^{P} y_p \ln\Phi(z_p) + y(b_p,a_p) \left(1-\ln\Phi(z_p)\right) \Bigg] \nonumber\\
% & -\frac{1}{2}\left\{
% \ln | \bs K | - \mathbb{E}[\ln s] + \mathrm{tr}\left( \left(\hat{\bs f}^T \hat{\bs f} + \bs C\right)\bs K^{-1} \right)
%- \ln |\bs C| - N
% \right\}  \nonumber \\
% & - \Gamma(\alpha_0) + \alpha_0(\ln \beta_0) + (\alpha_0-\alpha)\mathbb{E}[\ln s] + \Gamma(\alpha) + (\beta-\beta_0) \mathbb{E}[s] - \alpha \ln \beta. \\
%\end{flalign}
%
%We now replace the likelihood with a Gaussian approximation:
%\begin{flalign}
%\mathcal{L}_1 \approx \mathcal{L}_2 & = \mathbb{E}_{q(\bs f)}\left[ \mathcal{N}( \bs y | \Phi(\bs z), \bs Q) \right]
% + \ln \mathcal{N}\left(\hat{\bs f}; \bs\mu, \bs K/\mathbb{E}[s] \right) - \ln\mathcal{N}\left(\hat{\bs f}; \hat{\bs f}, \bs C \right) 
%&\nonumber\\
%& + \mathbb{E}_{q(s)}\left[ \ln\mathcal{G}\left( s; \alpha_0, \beta_0\right) - \ln\mathcal{G}\left(s; \alpha, \beta \right) \right] \nonumber&\\
%& =  - \frac{1}{2} \left\{ L \ln 2\pi + \ln |\bs Q| - \ln|\bs C| 
% + \ln|\bs K| - \mathbb{E}[\ln s] + (\hat{\bs f} - \bs\mu)\mathbb{E}[s]\bs K^{-1}
%(\hat{\bs f} - \bs\mu) \right. \nonumber &\\
%& \left. + \mathbb{E}_{q(\bs f)}\left[ (\bs y - \Phi(\bs z))^T \bs Q^{-1} (\bs y - \Phi(\bs z)) \right] \right\}
% - \Gamma(\alpha_0) + \alpha_0(\ln \beta_0) + (\alpha_0-\alpha)\mathbb{E}[\ln s] \nonumber&\\
%& + \Gamma(\alpha) + (\beta-\beta_0) \mathbb{E}[s] - \alpha \ln \beta,  &
%\end{flalign}
%where $\mathbb{E}[s] = \frac{\alpha}{\beta}$, $\mathbb{E}[\ln s] = \Psi(2\alpha) - \ln(2\beta)$,
%$\Psi$ is the digamma function and $\Gamma()$ is the gamma function, 
%Finally, we use a Taylor-series linearisation to make the remaining expectation tractable:
%\begin{flalign}
%\mathcal{L}_2 & \approx \mathcal{L}_3 = - \frac{1}{2} \left\{ L \ln 2\pi + \ln |\bs Q| - \ln|\bs C| \right.
% \left. + \ln|\bs K/\mathbb{E}[s]| + (\hat{\bs f} - \bs\mu)\mathbb{E}[s]\bs K^{-1}(\hat{\bs f} - \bs\mu) \right. \nonumber&&\\
% & \left. + (\bs y - \Phi(\hat{\bs z}))^T \bs Q^{-1} (\bs y - \Phi(\hat{\bs z}))\right\}
% - \Gamma(\alpha_0) + \alpha_0(\ln \beta_0) + (\alpha_0-\alpha)\mathbb{E}[\ln s] \nonumber&&\\
%& + \Gamma(\alpha) + (\beta-\beta_0) \mathbb{E}[s] - \alpha \ln \beta. &&
%\label{eq:vblb_terms} 
%\end{flalign}
Now we can introduce the sparse approximation to obtain the bound in Equation \ref{eq:lowerbound}:
\begin{flalign}
\mathcal{L} \approx \; & \mathbb{E}_{q(\bs f)}[\ln p(\bs y | \bs f)]
 + \mathbb{E}_{q(\bs f_m), q(s)}[\ln p(\bs f_m, s | \bs K, 
\alpha_0, \beta_0)] - \mathbb{E}_{q(\bs f_m)}[\ln q(\bs f_m)] 
- \mathbb{E}_{q(s)}[\ln q(s) ] & \nonumber \\ 
=\; & \sum_{p=1}^P \mathbb{E}_{q(\bs f)}[\ln p(y_p | f(\bs x_{a_p}), f(\bs x_{b_p}) )] - \frac{1}{2} \bigg\{ \ln|\bs K_{mm}| - \mathbb{E}[\ln s] - \ln|\bs S| - M
\nonumber &\\
& + \hat{\bs f}_m^T\mathbb{E}[s] \bs K_{mm}^{-1}\hat{\bs f}_m + 
\textrm{tr}(\mathbb{E}[s] \bs K_{mm}^{-1} \bs S) \bigg\}  + \ln\Gamma(\alpha) - \ln\Gamma(\alpha_0)  + \alpha_0(\ln \beta_0) \nonumber\\
& + (\alpha_0-\alpha)\mathbb{E}[\ln s]+ (\beta-\beta_0) \mathbb{E}[s] - \alpha \ln \beta, &
\label{eq:full_L_singleuser}
\end{flalign}
where the terms relating to $\mathbb{E}[p(\bs f | \bs f_m) - q(\bs f)]$ cancel.
% Without stochastic sampling, the variational factor $\ln q(\bs f_m)$ is given by:
% \begin{flalign}
% \ln q(\bs f_m) &= \ln \mathcal{N}\left(\bs y; \tilde{\Phi}(\bs z), \bs Q\right)]
% + \ln\mathcal{N}\left(\bs f_m; \bs 0, \bs K_{mm}/\mathbb{E}\left[s\right]\right)  + \textrm{const}, \nonumber \\
% %&= \ln \int \mathcal{N}(\bs y - 0.5; \bs G \bs f, \bs Q) 
% %\mathcal{N}(\bs f; \bs A \bs f_m, \bs K - \bs A \bs K_{nm}^T) & \nonumber\\
% %& \hspace{3.2cm} \mathcal{N}(\bs f_m; \bs 0, \bs K_{mm}\mathbb{E}[1/s]) \textrm{d} \bs f + \textrm{const} & \nonumber\\
%  & = \ln \mathcal{N}(\bs f_m; \hat{\bs f}_m, \bs S ), \\
% \bs S^{-1} &= \bs K^{-1}_{mm}/\mathbb{E}[s] + \bs A^T \bs G^T \bs Q^{-1} \bs G \bs A, \label{eq:S}\\
% \hat{\bs f}_m &= \bs S \bs A^T \bs G^T \bs Q^{-1} (\bs y - \Phi(\mathbb{E}[\bs z]) + \bs G \mathbb{E}[\bs f] ). %\label{eq:fhat_m}
% \end{flalign}
\section{Variational Lower Bound for crowdGPPL}
\label{sec:crowdL}

For crowdGPPL, our approximate variational lower bound is:
\begin{flalign}
\mathcal{L}_{cr} & = \label{eq:lowerbound_crowd_full}
\sum_{p=1}^P \ln p(y_p | \hat{\bs v}_{\!.,a_p}^T \! \hat{\bs w}_{\!.,j_p} \!+ \hat{t}_{a_p}\!,
 \hat{\bs v}_{\!.,b_p}^T\! \hat{\bs w}_{\!.,j_p} \!+ \hat{t}_{b_p})
- \frac{1}{2} 
\Bigg\{  \sum_{c=1}^C \bigg\{  
 \ln|\bs K_{mm}| 
\! - \! \mathbb{E}\left[\ln s^{(v)}_c\right]
\! - \! \ln|\bs S^{(v)}_{c}|  
& \nonumber \\
& 
\! - \! M_{\mathrm{items}} 
+ \hat{\bs v}_{m,c}^T \mathbb{E}\left[s^{(v)}_c\right] \bs K_{mm}^{-1}\hat{\bs v}_{m,c} 
+ \textrm{tr}\left(\mathbb{E}\left[s_c^{(v)}\right] \bs K_{mm}^{-1} \bs S_{v,c}\right) 
+ \ln|\bs L_{mm}|
- \mathbb{E}\left[\ln s^{(w)}_c \right]
& \nonumber \\
&  
- \ln|\bs \Sigma_{c}| 
\! - \! M_{\mathrm{users}}
  + \hat{\bs w}_{m,c}^T \mathbb{E}\left[ s_c^{(w)} \right] \bs L_{mm}^{-1}\hat{\bs w}_{m,c} 
+ \textrm{tr}\left( \mathbb{E}\left[ s_c^{(w)} \right] \bs L_{mm}^{-1} \bs \Sigma_{c} \right)
+ \ln|\bs K_{mm}|   
\bigg\}
& \nonumber \\
&
 - \mathbb{E}\left[\ln s^{(t)} \right]  
- \ln|\bs S^{(t)}| 
- M_{\mathrm{items}} 
+ \hat{\bs t}^T \mathbb{E}\left[s^{(t)}\right] \bs K_{mm}^{-1} \hat{\bs t} 
+ \textrm{tr}\left(\mathbb{E}\left[s^{(t)}\right] \bs K_{mm}^{-1} \bs S^{(t)} \right)
\Bigg\} 
& \nonumber \\
&
+ \sum_{c=1}^C \bigg\{ 
\ln\Gamma\left(\alpha_0^{(v)}\right)  + \alpha_0^{(v)}\left(\ln \beta^{(v)}_0\right)
+ \ln\Gamma\left(\alpha_c^{(v)}\right) + \left(\alpha_0^{(v)} - \alpha_c^{(v)}\right)\mathbb{E}\left[\ln s^{(v)}_c\right]
 & 
\nonumber \\ 
&
+ \left(\beta_c^{(v)} - \beta^{(v)}_0\right) \mathbb{E}[s^{(v)}_c] - \alpha_c^{(v)} \ln \beta_c^{(v)} 
+ \ln\Gamma\left(\alpha_0^{(w)}\right)  + \alpha_0^{(w)}\left(\ln \beta^{(w)}_0\right)
+ \ln\Gamma\left(\alpha_c^{(w)}\right) 
 & 
\nonumber \\ 
&
+ \left(\alpha_0^{(w)} - \alpha_c^{(w)}\right)\mathbb{E}\left[\ln s^{(w)}_c\right]
+ \left(\beta_c^{(w)} - \beta^{(w)}_0\right) \mathbb{E}[s^{(w)}_c] - \alpha_c^{(w)} \ln \beta_c^{(w)} \bigg\}
 + \ln\Gamma\left(\alpha_0^{(t)}\right)  
 & 
\nonumber \\ 
& 
 + \alpha_0^{(t)} \! \left(\ln \beta^{(t)}_0\right)
+  \ln\Gamma\left(\alpha^{(t)}\right) + \left( \! \alpha^{(t)}_0 \!-\! \alpha^{(t)} \! \right)\mathbb{E}\left[\ln s^{(t)}\right]
\! + \!  \left(\! \beta^{(t)} \!-\! \beta^{(t)}_0 \! \right) \mathbb{E}\left[s^{(t)}\right] \! - \!  \alpha^{(t)} \! \ln \beta^{(t)}
. &
\end{flalign}

\section{Posterior Parameters for Variational Factors in CrowdGPPL}
\label{sec:post_params}

For the latent item components, the posterior precision estimate for $\bs S^{-1}_{v,c}$ at iteration $i$ is given by:
\begin{flalign}
\left(  \bs S^{(v)}_{c,i}  \right)^{\!-1} \!\!\! = (1-\rho_i) \left(  \bs S^{(v)}_{c,i-1} \right)^{\!-1} 
\!\! + \rho_i \bs K^{-1}_{mm}\mathbb{E}\left[ s^{(v)}_c \right] \!
+ \rho_i\pi_i \bs A_{i}^T \bs G_i^T \textrm{diag}\left(\hat{\bs w}_{c,\bs u}^2 \! + \bs\Sigma_{c,\bs u,\bs u} \right) 
\bs Q_i^{-1} \bs G_i \bs A_{i}
\!\!, &&
\label{eq:Sv}
\end{flalign}
where $\bs A_{i} = \bs K_{im} \bs K_{mm}^{-1}$, 
$\hat{\bs w}_{c}$ and $\bs\Sigma_{c}$ are the variational mean and covariance of 
the $c$th latent user component (defined below in Equations \ref{eq:what} and \ref{eq:Sigma}),
and ${\bs u} = \{ u_p \forall p \in \bs P_i \}$ is the vector of user indexes in the sample of observations.
%The term $\textrm{diag}(\hat{\bs w}_{c,\bs j}^2 + \bs\Sigma_{c,\bs j})$ 
%scales the diagonal observation precision, $\bs Q^{-1}$, by the latent user factors.
We use $\bs S_{v,c}^{-1}$ to compute the means for each row of $\bs V_m$:
\begin{flalign}
\hat{\bs v}_{m,c,i} = & \; \bs S^{(v)}_{c,i}\left( 
(1-\rho_i) \left( \bs S^{(v)}_{c,i-1} \right)^{-1} \hat{\bs v}_{m,c,i-1} \right.& \label{eq:hatv} \\
& \left.
+ \rho_i \pi_i 
\bs S^{(v)}_{c,i} \bs A_{i}^T \bs G_i^T \textrm{diag}(\hat{\bs w}_{c,\bs u}) \bs Q_i^{-1} \right. 
 \left(\bs y_i - \Phi(\hat{\bs z}_i) + \mathrm{diag}(\hat{\bs w}_{c,\bs u}) \bs G_i \bs A_i \hat{\bs v}_{c,m,i-1}^T\right) \bigg). &\nonumber
\end{flalign}

For the consensus, the precision and mean are updated according to the following:
%This is needed to replace $\bs G$ in the single-user model, since the vector of utilities,
%$\bs f$, has been replaced by the matrix $\bs F$, where each column of $\bs F$ corresponds to a single user.
\begin{flalign}
\left( \bs S^{(t)}_i \right)^{-1} = & \; (1-\rho_i) \left( \bs S^{(t)}_{i-1} \right) + \rho_i\bs K^{-1}_{mm}\mathbb{E}\left[s^{(t)}\right] 
+ \rho_i \pi_i \bs A_{i}^T \bs G_i^T \bs Q_i^{-1} \bs G_i \bs A_{i} & \label{eq:St}\\
\hat{\bs t}_{m,i} = & \; \bs S^{(t)}_{i}\left(
(1 - \rho_i) \left( \bs S^{(t)}_{i-1} \right)^{-1}\hat{\bs t}_{m,i-1}  
 + \rho_i \pi_i \bs A_{i}^T \bs G_i^T \bs Q_i^{-1}
\left(\bs y_i - \Phi(\hat{\bs z}_i) + \bs G_i \bs A_{i} \hat{\bs t}_{i} \right) \right). & \label{eq:hatt}
\end{flalign}

For the latent user components, the SVI updates for the parameters are:
\begin{flalign}
\bs \Sigma^{-1}_{c,i} = & \; (1-\rho_i)\bs \Sigma^{-1}_{c,i-1}
+ \rho_i\bs L^{-1}_{mm} \mathbb{E} \left[ s_c^{(w)} \right]
+ \rho_i \pi_i \bs A_{w,i}^T 
& \label{eq:Sigma} \\
& \bigg( \bs H_i^T 
\textrm{diag}\left(\hat{\bs v}_{c,\bs a}^2 
  + \bs S^{(v)}_{c,\bs a, \bs a} + 
\hat{\bs v}_{c,\bs b}^2 + \bs S^{(v)}_{c,\bs b, \bs b}  
- 2\hat{\bs v}_{c,\bs a}\hat{\bs v}_{c,\bs b} - 2\bs S^{(v)}_{c,\bs a, \bs b} \right) \bs Q_i^{-1} 
\bs H_i \bigg) \bs A_{w,i} & \nonumber \\
%%%%
\hat{\bs w}_{m,c,i} = &\; 
\bs \Sigma_{c,i} \bigg( (1 - \rho_i)\bs \Sigma_{c,i-1}\hat{\bs w}_{m,c,i-1} 
+ \rho_i \pi_i \bs A_{w,i}^T \bs H_i^T \textrm{diag}(\hat{\bs v}_{c,\bs a} - \hat{\bs v}_{c,\bs b})
\bs Q_i^{-1} 
& \label{eq:what} \\
& \Big(\bs y_i - \Phi(\hat{\bs z}_i) + \textrm{diag}(\hat{\bs v}_{c,\bs a} - \hat{\bs v}_{c,\bs b}) \bs H^{(i)}_u \hat{\bs w}_{c,m,i-1}^T\Big) \bigg), & \nonumber
\end{flalign}
where the subscripts $\bs a = \{ a_p \forall p \in P_i \}$
and  $\bs b = \{b_p \forall p \in P_i \}$ are lists of indices to the first and 
second items in the pairs, respectively,
$\bs A_{w,i} = \bs L_{im} \bs L_{mm}^{-1}$,
and $\bs H_i \in U_i \times P_i$ contains partial derivatives of the likelihood corresponding to each user ($U_i$ is the
number of users referred to by pairs in $\bs P_i$), 
with elements given by:
\begin{flalign}
H_{p,j} = \Phi(\mathbb{E}[z_p])(1 - \Phi(\mathbb{E}[z_p])) (2y_p - 1)[j = u_p]. &
\end{flalign}

\begin{algorithm}[h]
 \KwIn{ Pairwise labels, $\bs y$, training item features, $\bs x$, training user features $\bs u$, 
 test item features $\bs x^*$, test user features $\bs u^*$}
 \nl Compute kernel matrices $\bs K$, $\bs K_{mm}$ and $\bs K_{nm}$ given $\bs x$\;
 \nl Compute kernel matrices $\bs L$, $\bs L_{mm}$ and $\bs L_{nm}$ given $\bs u$\;
 \nl Initialise $\mathbb{E} \!\left[s^{(t)}\!\right]$, $\mathbb{E}\!\left[s^{(v)}_c\!\right]\forall c$, 
 $\mathbb{E}\!\left[s^{(w)}_c\!\right]\forall c$, $\mathbb{E}[\bs V]$, $\hat{\bs V}_m$,
 $\mathbb{E}[\bs W]$, $\hat{\bs W}_m$,
  $\mathbb{E}[\bs t]$, $\hat{\bs t}_m$ 
  to prior means\;
 \nl Initialise $\bs S_{v,c}\forall c$ and $\bs S_t$ to prior covariance $\bs K_{mm}$\;
\nl Initialise $\bs S_{w,c}\forall c$ to prior covariance $\bs L_{mm}$\;
 \While{$\mathcal{L}$ not converged}
 {
 \nl Select random sample, $\bs P_i$, of $P$ observations\;
 \While{$\bs G_i$ not converged}
  {
  \nl Compute $\bs G_i$ given $\mathbb{E}[\bs F_i]$ \;
  \nl Compute $\hat{\bs t}_{m,i}$ and $\bs S_{i}^{(t)}$ \;
  \For{c in 1,...,C}
  {
    \nl Update $\mathbb{E}[\bs F_i]$ \;
    \nl Compute $\hat{\bs v}_{m,c,i}$ and $\bs S_{i,c}^{(v)}$ \;
    \nl Update $q\left(s^{(v)}_c\right)$, compute $\mathbb{E}\left[s^{(v)}_c\right]$ and 
    $\mathbb{E}\left[\ln s^{(v)}_c\right]$\; 
    \nl Update $\mathbb{E}[\bs F_i]$ \;
    \nl Compute $\hat{\bs W}_{m,c,i}$ and $\bs \Sigma_{i,c}$ \;    
    \nl Update $q\left(s^{(w)}_c\right)$, compute $\mathbb{E}\left[s^{(w)}_c\right]$ 
    and $\mathbb{E}\left[\ln s^{(w)}_c\right]$\;
  }
  \nl Update $\mathbb{E}[\bs F_i]$ \;
 }
 \nl Update $q\left(s^{(t)}\right)$, compute $\mathbb{E}\left[s^{(t)}\right]$ and
 $\mathbb{E}\left[\ln s^{(t)}\right]$ \;
 }
\nl Compute kernel matrices for test items, $\bs K_{**}$ and $\bs K_{*m}$, given $\bs x^*$ \;
\nl Compute kernel matrices for test users, $\bs L_{**}$ and $\bs L_{*m}$, given $\bs u^*$ \;
\nl Use converged values of $\mathbb{E}[\bs F]$ and $\hat{\bs F}_m$ to estimate
posterior over $\bs F^*$ at test points \;
\KwOut{ Posterior mean of the test values, $\mathbb{E}[\bs F^*]$ and covariance, $\bs C^*$ }
\vspace{0.5cm}
\caption{The SVI algorithm for crowdGPPL.}
\label{al:crowdgppl}
\end{algorithm}

\section{Predictions with CrowdGPPL}
\label{sec:predictions}

The means, item covariances and user variance required for predictions with crowdGPPL (Equation \ref{eq:predict_crowd})
 are defined as follows:
\begin{flalign}
\hat{\bs t}^* & = \bs K_{*m} \bs K^{-1}_{mm} \hat{\bs t}_{m}, \hspace{1.5cm} 
\bs C^{(t)*}* = \frac{\bs K_{**}}{\mathbb{E}\left[s^{(t)}\right]} + \bs A_{*m}\left(\bs S^{(t)} \!-\! \bs K_{mm}\right) 
\bs A_{*m}^T, 
\label{eq:tstar} & \\
\hat{\bs v}_{c}^* & = \bs K_{*m} \bs K^{-1}_{mm} \hat{\bs v}_{m,c}, \hspace{1.35cm} 
\bs C^{(v)*}_{c} = \frac{\bs K_{**}}{\mathbb{E}\left[s^{(v)}_c\right ]} + \bs A_{*m} \left(\bs S^{(v)}_{c} 
\!\!-\! \bs K_{mm} \right) \bs A_{*m}^T  & \\
\hat{\bs w}_{c}^* & = \bs L_{*m} \bs L^{-1}_{mm} \hat{\bs w}_{m,c}, \hspace{1.4cm}
\omega_{c,u}^* = 1/ \mathbb{E}\left[s^{(w)}_c \right] + \bs A^{(w)}_{um}(\bs \Sigma_{w,c} - \bs L_{mm}) \bs A^{(w)T}_{um} & \label{eq:omegastar}
\end{flalign}
where  $\bs A_{*m}=\bs K_{*m}\bs K_{mm}^{-1}$,
$\bs A^{(w)}_{um}=\bs L_{um}\bs L_{mm}^{-1}$ and $\bs L_{um}$ is the covariance between user $u$ and the inducing 
users.

% \section{Converged Lower Bound Derivatives}
% \label{sec:gradients}
% % The gradient of $\mathcal{L}_3$ with respect to the lengthscale, $l_d$, is as follows:
% % \begin{flalign}
% % \nabla_{l_d} \mathcal{L}_3 & =  - \frac{1}{2} \left\lbrace 
% % \frac{\partial \ln|\bs K/\mathbb{E}[s]|}{\partial l_d} - \frac{\partial \ln|\bs C|}{\partial l_d} 
% % \nonumber \right.
% %  \left.  - (\hat{\bs f}-\bs\mu)\mathbb{E}[s] \frac{\partial K^{-1}}{\partial l_d} (\hat{\bs f}-\bs\mu)
% % \right\rbrace \nonumber & \\
% % %& = \frac{1}{2} \mathbb{E}[s] \left\lbrace \frac{\partial \ln |\bs C \bs K^{-1}|}{\partial l_d}
% % %\right. \\
% % %& \left.  - (\hat{\bs f}-\bs\mu) \bs K^{-1} \frac{\partial \bs K}{\partial l_d} \bs K^{-1} (\hat{\bs f}-\bs\mu)
% % %\right\rbrace  \nonumber \\
% % & =  -\frac{1}{2} \left\lbrace  \frac{\partial \ln | \frac{1}{\mathbb{E}[s]}\bs K \bs C^{-1} |}{\partial l_d} \right. 
% % \left.  + \mathbb{E}[s] (\hat{\bs f}-\bs\mu) \bs K^{-1} \frac{\partial \bs K}{\partial l_d} \bs K^{-1} (\hat{\bs f}-\bs\mu)
% % \right\rbrace   &
% % %& =  - \frac{1}{2} \left\lbrace \frac{\partial \ln|\bs K/s| }{\partial l_d} + \frac{\partial \ln |\bs K^{-1}s + \bs G\bs Q^{-1}\bs G^T|}{\partial l_d}
% % %\right. \\
% % %& \left.  - \mathbb{E}[s] (\hat{\bs f}-\bs\mu) \bs K^{-1} \frac{\partial \bs K}{\partial l_d} \bs K^{-1} (\hat{\bs f}-\bs\mu)
% % %\right\rbrace  \nonumber\\
% % %& =  -\frac{1}{2} \left\lbrace \frac{\partial \ln |\bs I + \bs K/s\bs G\bs Q^{-1}\bs G^T|}{\partial l_d}
% % %\right. \\
% % %& \left.  - \mathbb{E}[s] (\hat{\bs f}-\bs\mu) \bs K^{-1} \frac{\partial \bs K}{\partial l_d} \bs K^{-1} (\hat{\bs f}-\bs\mu)
% % %\right\rbrace  \nonumber
% % \end{flalign}
% % Using the fact that $\ln | A | = \mathrm{tr}(\ln A)$, $\bs C = \left[\bs K^{-1} - \bs G \bs Q^{-1} \bs G^T \right]^{-1}$, and $\bs C = \bs C^{T}$, we obtain:
% % \begin{flalign}
% % \nabla_{l_d} \mathcal{L}_3 & =  -\frac{1}{2} \mathrm{tr}\left(\left(\mathbb{E}[s]\bs K^{-1}\bs C\right) \bs G\bs Q^{-1}\bs G^T \frac{\partial \bs K}{\partial l_d}
% % \right)
% %  + \frac{1}{2}\mathbb{E}[s] (\hat{\bs f}-\bs\mu) \bs K^{-1} \frac{\partial \bs K}{\partial l_d} \bs K^{-1} (\hat{\bs f}-\bs\mu)  \nonumber\\ 
% % & =  -\frac{1}{2} \mathrm{tr}\left(\left(\mathbb{E}[s]\bs K^{-1}\bs C\right)
% % \left(\bs C^{-1} - \bs K^{-1}/\mathbb{E}[s]\right) \frac{\partial \bs K}{\partial l_d}
% % \right) 
% % + \frac{1}{2}\mathbb{E}[s] (\hat{\bs f}-\bs\mu) \bs K^{-1} \frac{\partial \bs K}{\partial l_d} \bs K^{-1} (\hat{\bs f}-\bs\mu).  \label{eq:gradient_ls}
% % \end{flalign}
% % Assuming a product over kernels for each feature, $\bs K=\prod_{d=1}^{D} \bs K_d$, we can compute the kernel gradient 
% % as follows for the Mat\'ern $\frac{3}{2}$ kernel function:
% % \begin{flalign}
% % \frac{\partial \bs K}{\partial l_d} & = \prod_{d'=1,d'\neq d}^D K_{d} \frac{\partial K_{l_d}}{\partial l_d} \\
% % \frac{\partial K_{l_d}}{\partial l_d} & = \frac{3\bs |\bs x_d - \bs x_d'|^2}{l_d^3} \exp\left( - \frac{\sqrt{3} \bs |\bs x_d - \bs x_d'|}{l_d} \right)
% % \label{eq:kernel_der}
% % \end{flalign}
% % where $|\bs x_d - \bs x_d'|$ is the distance between input points.
%
% When $\mathcal{L}$ has converged to a maximum, 
% $\nabla_{l_{\! d}} \mathcal{L}$ simplifies to:
% \begin{flalign}
%  &\nabla_{\!l_{\! d}} \mathcal{L} \longrightarrow 
% \frac{1}{2} \mathrm{tr}\!\left(\! \left(
% \mathbb{E}[s](\hat{\bs f}_{\! m} \hat{\bs f}_{\! m}^T + \bs S^T)\bs K_{\! mm}^{-1} \! -  \bs I \! \right)
%  \!\frac{\partial \bs K_{\! mm}}{\partial l_d} \bs K_{\! mm}^{-1} \right) \!. &
% \label{eq:gradient_single}
% \end{flalign}
% For crowdGPPL, assuming that $\bs V$ and $\bs t$ have the same kernel function,
% the gradient
% %The gradients with respect to the length-scale, $l_{w,d}$,
% for the $d$th item feature is given by:
% \begin{flalign}
%  &\nabla_{l_{ v,d}} \mathcal{L}_{cr} \longrightarrow
% %  \sum_{c=1}^{C} \bigg\{ \mathbb{E}[s_c] 
% %  \hat{\bs v}_{ m,c}^T \bs K_{ mm,v}^{-1} 
% % \frac{\partial \bs K_{ mm,v}}{\partial l_{w,d}} \bs K_{ mm,v}^{-1} \hat{\bs v}_{ m,c} 
% %  + & \nonumber \\
% %  & \mathrm{tr}\left( \left(
% % \mathbb{E}[s_c]\bs S_{v,c}^T\bs K_{ mm,v}^{-1}  - \frac{1}{2} \bs I  \right)
% %  \frac{\partial \bs K_{ mm,v}}{\partial l_{w,d}} \bs K_{ mm,v}^{-1} \right) \bigg\}
% %  + & \nonumber \\
% %  & \mathbb{E}[s_t] 
% %  \hat{\bs t}_{ m}^T \bs K_{ mm,t}^{-1} 
% % \frac{\partial \bs K_{ mm,t}}{\partial l_{w,d}} \bs K_{ mm,t}^{-1} \hat{\bs t}_{ m} 
% %  + \mathrm{tr}\left( \left(
% % \mathbb{E}[s_t]\bs S_{t}^T\bs K_{ mm,t}^{-1}  - \frac{1}{2} \bs I  \right)
% %  \frac{\partial \bs K_{ mm,t}}{\partial l_{w,d}} \bs K_{ mm,t}^{-1} \right)
% %  & \nonumber \\
% % & = 
% \frac{1}{2} \mathrm{tr}\left( \left( \sum_{c=1}^{C} \mathbb{E}[s_c] \left\{ \hat{\bs v}_{m,c} 
%  \hat{\bs v}_{m,c}^T + \bs S_{v,c}^T \right\}
%  \bs K_{ mm,v}^{-1}  - C\bs I  \right)
%  \frac{\partial \bs K_{ mm,v}}{\partial l_{w,d}} \right.
%  & \nonumber \\
%  & \left.  \bs K_{ mm,v}^{-1} \right) + \frac{1}{2}\mathrm{tr}\left( \left(
% \mathbb{E}[s_t](\hat{\bs t}_{ m}\hat{\bs t}_{ m}^T + \bs S_{t}^T) \bs K_{ mm,t}^{-1}  
% - \bs I  \right)
%  \frac{\partial \bs K_{ mm,t}}{\partial l_{w,d}} \bs K_{ mm,t}^{-1} \right)
% .&
% \label{eq:gradient_crowd_items}
% \end{flalign}
% % If different kernels are used for different components, then the equation above can be modified to
% % simply sum over terms relating to the components with a shared kernel function. 
% The gradients for the $d$th user feature length-scale, $l_{w,d}$, follows the same form:
% \begin{flalign}
%  &\nabla_{l_{w,d}} \mathcal{L}_{cr} \!\!\!\longrightarrow \frac{1}{2} 
%  \!\mathrm{tr}\left( \!\left( \sum_{c=1}^{C} \left\{ \hat{\bs w}_{m,c} \hat{\bs w}_{m,c}^T \!+
% \bs \Sigma_c^T\right\} \!\bs K_{mm,w}^{-1} \! - C\bs I  \right)
%  \frac{\partial \bs K_{mm,w}}{\partial l_{w,d}} \bs K_{mm,w}^{-1} \!\right) \!. &
% \label{eq:gradient_crowd_users}
% \end{flalign}
%
% % When combining kernel functions for each features using a product,
% % as in Equation \ref{eq:kernel}, the partial derivative of the covariance matrix $\bs K_{mm}$ with respect to 
% % $l_d$ is given by:
% % \begin{flalign}
% % \frac{\partial \bs K_{mm}}{\partial l_d} 
% % & = \frac{\bs K_{mm}}{\bs K_{d}}
% % \frac{ \bs K_{d}(|\bs x_{mm,d}, \bs x'_{mm,d})}{\partial l_d} \nonumber ,\\
% % \end{flalign}
% The partial derivative of the covariance matrix $\bs K_{mm}$ with respect to 
% $l_d$ depends on the choice of kernel function. 
% The Mat\`ern $\frac{3}{2}$ function is a widely-applicable, differentiable kernel function 
% that has been shown empirically to outperform other well-established kernels 
% such as the squared exponential, and makes weaker assumptions of smoothness of 
% the latent function~\citep{rasmussen_gaussian_2006}. 
% It is defined as:
% \begin{flalign}
% k_d\left(\frac{|x_d - x_d'|}{l_d} \right) = \left(1 + \frac{\sqrt{3} | x_d - x_d'|}{l_d}\right) 
% \exp \left(- \frac{\sqrt {3} | x_d - x_d'|}{l_d}\right).
% \end{flalign}
% %For the Mat\`ern $\frac{3}{2}$ kernel,  
% Assuming that the kernel functions for each feature, $k_d$, are combined using
% a product, as in Equation \ref{eq:kernel}, 
% the partial derivative $\frac{\partial \bs K_{mm}}{\partial l_d}$ is a matrix, where each 
% entry, $i,j$,  is defined by:
% \begin{flalign}
% & \frac{\partial K_{mm,ij}}{\partial l_d} = 
% \prod_{d'=1, d' \neq d}^D k_{d'}\left(\frac{|x_{d'} - x_{d'}'|}{l_{d'}}\right)
% \frac{3 (\bs x_{i,d} - \bs x_{j,d})^2}{l_d^3} \exp\left( - \frac{\sqrt{3} \bs |\bs x_{i,d} - \bs x_{j,d}|}{l_d} \right), &
% \label{eq:kernel_der}
% \end{flalign}
% where we assume the use of Equation\ref{eq:kernel} to combine kernel 
% functions over features using a product.
%
%
% To make use of Equations \ref{eq:gradient_single} to \ref{eq:kernel_der},
% we nest the variational algorithm defined in Section \ref{sec:inf} inside
% an iterative gradient-based optimization method.
% Optimization then begins with an initial guess for all length-scales, $l_d$,
% such as the median heuristic.
% Given the current values of $l_d$, the optimizer (e.g. L-BFGS-B)
% runs the VB algorithm to convergence, 
% computes $\nabla_{l_{\! d}} \mathcal{L}$,
% then proposes a new candidate value of $l_d$.
% The process repeats until the optimizer converges or reaches a maximum number 
% of iterations, and returns the value of $l_d$ that maximized $\mathcal{L}$.

\section{Mathematical Notation}
\label{sec:not}

\begin{table}[h!]
 \begin{tabularx}{\columnwidth}{p{1.7cm} X }
 \toprule 
 Symbol & Meaning \\
 \midrule 
 \multicolumn{2}{l}{\textbf{General symbols used with multiple variables}} \\
 $\hat{}$ & an expectation over a variable \\
 $\tilde{}$ & an approximation to the variable \\
 upper case, bold letter & a matrix \\
 lower case, bold letter & a vector \\
 lower case, normal letter & a function or scalar \\
 * & indicates that the variable refers to the test set, rather than the training set \\
  \multicolumn{2}{l}{\textbf{Pairwise preference labels}} \\
 $y(a,b)$ & a binary label indicating whether item $a$ is preferred to item $b$ \\
 $y_p$ & the $p$th pairwise label in a set of observations \\
 $\bs y$ & the set of observed values of pairwise labels \\
 $\Phi$ & cumulative density function of the standard Gaussian (normal) distribution \\
 $\bs x_a$ & the features of item a (a numerical vector) \\
 $\bs X$ & the features of all items in the training set \\
 $D$ & the size of the feature vector \\
 $N$ & number of items in the training set \\
 $P$ & number of pairwise labels in the training set \\
 $\bs x^*$ & the features of all items in the test set \\
 $\delta_a$ & observation noise in the utility of item $a$ \\
 $\sigma^2$ & variance of the observation noise in the utilities \\
 $z_p$ & the difference in utilities of items in pair $p$, normalised by its total variance \\
 $\bs z$ & set of $z_p$ values for training pairs \\ 
  \bottomrule
 \end{tabularx}
 \caption{Table of symbols used to represent variables in this paper (continued on next page
 in Table \ref{tab:sym2}).}
 \label{tab:sym1}
\end{table}
\begin{table}
 \begin{tabularx}{\columnwidth}{p{1.7cm} X }
 \toprule 
 Symbol & Meaning \\
 \midrule 
\multicolumn{2}{l}{\textbf{GPPL (some terms also appear in crowdGPPL)}} \\
 $f$ & latent utility function over items in single-user GPPL \\
 $\bs f$ & utilities, i.e., values of the latent utility function for a given set of items \\
 $\bs C$ & posterior covariance in $\bs f$; in crowdGPPL, superscripts indicate 
 whether this is the covariance of consensus values or latent item components \\
 $s$ & an inverse function scale; in crowdGPPL, superscripts indicate which function this variable scales \\
 $k$ & kernel function \\
 $\theta$ & kernel hyperparameters for the items \\
 $\bs K$ & prior covariance matrix over items \\
 $\alpha_0$ & shape hyperparameter of the inverse function scale prior \\
 $\beta_0$ & scale hyperparameters of the inverse function scale prior \\
 \multicolumn{2}{l}{\textbf{CrowdGPPL}} \\
 $\bs F$ & matrix of utilities, where rows correspond to items and columns to users \\
 $\bs t$ & consensus utilities \\
 $C$ & number of latent components \\
 $c$ & index of a component \\
 $\bs V$ & matrix of latent item components, where rows correspond to components \\
 $\bs v_c$ & a row of $\bs V$ for the $c$th component \\
 $\bs W$ & matrix of latent user components, where rows correspond to components \\
 $\bs w_c$ & a row of $\bs W$ for the $c$th component \\ 
 $\bs \omega_c$ & posterior variance for the $c$th user component \\
 $\eta$ & kernel hyperparameters for the users \\
 $\bs L$ & prior covariance matrix over users \\
 $\bs u_j$ & user features for user $j$ \\
 $U$ & number of users in the training set \\
 $\bs U$ & matrix of features for all users in the training set \\ 
 \multicolumn{2}{l}{\textbf{Probability distributions}} \\
 $\mathcal{N}$ & (multivariate) Gaussian or normal distribution \\
 $\mathcal{G}$ & Gamma distribution \\
% \bottomrule
% \end{tabularx}
% \caption{Table of symbols used to represent variables in this paper (continued on next page
% in Table \ref{tab:sym2}).}
% \label{tab:sym1}
%\end{table}
%\begin{table}
% \begin{tabularx}{\columnwidth}{p{1.7cm} X }
% \toprule 
% Symbol & Meaning \\
% \midrule 
 \multicolumn{2}{l}{\textbf{Stochastic Variational Inference (SVI) }} \\
 $M$ & number of inducing items \\
 $\bs Q$ & estimated observation noise variance for the approximate posterior \\
 $\gamma, \lambda$ & estimated hyperparameters of a Beta prior distribution over $\Phi(z_p)$ \\
 $i$ & iteration counter for stochastic variational inference \\
 $\bs f_m$ & utilities of inducing items \\
 $\bs K_{mm}$ & prior covariance of the inducing items \\
 $\bs K_{nm}$ & prior covariance between training and inducing items \\
 $\bs S$ & posterior covariance of the inducing items; in crowdGPPL, a superscript and subscript 
 indicate which variable this is the posterior covariance for \\
 $\bs\Sigma$ & posterior covariance over the latent user components \\
 $\bs A$ & $\bs K_{nm} \bs K_{mm}^{-1}$ \\
 $\bs G$ & linearisation term used to approximate the likelihood \\
 $a$ & posterior shape parameter for the Gamma distribution over $s$ \\
 $b$ & posterior scale parameter for the Gamma distribution over $s$ \\
 $\rho_i$ & a mixing coefficient, i.e., a weight given to the $i$th update when combining with current values of variational
 parameters \\
 $\epsilon$ & delay \\
 $r$ & forgetting rate \\
 $\pi_i$ & weight given to the update at the $i$th iteration \\
 $\bs P_i$ & subset of pairwise labels used in the $i$th iteration \\
 $P_i$ & number of pairwise labels in the $i$th iteration subsample \\
 $U_i$ & number of users referred to in the $i$th subsample \\
 $\bs u$ & users in the $i$th subsample \\
 $\bs a$ & indexes of first items in the pairs in the $i$th subsample \\
 $\bs b$ & indexes of first items in the pairs in the $i$th subsample \\
 \bottomrule
 \end{tabularx}
 \caption{Table of symbols used to represent variables in this paper (continued from Table \ref{tab:sym1} on previous page).}
 \label{tab:sym2}
\end{table}