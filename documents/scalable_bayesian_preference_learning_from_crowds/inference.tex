\section{Scalable Inference}\label{sec:inf}

In the single user case, the goal is to infer the posterior distribution over the utilities of test items, $\bs f^*$, 
given a set of pairwise training labels, $\bs y$. In the multi-user case, we aim to find the posterior over the matrix
$\bs F^*=\bs V^{*T} \bs W^*$ of utilities for test items and test users.
The non-Gaussian likelihood makes exact inference intractable, hence previous work has used
 the Laplace approximation for the single user case~\citep{chu2005preference}
or a combination of expectation propagation (EP) with variational Bayes (VB) for a 
multi-user model~\citep{houlsby2012collaborative}.
The Laplace approximation is a maximum a-posteriori (MAP) solution that
takes the most probable values of parameters rather than integrating over their distributions,
and has been shown to perform poorly for tasks such as classification~\citep{nickisch2008approximations}. 
EP and VB approximate the true posterior with a simpler, factorized distribution
that can be learned using an iterative algorithm.
For crowdGPPL, the true posterior is multi-modal, since the latent factors can be re-ordered arbitrarily without
affecting $\bs F$, causing a \emph{non-identifiability problem}.
EP would average these modes and produce uninformative predictions over $\bs F$, so
~\citet{houlsby2012collaborative} incorporate a VB step that approximates a single mode.
A drawback of EP is that unlike VB, convergence is not guaranteed~\citep{minka2001expectation}.
%do they also linearise in the same way? -- both linearise. But EP uses a joint over y and f as its approximation to p(y|f), then optimises the parameters iteratively. It's not guaranteed to converge. Variational EGP instead approximates
% p(y|f) directly with the best fit Gaussian. It's not clear whether this could be updated iteratively but it doesn't
% seem to work if done simultaneously with the other variables we need to learn (the linearisation), 
% perhaps because the algorithm for learning the weights breaks if the variance of q(y|f), Q, keeps changing. 
% Possibly because Q does not change incrementally. So it's
% possible that an outer loop could be used.

%TODO: remove redundancy with the related work section. Consider whether this should actually be in a background section. NM^2 is limiting, not just the other costs. The other costs NP etc come into play in this pairwise model only.
Exact inference for a Gaussian process has computational complexity $\mathcal{O}(N^3)$ 
and memory complexity $\mathcal{O}(N^2)$.
The cost of inference can be reduced using a \emph{sparse} approximation based on a set of 
\emph{inducing points}, which act as substitutes for the set of points in the training dataset.
By choosing a fixed number of inducing points, $M \ll N$, the computational cost is cut to $\mathcal{O}(NM^2)$,
and the memory complexity to $\mathcal{O}(NM)$.
These points must be selected so as to give a good approximation, 
using either heuristics or optimizing their positions to maximize the approximate
marginal likelihood. 
The sparse approximation used by \citet{houlsby2012collaborative} for the collaborative GP 
is the \emph{generalized fully independent training conditional} (GFITC)~\citep{snelson2006sparse}.
In practice, GFITC is unsuitable for datasets with more than a few thousands points,
as the $\mathcal{O}(NM^2)$ computational and $\mathcal{O}(NM)$ costs become prohibitively high
when $N$ is large,
and GFITC is not amenable to distributed computation~\citep{hensman2015scalable}.  
In contrast to crowdGPPL, \citet{houlsby2012collaborative} place GPs over the space of pairs rather than items,
which is typically much larger, meaning that $\mathcal{O}(PM^2 + UM^2)$ computational and $\mathcal{O}(PM + UM)$
memory costs dominate. 
We derive a more scalable approach for GPPL and crowdGPPL using
stochastic variational inference (SVI), an iterative scheme that allows the computational and memory costs at
each iteration to be constrained~\citep{hoffman2013stochastic}.
% how does hensman 2015 optimize inducing points? This is one selling point of SVI. 
% Advantages of VB:  Nickisch 2008 got poorer results for VB methods they tried than EP. But our method may be different?, Seeger 2000
First, we define an approximate likelihood that enables the SVI method.
%then derive the SVI method for single user GPPL. update equations for the iterative algorithm to optimize
%the approximation. We begin with single user GPPL then extend this to crowdGPPL. 

\subsection{Approximate Pairwise Likelihood}

To obtain a tracatable approximate posterior, we begin by approximating 
the expected preference likelihood (Equation \ref{eq:plphi}) with a Gaussian:
\begin{flalign}
p(\bs y | \bs f) = \mathbb{E}\left[\prod_{p=1}^P \Phi(z_p)\right] = \prod_{p=1}^P \Phi(\hat{z}_p) \approx \mathcal{N}(\bs y; \Phi(\hat{\bs z}), \bs Q),
\label{eq:likelihood_approx}
\end{flalign}
where $\hat{\bs z}=\{\hat{z}_1, ..., \hat{z}_P\}$
and $\bs Q$ is a diagonal noise covariance matrix.
Since $\Phi(\hat{z}_p)$ defines a bernoulli distribution, for which the conjugate prior is a beta distribution,
we moment match the diagonal entries of $\bs Q$ to the expected variance of a bernoulli distribution as follows:
\begin{flalign}
Q_{p,p} & = \mathbb{E}_{f}[\Phi(z_p)(1 - \Phi(z_p))] 
%= \mathbb{E}_{\bs f}[\Phi(z_p)] - \mathbb{E}_{\bs f}[\Phi(z_p)]^2 - \mathbb{V}_{\bs f}[\Phi(z_p)], 
 \nonumber \\
& \approx \frac{ (y_p + \gamma_0)(1-y_p + \lambda_0) }{ (\gamma_0 + \lambda_0 + 1)} - \frac{ (y_p + \gamma_0)(1-y_p + \lambda_0) }{ (\gamma_0 + \lambda_0 + 1)^2(\gamma_0 + \lambda_0 + 2)}, &
\end{flalign}
where $\gamma_0$ and $\lambda_0$ are parameters of a beta distribution with the same variance as the prior 
$p(\Phi(z_p) | \bs K_{\theta}, \alpha_0, \beta_0)$ estimated using numerical integration.
%Setting $\bs Q$ in this way matches the moments of the true likelihood, $\Phi(z_p)$,
%to those of the Gaussian approximation.

Unfortunately, the nonlinear term, $\Phi(\bs z)$ means that the posterior is still intractable, 
so we linearize $\Phi(\bs z)$ by taking its first-order Taylor series expansion
about the expectation of $\bs f$ for single user GPPL (for crowdGPPL, replace $\bs f$ with $\bs F$ in the following):
\begin{flalign}
\Phi(\bs z) &\approx \tilde{\Phi}(\bs z) = \bs G (\bs f-\mathbb{E}[\bs f]) + \Phi(\mathbb{E}[\bs z]), \\
G_{p,i} &= \Phi(\mathbb{E}[z_p])(1 - \Phi(\mathbb{E}[z_p])) (2y_p - 1)( [i = a_p] - [i = b_p]) 
\end{flalign}
where $\bs G$ is a matrix containing elements $G_{p,i}$, which are the
partial derivatives of the pairwise likelihood with respect to each of 
the latent function values, $\bs f$.
This creates a dependency between the posterior mean of $\bs f$ and the linearization terms in the likelihood,
which can be estimated iteratively using variational inference~\citep{steinberg2014extended},
as we describe below.
The linearization makes the approximate likelihood conjugate to the prior, $\mathcal{N}(\bs f; \bs 0, \bs K_{\theta}/s)$,
so that the approximate posterior over $\bs f$ is also Gaussian. 
%We now use our likelihood approximation to define an approximate
%posterior over all variables for GPPL:
%use variational inference to estimate the marginal over $\bs f$,
%by optimizing an approximate posterior over all latent variables:
%\begin{flalign}
%p(\bs f, s | \bs y, \bs x, k_{\theta}, & \alpha_0, \beta_0) \approx q(s)q(\bs f), & \nonumber \\
%\textrm{where } \log q(s) & = \mathbb{E}_{\bs f}[\log p(s | \bs f, \alpha_0, \beta_0)] & \nonumber \\
%& = \log \mathcal{N}(\mathbb{E}[\bs f]; \bs 0, \bs K_{\theta}/s) + \log \mathcal{G}(s; \alpha_0, \beta_0) + \textrm{const}, & \nonumber \\
%\log q(\bs f) & = \mathbb{E}_{s}[\log p(\bs f | k_{\theta}, s, \bs x, \bs y) ] & \nonumber \\
%& = \log \mathcal{N}(\bs y; \tilde{\Phi}(\bs z), \bs Q) + \log \mathcal{N}(\bs f; \bs 0, \bs K_{\theta}/\mathbb{E}[s]) + \textrm{const}. &
%\label{eq:vb_approx}
%\end{flalign}
%don't some of the expectations over s simplify out?
%Since the Gamma is conjugate to $s$, $q(s)$ is also a Gamma distribution. 
%Likewise, the mean of the Gaussian likelihood approximation is
%a linear function of $\bs f$, 
%which has a multivariate Gaussian prior, hence
%$q(\bs f)$ is also a multivariate Gaussian.
The Gaussian likelihood approximation and linearization
also appear in GP inference methods based on expectation propagation~\citep{rasmussen_gaussian_2006} 
and the extended Kalman filter~\citep{reece2011determining,steinberg2014extended}.
%However, neither these methods nor our approximation in Equation \ref{eq:vb_approx}
%make inference sufficiently scalable, as they all require
%inversion of an $N \times N$ matrix and further computations involving $N \times P$ and $P \times P$ matrices.
In the next section, we use the approximate likelihood to define an approximate posterior
% modify Equation \ref{eq:vb_approx} to enable 
for stochastic variational inference (SVI).

\subsection{SVI for Single User GPPL}

%TODO can we ommit the f from the posterior here?

We introduce a sparse approximation to the Gaussian process that allows
us to limit the size of the covariance matrices we need to work with.
% that needs to be inverted,
%and permit stochastic inference methods that consider only a subset of the $P$ observations at each iteration
%~\citep{hensman2013gaussian,hensman2015scalable}. 
To do this, we introduce a set of $M \ll N$ \emph{inducing} items with inputs 
$\bs x_m$,
utilities $\bs f_m$, covariance $\bs K_{mm}$,
and covariance between the observed and inducing items, $\bs K_{nm}$.
% The inducing points act as proxies for the observed points during inference,
% and thereby reduce the number of data points we have to perform costly operations % over.
%We modify the variational approximation in Equation \ref{eq:vb_approx} to introduce the inducing points 
For clarity, we omit $\theta$ from this point on, and provide further detailed equations in 
Appendix \ref{sec:vb_eqns}.
The posterior over the inducing and training items is approximated as:
\begin{flalign}
p(\bs f, \bs f_m, s | \bs y, \bs x, \bs x_m, k_{\theta}, \alpha_0, \beta_0) &\approx q(\bs f, \bs f_m, s) = q(s)q(\bs f)q(\bs f_m). \label{eq:svi_approx} &&
\end{flalign}
We marginalize $\bs f$ to obtain the factor for $\bs f_m$:
\begin{flalign}
\log q(\bs f_m) &= \log \mathcal{N}\left(\bs y; \tilde{\Phi}(\bs z), \bs Q\right)]
+ \log\mathcal{N}\left(\bs f_m; \bs 0, \bs K_{mm}/\mathbb{E}\left[s\right]\right)  + \textrm{const}, & , \nonumber \\
 & = \log \mathcal{N}(\bs f_m; \hat{\bs f}_m, \bs S ),
 \label{eq:fhat_m}
\end{flalign}
where the variational parameters $\hat{\bs f}_m$ and $\bs S$ are computed using the iterative SVI procedure described below.
We choose an approximation of $q(\bs f)$ that depends only on the inducing point utilities, $\bs f_m$, and is independent of the observations:
 \begin{flalign}
\log q(\bs f) & = \log \mathcal{N}(\bs f; \bs A \hat{\bs f}_m, 
\bs K + \bs A (\bs S - \bs K_{mm}/\mathbb{E}[s]) \bs A^T ),
\end{flalign}
where $\bs A=\bs K_{nm} \bs K^{-1}_{mm}$.
This means we no longer need to invert an $N \times N$ covariance matrix to compute $q(\bs f)$.
The factor $q(s)$ is also modified to depend on the inducing points:
\begin{flalign}
& \log q(s) = \log\mathcal{N}(\mathbb{E}[\bs f_m| \bs 0, \bs K_{mm}/s] + \log \mathcal{G}(s; \alpha_0, \beta_0) + \mathrm{const}
= \log \mathcal{G}(s; \alpha, \beta), & \label{eq:qs}
\end{flalign}
where $\alpha= \alpha_0 + \frac{M}{2}$ and $\beta = \beta_0 + \frac{
\textrm{tr}(\bs K^{-1}_{mm}(S + \hat{\bs f}_m \hat{\bs f}_m^T))}{2}$.
None of the other factors depend on $\log q(\bs f)$,
so it need not be computed unless required for prediction.

%once the other factors have already been estimated, as explained in the next subsection.

%Since the inducing points stand in for the observed locations,
%their choice affects the quality of our approximation. 
The location of inducing points can be learned
as part of the variational inference procedure ~\citep{hensman2015scalable},
or by optimizing a bound on the log marginal likelihood.
However, the former breaks the convergence guarantees, and both approaches
may add substantial computational cost. 
We find that we are able to obtain good performance by choosing inducing points up-front using K-means++~\citep{arthur2007k} with $K=M$ to  
cluster the feature vectors, 
then taking the cluster centers as inducing points that represent the spread of observations across feature space.
%Compared to standard K-means, K-means++ introduces a new method 
%for choosing the initial cluster seeds that
%provides theoretical bounds on the error function. In practice, this 
%reduces the number of poor-quality clusterings.


%\subsection{SVI for Single-User Preference Learning}

%The approximate posterior can now be optimized using stochastic variational inference (SVI),
%which uses a series of stochastic updates involving different subsets of the observations.
%Variational inference 
We can apply variational inference to iteratively reduce the KL-divergence between our approximate posterior
%$q(s)q(\bs f)q(\bs f_m)$
and the true posterior (Equation \ref{eq:svi_approx}) %, $p(s, \bs f, \bs f_m | \bs K, \alpha_0, \beta_0, \bs y)$,
by maximizing a lower bound, $\mathcal{L}$, on the log marginal likelihood:
%(see also Equation \ref{eq:full_L_singleuser} in the Appendix):%, $\log p(\bs y | \bs K, \alpha_0, \beta_0)$ :
\begin{flalign}
\log p(\bs y | \bs K, \alpha_0, \beta_0) = \textrm{KL}(q(\bs f, \bs f_m, s)  || p(\bs f, \bs f_m, s | \bs y, \bs K, \alpha_0, \beta_0)) 
+ \mathcal{L}. &&\label{eq:lowerbound}
\\
%\end{flalign}
%Taking expectations with respect to the variational $q$ distributions, $\mathcal{L}$ is:
%\begin{flalign}
\mathcal{L} = \mathbb{E}_{q(\bs f, \bs f_m, s)}[\log p(\bs y | \bs f) + \log p(\bs f_m, s | \bs K, 
\alpha_0, \beta_0) -\log q(\bs f_m) - \log & q(s) ] & \nonumber
\end{flalign}
%         invK_mm_expecFF = self.invK_mm.dot(self.uS + self.um_minus_mu0.dot(self.um_minus_mu0.T))
%         self.rate_s = self.rate_s0 + 0.5 * np.trace(invK_mm_expecFF)
We optimize $\mathcal{L}$ by initializing the $q$ factors randomly, then
updating each one in turn, taking expectations with respect to the other factors. 

The only term in $\mathcal{L}$ that refers to the observations, $\bs y$, 
is a sum of $P$ terms, each of which refers to one observation only.
This means that $\mathcal{L}$ can be maximized iteratively by considering a random subset of 
observations at each iteration~\citep{hensman2013gaussian}.
%Therefore, the SVI solution replaces Equations \ref{eq:fhat_m} and \ref{eq:S} for computing
%$\hat{\bs f}_m$ and $\bs S$ over all observations with a sequence of stochastic updates.
For the $i$th update of $q(\bs f_m)$, we randomly select observations $\bs y_i = \{ y_p \forall p \in \bs P_i \}$, where $\bs P_i$ is random subset of indexes of observations.
%Rather than using the complete matrices, 
We then perform updates using $\bs Q_i$, which contains rows and columns of $\bs Q$ for observations in $\bs P_i$,
$\bs K_{im}$ and $\bs A_i$, which contain only rows referred to by $\{y_p \forall p \in \bs P_i\}$,
$\bs G_i$, which contains rows in $\bs P_i$ and columns referred to by any items $\{a_p \forall p \in \bs P_i \} \cup \{ b_p \forall p \in \bs P_i\}$,
and $\hat{\bs z}_i = \{ \mathbb{E}[\bs z_p] \forall p \in P_i \}$.
%All matrices with subscript $_i$ contain only the subset of elements relating to 
%observations in $\bs P_i$.
% The linearization matrix $\bs G_i$ is the subset of elements in $\bs G$ relating to observations in $\bs P_i$, 
%  is the corresponding subset of elements in $\bs Q$,
%  is the covariance between the items referred to by pairs in $\bs P_i$ 
% and the inducing points,
% and  contains the corresponding rows of $\bs A$.
The update equations optimize the natural parameters of the Gaussian distribution by following the
natural gradient~\citep{hensman2015scalable}:
\begin{flalign}
\bs S^{-1}_i  & = (1 - \rho_i) \bs S^{-1}_{i-1} + \rho_i\left( \mathbb{E}[s]\bs K_{mm}^{-1} + w_i\bs A_i^T \bs G^T_{i} \bs Q^{-1}_i \bs G_{i} \bs A_{i} \right)& 
\label{eq:S_stochastic} \\
\hat{\bs f}_{m,i}  & = \bs S_i \left( (1 - \rho_i) \bs S^{-1}_{i-1} \hat{\bs f}_{m,i-1}  + 
\right. \nonumber \\
& \left.\hspace{1.5cm} \rho_i w_i  
\bs A_{i}^{T} \bs G_{i}^T \bs Q_i^{-1} \left( \bs y_i  - \Phi(\mathbb{E}[\bs z_i]) + \bs G_{i} \bs A_i \hat{\bs f}_{m,i} \right) \right) & 
\label{eq:fhat_stochastic}
\end{flalign}
where
$\rho_i=(i + \epsilon)^{-r}$ is a mixing coefficient that controls the update rate,
$w_i = \frac{P}{|P_i|}$ weights each update according so sample size,
 $\epsilon$ is a delay hyperparameter and $r$ is a forgetting rate~\citep{hoffman2013stochastic}.

For the inverse scale, %, $s$, %can also be learned as part of the SVI procedure. Its variational factor,
$q(s)$ is updated using Equation \ref{eq:qs}, then its expected value is computed as %has the following update equations:
$\mathbb{E}[s] = \frac{\alpha}{\beta}$.

\begin{algorithm}[t]
 \KwIn{ Pairwise labels, $\bs y$, training item features, $\bs x$, 
 test item features $\bs x^*$}
 \nl Compute kernel matrices $\bs K$, $\bs K_{mm}$ and $\bs K_{nm}$ given $\bs x$
 \nl Initialise $\mathbb{E}[s]$, $\mathbb{E}[\bs f]$and $\hat{\bs f}_m$ to prior means
 and $\bs S$ to prior covariance $\bs K_mm$\;
 \While{$\mathcal{L}$ not converged}
 {
 \nl Select random sample, $\bs P_i$, of $P$ observations
 \While{$\bs G_i$ not converged}
  {
  \nl Compute $\bs G_i$ given $\mathbb{E}[\bs f_i]$ \;
  \nl Compute $\hat{\bs f}_{m,i}$ and $\bs S_{i}$ \;
  \nl Compute $\mathbb{E}[\bs f_i]$ \;
  }
 \nl Update $q(s)$ and compute $\mathbb{E}[s]$ and $\mathbb{E}[\log s]$\;
 }
\nl Compute kernel matrices for test items, $\bs K_{**}$ and $\bs K_{*m}$, given $\bs x^*$ \;
\nl Use converged values of $\mathbb{E}[\bs f]$and $\hat{\bs f}_m$ to estimate
posterior over $\bs f^*$ at test points \;
\KwOut{ Posterior mean of the test values, $\mathbb{E}[\bs f^*]$ and covariance, $\bs C^*$ }
\caption{The SVI algorithm for preference learning with a single user.}
\label{al:singleuser}
\end{algorithm}
The complete SVI algorithm is summarized in Algorithm \ref{al:singleuser}.
The use of an inner loop to learn $\bs G_i$ avoids the need to store the complete matrix, 
$\bs G$.
The inferred distribution over the inducing points can be used 
for predicting the values of test items, $f(\bs x^*)$:
\begin{flalign}
\bs f^* &= \bs K_{*m} \bs K_{mm}^{-1} \hat{\bs f}_m, \\
\bs C^* &= \bs K_{**} + \bs K_{*m} \bs K_{mm}^{-1} (\bs S - \bs K_{mm} / \mathbb{E}[s] ) \bs K_{*m}^T \bs K_{mm}^{-1},
\end{flalign}
where $\bs C^*$ is the posterior covariance of the test items, $\bs K_{**}$ is their prior covariance, and
$\bs K_{*m}$ is the covariance between test and inducing items.
%It is possible to recover the lower bound proposed by 
%\citet{hensman2015scalable} for classification by generalizing the
%likelihood to arbitrary nonlinear functions, and omitting terms relating to $p(s|\alpha_0,\beta_0)$ and $q(s)$.
% However, our approach avoids expensive quadrature methods by linearizing the likelihood to enable analytical updates. We also infer $s$ in a Bayesian manner, 
% rather than treating as a hyper-parameter, which is important for preference learning where $s$ controls the noise level of the observations relative to  $f$. 

\subsection{SVI for CrowdGPPL}

We now extend the SVI method to the crowd preference learning model proposed in
Section \ref{sec:crowd_model}.
To begin with, we extend the variational posterior in Equation \ref{eq:svi_approx}
to approximate the crowd model defined in Equation \ref{eq:joint_crowd}:
\begin{flalign}
& p( \bs V, \bs V_m, \bs W, \bs W_m, \bs t, \bs t_m, s_1, ..., s_C, \sigma | \bs y, \bs x, \bs x_m, \bs u, \bs u_m, k, \alpha_0, \beta_0 ) \approx & \nonumber \\
& \hspace{3.2cm} q(\bs V)q(\bs W)q(\bs t)q(\bs V_m)q(\bs W_m)q(\bs t_m)\prod_{c=1}^{C}q(s_c)q(\sigma), &
\end{flalign}
where $\bs u_m$ are the feature vectors of inducing users. By updating 
each of the $q$ factors in turn,
the SVI procedure optimizes the lower bound on the marginal likelihood 
(see also Equation \ref{eq:lowerbound_crowd_full} in the Appendix):
\begin{flalign}
& \mathcal{L}_{cr} = \label{eq:lowerbound_crowd}
\mathbb{E}_{q%(\bs t, \bs t_m, \bs V, \bs V_m, \bs W, \bs W_m, s_1,...,s_c,\sigma)
} \left[ 
\log p(\bs y | \bs F) +
\sum_{c=1}^C \left\{ \log p(\bs v_{m,c}, s_c | \bs K_{mm}, \alpha_0, \beta_0) - \log q(\bs v_{m,c}) \right. \right. & \nonumber\\
& \left. \left. + \log p(\bs w_{m,c} | \bs K_{w,mm} )
 - \log q(\bs w_{m,c} ) \right\}
+ \log p(\bs t_m, \sigma | \bs K_{mm}, \alpha_0, \beta_0)
- \log q(\bs t_m)
\right]. &
\end{flalign}
The algorithm follows the same pattern as Algorithm \ref{al:singleuser}, computing means and covariances
for  $\bs V_m$, $\bs W_m$ and $\bs T_m$ instead of $\bs f_m$ using the equations given below.

The expectations for the inverse scales, $s_1,...,s_c$ and $\sigma$, can be 
computed using Equation \ref{eq:qs} by
substituting in the corresponding terms for each $\bs v_c$ or $\bs t$ instead of $\bs f$. 
%This approximation factorizes the joint distribution between the latent item factors, $\bs V$, the latent user factors, $\bs W$, and the common means, $\bs t$. 
%but does not requiring factorization across the latent dimensions $\bs w_1,...,\bs w_C$ and $\bs v_1,...,\bs v_C$.
The variational factor for the inducing item factors is:
%TODO make the vectors all lower case for V_c and W_c
%TODO do something to make sure all the block diags are properly indiciated including A_v... but actually I think they are useless because the off-diagonal blocks only affect the collapsed posterior covariance and are never used in computing any other variables.
%TODO intialization of the factors
%TODO put definition of f in somewhere in the model section 
%TODO big Sigma is variance of W because it's different...
%TODO why is the scaling by W nice and simple without off-diagonals, but scaling by V is not? I think that when Q is diagonal, off-diagonals in any scaling factors are irrelevant.
\begin{flalign}
\log q(\bs V_m) = \;\;& \mathbb{E}\left[\log \mathcal{N}\left( \bs y; \tilde{\Phi}(\bs z), Q \right)\right] 
 + \sum_{c=1}^C \log\mathcal{N}\left(\bs v_{m,c}; \bs 0, \frac{\bs K_{v,mm}}{\mathbb{E}[s_c]}\right) 
+ \textrm{const} & \nonumber \\
% are the dimensions collapsed to a single MVN?
= \;& \sum_{c=1}^C \log \mathcal{N}(\bs v_{m,c}; \hat{\bs v}_{m,c}, \bs S_{v,c}). &
\end{flalign}
The precision estimate for $\bs S^{-1}_{v,c}$ at iteration $i$ is given by:
\begin{flalign}
\bs S_{v,c,i}^{-1} & = (1-\rho_i)\bs S^{-1}_{v,c,i-1} + \rho_i\left( \bs K^{-1}_{v,mm}\mathbb{E}[s_c] 
\right. \nonumber & \\ 
& \left. \hspace{1.5cm} + w_i \bs A_{v,i}^T \bs G_i^T \textrm{diag}(\hat{\bs w}_{c,\bs u}^2 + \bs\Sigma_{c,\bs u,\bs u})\bs Q_i^{-1} \bs G_i \bs A_{v,i} \right), &
\end{flalign}
where $\bs A_{i} = \bs K_{im} \bs K_{mm}^{-1}$, 
$\hat{\bs w}_{c}$ and $\bs\Sigma_{c}$ are the variational mean and covariance of 
the $c$th latent user component (defined below in Equations \ref{eq:what} and \ref{eq:Sigma}),
and ${\bs u} = \{ u_p \forall p \in P_i \}$ is the vector of user indexes in the sample of observations.
%The term $\textrm{diag}(\hat{\bs w}_{c,\bs j}^2 + \bs\Sigma_{c,\bs j})$ 
%scales the diagonal observation precision, $\bs Q^{-1}$, by the latent user factors.
We use $\bs S_{v,c}^{-1}$ to compute the means for each row of $\bs V_m$:
\begin{flalign}
\hat{\bs v}_{m,c,i} & = \bs S_{v,c,i}\left( 
(1-\rho_i)\bs S^{-1}_{v,c,i-1}\hat{\bs v}_{m,c,i-1} + \rho_i w_i \bigg(
\bs S_{v,c,i} \bs A_{i}^T \bs G_i^T \textrm{diag}(\hat{\bs w}_{c,\bs u}) \bs Q_i^{-1} \right. & \nonumber \\
&  \Big(\bs y_i - \Phi(\mathbb{E}[\bs z_i]) + \sum_{j=1}^U \bs H^{(i)}_{j}(\hat{\bs v}_c^T \hat{\bs w}_{c,j})\Big) \bigg) \bigg), &
\end{flalign}
where $\bs H^{(i)}_{j} \in |P_i| \times N$ contains partial derivatives of the pairwise likelihood
with respect to $F_{n,j} = \hat{v}_{c,n} \hat{w}_{c,j}$, 
with elements given by:
\begin{flalign}
H^{(i)}_{j,p,n} & = \Phi(\mathbb{E}[z_p])(1 - \Phi(\mathbb{E}[z_p])) (2y_p - 1)( [n = a_p] - [n = b_p]) [j = u_p]. &
\end{flalign}
%This is needed to replace $\bs G$ in the single-user model, since the vector of utilities,
%$\bs f$, has been replaced by the matrix $\bs F$, where each column of $\bs F$ corresponds to a single user.

The variational component for the inducing points of the common item mean follows a similar pattern:
\begin{flalign}
\log q(\bs t_m) = \;\;& \mathbb{E}[\log \mathcal{N}\left( \bs y; \tilde{\Phi}(\bs z), Q \right)] 
+ \mathbb{E}[\log\mathcal{N}(\bs t_m; \bs 0, \bs K_{t,mm}/s)] 
+ \textrm{const} & \nonumber \\
= \;\;& \log \mathcal{N}\left( \bs t; \hat{\bs t}, \bs S_t \right) & \\
\bs S_{t,i}^{-1} = \;\;& (1-\rho_i)\bs S^{-1}_{t,i-1} + \rho_i\bs K^{-1}_{t,mm}/\mathbb{E}[\sigma] 
+ \rho_i w_i \bs A_{i}^T \bs G_i^T \bs Q_i^{-1} \bs G_i \bs A_{i} & \\
\hat{\bs t}_{m,i} = \;\;& \bs S_{t,i}\left(
(1 - \rho_i) \bs S_{t,i-1}^{-1}\hat{\bs t}_{m,i-1}  \right. & \nonumber \\
& \left. + \rho_i w_i \bs A_{i}^T \bs G_i^T \bs Q_i^{-1}
\left(\bs y_i - \Phi(\mathbb{E}[\bs z_i]) + \bs G_i \bs A_{i} \hat{\bs t}_{m,i} \right) \right). &
\end{flalign}

Finally, %require a different linearization matrix, $\bs J \in P \times U$, containing partial derivatives 
%of the pairwise likelihood with respect to $\hat{w}_c$. Its elements are given by:
%\begin{flalign}
%J_{p,j} = \Phi(\mathbb{E}[z_p])(1 - \Phi(\mathbb{E}[z_p]) (2y_p - 1) [u_p = j] % needs to be added or subtracted depending on a or b
%\end{flalign} 
%now multiply by V. What about covariances between v?
the variational distribution for the inducing users factors is:% then as follows:
\begin{flalign}
\log q(\bs W_m) = \;\;& \mathbb{E}\left[\log \mathcal{N}\left( \bs y; \tilde{\Phi}(\bs z), Q \right)\right] 
+ \sum_{c=1}^C \mathbb{E}[\log\mathcal{N}(\bs w_c; \bs 0, \bs K_{w,mm})]
+ \textrm{const} & \nonumber \\
= \;\;& \sum_{c=1}^C \log \mathcal{N}\left( \bs w_c; \hat{\bs w}_c, \bs \Sigma \right). & 
\end{flalign}
The SVI updates for the parameters are:
\begin{flalign}
& \bs \Sigma^{-1}_{c,i} = (1-\rho_i)\bs \Sigma^{-1}_{c,i-1}
+ \rho_i\bs K^{-1}_{w,mm}
+ \rho_i w_i \bs A_{w,i}^T \bigg( \sum_{p \in P_i} \bs H^{(i)T}_{.,p} \textrm{diag}\left(\hat{\bs v}_{c,\bs a}^2 + \right. &\nonumber \\
& \left. \bs S_{c,\bs a, \bs a} + 
\hat{\bs v}_{c,\bs b}^2 + \bs S_{c,\bs b, \bs b}  
- 2\hat{\bs v}_{c,\bs a}\hat{\bs v}_{c,\bs b} - 2\bs S_{c,\bs a, \bs b} \right) \bs Q_i^{-1} \sum_{p \in P_i} \bs H^{(i)}_{.,p} \bigg) \bs A_{w,i} & \label{eq:Sigma} \\
%%%%
& \hat{\bs w}_{m,c,i} = \bs \Sigma_{c,i} \bigg( (1 - \rho_i)\bs \Sigma_{c,i-1}\hat{\bs w}_{m,c,i-1} + 
 \rho_i w_i \bs A_{w,i}^T \sum_{p \in P_i} \bs H^{(i)}_{.,p}
\left( \textrm{diag}(\hat{\bs v}_{c,\bs a}) \right. & \nonumber  \\
& \left. - \textrm{diag}(\hat{\bs v}_{c,\bs b}) \right) \bs Q_i^{-1} 
\bigg(\bs y_i - \Phi(\mathbb{E}[\bs z_i]) + \sum_{j=1}^U \bs H^{(i)}_u (\hat{\bs v}_c^T \hat{\bs w}_{c,j})\bigg) \bigg), & \label{eq:what}
\end{flalign}
where the subscripts $\bs a = \{ a_p \forall p \in P_i \}$
and  $\bs b = \{b_p \forall p \in P_i \}$ are lists of indices to the first and 
second items in the pairs, respectively, and $\bs A_{w,i} = \bs K_{w,im} \bs K_{w,mm}^{-1}$.

% The equations for the means and covariances 
% can be adapted for stochastic updating by applying weighted sums over
% the stochastic update and the previous values in the 
% same way as  Equation \ref{eq:S_stochastic} and \ref{eq:fhat_stochastic}.
% The stochastic updates for the inducing points of the latent factors depend 
% on expectations with respect to the observed points. 
% As with the single user case, the variational factors at the observed items are independent of the observations given the variational factors of the inducing points
% (likewise for the observed users):
% \begin{flalign}
% \log q(\bs V) & = \sum_{c=1}^C \log \mathcal{N}\left( \bs v_c; \bs A_v\hat{\bs v}_{m,c}, 
% \frac{\bs K_{v}}{\mathbb{E}[s_c]} + \bs A_v (\bs S_{m,c} - \frac{\bs K_{v,mm}}{\mathbb{E}[s_c]})\bs A_v \right) & \label{eq:qv} \\
% \log q(\bs t) & = \log \mathcal{N}\left( \bs t; \bs A_t \hat{\bs t}_m, 
% \frac{\bs K_{t}}{\mathbb{E}[\sigma]} + \bs A_t (\bs S_t - \frac{\bs K_{t,mm}}{\mathbb{E}[\sigma]})\bs A_t \right)  & \label{eq:qt}\\
% \log q(\bs W) & = \sum_{c=1}^C \log \mathcal{N}\left( \bs w_c; \bs A_w \hat{\bs w}_{m,c}, \bs K_{w} + \bs A_w (\bs\Sigma - \bs K_{w,mm}) \bs A_w \right). &
% \label{eq:qw}
% \end{flalign}

Predictions for crowdGPPL can be made by computing the posterior mean utilities, $\bs F^*$, 
and the covariance $\bs \Lambda_u^*$ for all users $u$ in the test set:
\begin{flalign}
&\bs F^* = \hat{\bs t}^* + \sum_{c=1}^C \hat{\bs v}_{c}^{*T} \hat{\bs w}_{c}^*, \hspace{1cm} \bs \Lambda_u^* = \bs C_{t}^* + \sum_{c=1}^C \omega_{c,u}^* \bs C_{v,c}^* + \hat{w}_{c,u}^2  \bs C_{v,c}^*  +\omega_{c,u}^* \hat{\bs v}_{c}\hat{\bs v}_{c}^T, &\\
&\hat{\bs t}^* = \bs K_{*m} \bs K^{-1}_{mm} \hat{\bs t}_{m}, \hspace{0.7cm}
\hat{\bs v}_{c}^* = \bs K_{*m} \bs K^{-1}_{mm} \hat{\bs v}_{m,c}, \hspace{0.7cm}
\hat{\bs w}_{c}^* = \bs K_{w,*m} \bs K^{-1}_{w,mm} \hat{\bs w}_{m,c}, & \\
&\bs C_{t}^* \!= \frac{\bs K_{**}}{\mathbb{E}[\sigma]} + \bs A_{*m}(\bs S_{t} \!-\! \bs K_{mm}) \bs A_{*m}^T, \hspace{0.5cm}
\bs C_{v,c}^* \!= \frac{\bs K_{**}}{\mathbb{E}[s_c]} + \bs A_{*m}(\bs S_{v,c} \!\!-\! \bs K_{mm}) \bs A_{*m}^T  & \\
&\omega_{c,u}^* = 1 + \bs A_{w,um}(\bs \Sigma_{w,c} - \bs K_{w,mm}) \bs A_{w,um}^T &
\end{flalign}
where  $\bs A_{*m}=\bs K_{*m}\bs K_{mm}^{-1}$,
$\bs A_{w,um}=\bs K_{w,um}\bs K_{w,mm}^{-1}$ and $\bs K_{w,um}$ is the covariance between user $u$ and the inducing 
users.
The mean and covariance estimates can be inserted into Equations \ref{eq:predict_z} and \ref{eq:plphi} to predict pairwise labels.
In practice, the full covariance terms are needed only for Equations \ref{eq:predict_z}, so need only be computed
between items for which we wish to predict pairwise labels. %Hence, the covariance for a large test set need not be computed at once.

%In this section, we proposed an SVI scheme for GPPL and crowdGPPL, 
%which can readily be adapted to regression or classification tasks by swapping out the preference likelihood, resulting in 
%different values for $\bs G$ and $\bs H$. 
% We now show how to learn the  
% length-scale parameters required to compute the prior covariances using typical kernel functions.
%then demonstrate how our method can be applied to learning user preferences or
%consensus opinion when faced with disagreement.
 
