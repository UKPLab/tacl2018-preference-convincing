\section{Scalable Inference}\label{sec:inf}

In the single user case, the goal is to infer the posterior distribution over the utilities of test items, $\bs f^*$, 
given a set of pairwise training labels, $\bs y$. In the multi-user case, we aim to find the posterior over the matrix
$\bs F^*=\bs V^{*T} \bs W^*$ of utilities for test items and test users.
The non-Gaussian likelihood makes exact inference intractable, hence previous work has used
 the Laplace approximation for the single user case~\citep{chu2005preference}
or a combination of expectation propagation (EP) with variational Bayes (VB) for a 
multi-user model~\citep{houlsby2012collaborative}.
The Laplace approximation is a maximum a-posteriori (MAP) solution that
takes the most probable values of parameters rather than integrating over their distributions,
and has been shown to perform poorly for classification~\citep{nickisch2008approximations}. 
EP and VB approximate the true posterior with a simpler, factorized distribution
that can be learned using an iterative algorithm.
For crowdGPPL, the true posterior is multi-modal, since the latent factors can be re-ordered arbitrarily without
affecting $\bs F$, causing a \emph{non-identifiability problem}.
EP would average these modes and produce uninformative predictions over $\bs F$, so
\citet{houlsby2012collaborative} incorporate a VB step that approximates a single mode.
A drawback of EP is that unlike VB, convergence is not guaranteed~\citep{minka2001expectation}.
\todo{Page 8, line 12: VB is only guaranteed to converge for conjugate distributions.}
%do they also linearise in the same way? -- both linearise. But EP uses a joint over y and f as its approximation to p(y|f), then optimises the parameters iteratively. It's not guaranteed to converge. Variational EGP instead approximates
% p(y|f) directly with the best fit Gaussian. It's not clear whether this could be updated iteratively but it doesn't
% seem to work if done simultaneously with the other variables we need to learn (the linearisation), 
% perhaps because the algorithm for learning the weights breaks if the variance of q(y|f), Q, keeps changing. 
% Possibly because Q does not change incrementally. So it's
% possible that an outer loop could be used.

\todo{Page 8, second paragraph: you criticise GFITC for not being decentralisable.  It is
not clear to me that crowdGPPL is decentralisable.  }
%TODO: remove redundancy with the related work section. Consider whether this should actually be in a background section. NM^2 is limiting, not just the other costs. The other costs NP etc come into play in this pairwise model only.
Exact inference for a Gaussian process has computational complexity $\mathcal{O}(N^3)$ 
and memory complexity $\mathcal{O}(N^2)$, where $N$ is the number of data points.
The cost of inference can be reduced using a \emph{sparse} approximation based on a set of 
\emph{inducing points}, which act as substitutes for the set of points in the training dataset.
By choosing a fixed number of inducing points, $M \ll N$, the computational cost is cut to $\mathcal{O}(NM^2)$,
and the memory complexity to $\mathcal{O}(NM)$.
These points must be selected so as to give a good approximation, 
using either heuristics or optimizing their positions to maximize the approximate
marginal likelihood. 
The sparse approximation used by \citet{houlsby2012collaborative} for the collaborative GP 
is the \emph{generalized fully independent training conditional} (GFITC)~\citep{snelson2006sparse}.
In practice, GFITC is unsuitable for datasets with more than a few thousands points,
as the computational and memory costs cannot be constrained
and GFITC is not amenable to distributed computation~\citep{hensman2015scalable}.  
%In contrast to crowdGPPL, \citet{houlsby2012collaborative} place GPs over the space of pairs rather than items,
%which is typically much larger, meaning that $\mathcal{O}(PM^2 + UM^2)$ computational and $\mathcal{O}(PM + UM)$
%memory costs dominate. 
We derive a more scalable approach for GPPL and crowdGPPL using
stochastic variational inference (SVI), an iterative scheme that limits the computational and memory costs at
each iteration~\citep{hoffman2013stochastic}.
% how does hensman 2015 optimize inducing points? This is one selling point of SVI. 
% Advantages of VB:  Nickisch 2008 got poorer results for VB methods they tried than EP. But our method may be different?, Seeger 2000
First, we define an approximate likelihood that enables the SVI method.
%then derive the SVI method for single user GPPL. update equations for the iterative algorithm to optimize
%the approximation. We begin with single user GPPL then extend this to crowdGPPL. 

\subsection{Approximate Pairwise Likelihood}

\todo{ got really confused by Section 4.1.  In equation (7), p(y|f) is a product of the cdfs.  Where does the expectation come from just after the first equal sign? The second equal sign in (7) should be an approximation.  In equation (7), Q is (approximately) the covariance of y conditioned on f and yet, in equation (8), Q is determined by the expectation over f.  Can you please check this section and provide some further explanation.}
\todo{There are many approximations in Sec 4.1 in order to make a tractable inference. It is not clear after so many approximations, eq (7), eq (8), eq (9), eq (11)... , the resultant objective may be very far away from the original one. The properties from GP may not be effective. There is no study to check the gap.}
To obtain a tracatable approximate posterior, we begin by approximating 
the expected preference likelihood (Equation \ref{eq:plphi}) with a Gaussian:
\begin{flalign}
p(\bs y | \bs f) = \mathbb{E}\left[\prod_{p=1}^P \Phi(z_p)\right] = \prod_{p=1}^P \Phi(\hat{z}_p) \approx \mathcal{N}(\bs y; \Phi(\hat{\bs z}), \bs Q),
\label{eq:likelihood_approx}
\end{flalign}
where $\bs Q$ is a diagonal noise covariance matrix
and $\hat{\bs z}=\{\hat{z}_1, ..., \hat{z}_P\}$, and $\hat{z}_p$ is defined by 
Equation \ref{eq:predict_z}.
Since $\Phi(\hat{z}_p)$ defines a bernoulli distribution, for which the conjugate prior is a beta distribution,
we moment match the diagonal entries of $\bs Q$ to the expected variance of a bernoulli distribution as follows:
\begin{flalign}
Q_{p,p} & = \mathbb{E}_{f}[\Phi(z_p)(1 - \Phi(z_p))] 
%= \mathbb{E}_{\bs f}[\Phi(z_p)] - \mathbb{E}_{\bs f}[\Phi(z_p)]^2 - \mathbb{V}_{\bs f}[\Phi(z_p)], 
 \approx \frac{ (y_p + \gamma)(1-y_p + \lambda) }{ (\gamma + \lambda + 1)} - \frac{ (y_p + \gamma)(1-y_p + \lambda) }{ (\gamma + \lambda + 1)^2(\gamma + \lambda + 2)}, &
\end{flalign}
where $\gamma$ and $\lambda$ are parameters of a beta distribution with the same variance as the prior,
$p(\Phi(z_p) | \bs K_{\theta}, \alpha_0, \beta_0)$, and are estimated using numerical integration.
%Setting $\bs Q$ in this way matches the moments of the true likelihood, $\Phi(z_p)$,
%to those of the Gaussian approximation.

Unfortunately, the nonlinear term, $\Phi(\bs z)$ means that the posterior is still intractable, 
so we linearize $\Phi(\bs z)$ by taking its first-order Taylor series expansion
about the expectation of $\bs f$ for GPPL (for crowdGPPL, replace $\bs f$ with $\bs F$ in the following):
\begin{flalign}
\Phi(\bs z) &\approx \tilde{\Phi}(\bs z) = \bs G (\bs f-\mathbb{E}[\bs f]) + \Phi(\hat{\bs z}), \\
G_{p,i} &= \Phi(\mathbb{E}[z_p])(1 - \Phi(\hat{z}_p)) (2y_p - 1)( [i = a_p] - [i = b_p]) 
\end{flalign}
where $\bs G$ is a matrix containing elements $G_{p,i}$, which are the
partial derivatives of the pairwise likelihood with respect to each of 
the latent function values, $\bs f$.
This creates a circular dependency between the posterior mean of $\bs f$ and $\bs G$, %the linearization terms in the likelihood,
which can be estimated iteratively using variational inference~\citep{steinberg2014extended},
as we describe below.
Linearization makes the approximate likelihood conjugate to the prior, $\mathcal{N}(\bs f; \bs 0, \bs K_{\theta}/s)$,
so that the approximate posterior over $\bs f$ is also Gaussian. 
%We now use our likelihood approximation to define an approximate
%posterior over all variables for GPPL:
%use variational inference to estimate the marginal over $\bs f$,
%by optimizing an approximate posterior over all latent variables:
%\begin{flalign}
%p(\bs f, s | \bs y, \bs x, k_{\theta}, & \alpha_0, \beta_0) \approx q(s)q(\bs f), & \nonumber \\
%\textrm{where } \ln q(s) & = \mathbb{E}_{\bs f}[\ln p(s | \bs f, \alpha_0, \beta_0)] & \nonumber \\
%& = \ln \mathcal{N}(\mathbb{E}[\bs f]; \bs 0, \bs K_{\theta}/s) + \ln \mathcal{G}(s; \alpha_0, \beta_0) + \textrm{const}, & \nonumber \\
%\ln q(\bs f) & = \mathbb{E}_{s}[\ln p(\bs f | k_{\theta}, s, \bs x, \bs y) ] & \nonumber \\
%& = \ln \mathcal{N}(\bs y; \tilde{\Phi}(\bs z), \bs Q) + \ln \mathcal{N}(\bs f; \bs 0, \bs K_{\theta}/\mathbb{E}[s]) + \textrm{const}. &
%\label{eq:vb_approx}
%\end{flalign}
%don't some of the expectations over s simplify out?
%Since the Gamma is conjugate to $s$, $q(s)$ is also a Gamma distribution. 
%Likewise, the mean of the Gaussian likelihood approximation is
%a linear function of $\bs f$, 
%which has a multivariate Gaussian prior, hence
%$q(\bs f)$ is also a multivariate Gaussian.
The Gaussian likelihood approximation and linearization
also appear in GP inference methods based on expectation propagation~\citep{rasmussen_gaussian_2006} 
and the extended Kalman filter~\citep{reece2011determining,steinberg2014extended}.
%However, neither these methods nor our approximation in Equation \ref{eq:vb_approx}
%make inference sufficiently scalable, as they all require
%inversion of an $N \times N$ matrix and further computations involving $N \times P$ and $P \times P$ matrices.
We can now use the approximate likelihood to derive equations for SVI.% stochastic variational inference (SVI).

\subsection{SVI for Single User GPPL}

%TODO can we ommit the f from the posterior here?
\todo{Page 9, Section 4.2: Please state up front that you are proposing a mean-field
variational approximation in equation (11).  This will then justify why E[s] appears in equation (13). }
We introduce a sparse approximation to the Gaussian process that allows
us to limit the size of the covariance matrices we need to work with.
% that needs to be inverted,
%and permit stochastic inference methods that consider only a subset of the $P$ observations at each iteration
%~\citep{hensman2013gaussian,hensman2015scalable}. 
To do this, we introduce a set of $M \ll N$ \emph{inducing} items with inputs 
$\bs x_m$,
utilities $\bs f_m$, covariance $\bs K_{mm}$,
and covariance between the observed and inducing items, $\bs K_{nm}$.
% The inducing points act as proxies for the observed points during inference,
% and thereby reduce the number of data points we have to perform costly operations % over.
%We modify the variational approximation in Equation \ref{eq:vb_approx} to introduce the inducing points 
For clarity, we omit $\theta$ from this point on.
We assume an approximation to the posterior over the inducing and training items
that factorises between different sets of latent variables:
\begin{flalign}
p(\bs f, \bs f_m, s | \bs y, \bs x, \bs x_m, k_{\theta}, \alpha_0, \beta_0) &\approx q(\bs f, \bs f_m, s) = q(s)q(\bs f)q(\bs f_m), \label{eq:svi_approx} &&
\end{flalign}
where are $q(.)$ are \emph{variational factors}, which we define below. 
Each factor for a subset of latent variables, $\bs z$, takes the form $\ln q(\bs z) = \mathbb{E}_{\not \bs z}[\ln p(\bs z, \bs x, \bs y)]$, that is, the expectation with respect
to all other latent variables, $\not \bs z$, of the log joint distribution
of the observations and the current subset of latent variables, $\bs z$.

We marginalize $\bs f$ to obtain the factor for $\bs f_m$:
\begin{flalign}
\ln q(\bs f_m) &= \ln \mathcal{N}\left(\bs y; \tilde{\Phi}(\bs z), \bs Q\right)
+ \ln\mathcal{N}\left(\bs f_m; \bs 0, \bs K_{mm}/\mathbb{E}\left[s\right]\right)  + \textrm{const}, & \nonumber \\
 & = \ln \mathcal{N}(\bs f_m; \hat{\bs f}_m, \bs S ),
 \label{eq:fhat_m}
\end{flalign}
where the variational parameters $\hat{\bs f}_m$ and $\bs S$ are computed using the iterative SVI procedure described below.
We choose an approximation of $q(\bs f)$ that depends only on the inducing point utilities, $\bs f_m$, and is independent of the observations:
 \begin{flalign}
\ln q(\bs f) & = \ln \mathcal{N}(\bs f; \bs A \hat{\bs f}_m, 
\bs K + \bs A (\bs S - \bs K_{mm}/\mathbb{E}[s]) \bs A^T ),
\end{flalign}
where $\bs A=\bs K_{nm} \bs K^{-1}_{mm}$.
This means we no longer need to invert an $N \times N$ covariance matrix to compute $q(\bs f)$.
The factor $q(s)$ is also modified to depend on the inducing points:
\begin{flalign}
& \ln q(s) = \mathbb{E}_{q(\bs f_m)}[\ln\mathcal{N}(\bs f_m| \bs 0, \bs K_{mm}/s)] + \ln \mathcal{G}(s; \alpha_0, \beta_0) + \mathrm{const}
= \ln \mathcal{G}(s; \alpha, \beta), & \label{eq:qs}
\end{flalign}
where $\alpha= \alpha_0 + \frac{M}{2}$ and $\beta = \beta_0 + \frac{
\textrm{tr}(\bs K^{-1}_{mm}(S + \hat{\bs f}_m \hat{\bs f}_m^T))}{2}$.
%None of the other factors depend on $\ln q(\bs f)$,
%so it need not be computed unless required for prediction.

%once the other factors have already been estimated, as explained in the next subsection.

%Since the inducing points stand in for the observed locations,
%their choice affects the quality of our approximation. 
The location of inducing points can be learned
as part of the variational inference procedure ~\citep{hensman2015scalable},
or by optimizing a bound on the log marginal likelihood.
However, the former breaks the convergence guarantees, and both approaches
may add substantial computational cost. 
We find that we are able to obtain good performance by choosing inducing points up-front using K-means++~\citep{arthur2007k} with $K=M$ to  
cluster the feature vectors, 
then taking the cluster centers as inducing points that represent the spread of observations across feature space.
%Compared to standard K-means, K-means++ introduces a new method 
%for choosing the initial cluster seeds that
%provides theoretical bounds on the error function. In practice, this 
%reduces the number of poor-quality clusterings.


%\subsection{SVI for Single-User Preference Learning}

%The approximate posterior can now be optimized using stochastic variational inference (SVI),
%which uses a series of stochastic updates involving different subsets of the observations.
%Variational inference 
We can apply variational inference to iteratively reduce the KL-divergence between our approximate posterior
%$q(s)q(\bs f)q(\bs f_m)$
and the true posterior (both stated in Equation \ref{eq:svi_approx}) %, $p(s, \bs f, \bs f_m | \bs K, \alpha_0, \beta_0, \bs y)$,
by maximizing a lower bound, $\mathcal{L}$, on the log marginal likelihood (see also the detailed equations in Appendix \ref{sec:vb_eqns}):
%(see also Equation \ref{eq:full_L_singleuser} in the Appendix):%, $\ln p(\bs y | \bs K, \alpha_0, \beta_0)$ :
\begin{flalign}
\ln p(\bs y | \bs K, \alpha_0, \beta_0) = \textrm{KL}(q(\bs f, \bs f_m, s)  || p(\bs f, \bs f_m, s | \bs y, \bs K, \alpha_0, \beta_0)) 
+ \mathcal{L} &&\label{eq:lowerbound}
\\
%\end{flalign}
%Taking expectations with respect to the variational $q$ distributions, $\mathcal{L}$ is:
%\begin{flalign}
\mathcal{L} = \mathbb{E}_{q(\bs f)}[\ln p(\bs y | \bs f)]
+ \mathbb{E}_{q(\bs f_m, s))}[\ln p(\bs f_m, s | \bs K, 
\alpha_0, \beta_0) -\ln q(\bs f_m) - \ln & q(s) ]. && \nonumber
\end{flalign}
%         invK_mm_expecFF = self.invK_mm.dot(self.uS + self.um_minus_mu0.dot(self.um_minus_mu0.T))
%         self.rate_s = self.rate_s0 + 0.5 * np.trace(invK_mm_expecFF)
We optimize $\mathcal{L}$ by initializing the $q$ factors randomly, then
updating each one in turn, taking expectations with respect to the other factors. 
The only term in $\mathcal{L}$ that refers to the observations, $\bs y$, 
is a sum of $P$ terms, each of which refers to one observation only.
This means that $\mathcal{L}$ can be maximized iteratively by considering a random subset of 
observations at each iteration~\citep{hensman2013gaussian}.
%Therefore, the SVI solution replaces Equations \ref{eq:fhat_m} and \ref{eq:S} for computing
%$\hat{\bs f}_m$ and $\bs S$ over all observations with a sequence of stochastic updates.
For the $i$th update of $q(\bs f_m)$, we randomly select observations $\bs y_i = \{ y_p \forall p \in \bs P_i \}$, where $\bs P_i$ is random subset of indexes of observations.
%Rather than using the complete matrices, 
We then perform updates using $\bs Q_i$ (rows and columns of $\bs Q$ for observations in $\bs P_i$),
$\bs K_{im}$ and $\bs A_i$, (rows of $\bs K_{nm}$ and $\bs A$ referred to by $\{y_p \forall p \in \bs P_i\}$),
$\bs G_i$ (rows of $\bs G$ in $\bs P_i$ and columns referred to by any items $\{a_p \forall p \in \bs P_i \} \cup \{ b_p \forall p \in \bs P_i\}$),
and $\hat{\bs z}_i = \{ \mathbb{E}[\bs z_p] \forall p \in P_i \}$.
%All matrices with subscript $_i$ contain only the subset of elements relating to 
%observations in $\bs P_i$.
% The linearization matrix $\bs G_i$ is the subset of elements in $\bs G$ relating to observations in $\bs P_i$, 
%  is the corresponding subset of elements in $\bs Q$,
%  is the covariance between the items referred to by pairs in $\bs P_i$ 
% and the inducing points,
% and  contains the corresponding rows of $\bs A$.
The update equations optimize the natural parameters of the Gaussian distribution by following the
natural gradient~\citep{hensman2015scalable}:
\begin{flalign}
\bs S^{-1}_i  & = (1 - \rho_i) \bs S^{-1}_{i-1} + \rho_i\left( \mathbb{E}[s]\bs K_{mm}^{-1} + \pi_i\bs A_i^T \bs G^T_{i} \bs Q^{-1}_i \bs G_{i} \bs A_{i} \right)& 
\label{eq:S_stochastic} \\
\hat{\bs f}_{m,i}  & = \bs S_i \left( (1 - \rho_i) \bs S^{-1}_{i-1} \hat{\bs f}_{m,i-1}  + 
%\right. \nonumber \\
%& \left.\hspace{1.5cm} 
\rho_i \pi_i  
\bs A_{i}^{T} \bs G_{i}^T \bs Q_i^{-1} \left( \bs y_i  - \Phi(\mathbb{E}[\bs z_i]) + \bs G_{i} \bs A_i \hat{\bs f}_{m,i} \right) \right) & 
\label{eq:fhat_stochastic}
\end{flalign}
where
$\rho_i=(i + \epsilon)^{-r}$ is a mixing coefficient that controls the update rate,
$\pi_i = \frac{P}{|P_i|}$ weights each update according to sample size,
 $\epsilon$ is a delay hyperparameter and $r$ is a forgetting rate~\citep{hoffman2013stochastic}.
For the inverse scale, %, $s$, %can also be learned as part of the SVI procedure. Its variational factor,
$q(s)$ is updated using Equation \ref{eq:qs}, then its expected value is computed as %has the following update equations:
$\mathbb{E}[s] = \frac{\alpha}{\beta}$.

\begin{algorithm}[t]
 \KwIn{ Pairwise labels, $\bs y$, training item features, $\bs x$, 
 test item features $\bs x^*$}
 \nl Compute kernel matrices $\bs K$, $\bs K_{mm}$ and $\bs K_{nm}$ given $\bs x$
 \nl Initialise $\mathbb{E}[s]$, $\mathbb{E}[\bs f]$and $\hat{\bs f}_m$ to prior means
 and $\bs S$ to prior covariance $\bs K_mm$\;
 \While{$\mathcal{L}$ not converged}
 {
 \nl Select random sample, $\bs P_i$, of $P$ observations
 \While{$\bs G_i$ not converged}
  {
  \nl Compute $\bs G_i$ given $\mathbb{E}[\bs f_i]$ \;
  \nl Compute $\hat{\bs f}_{m,i}$ and $\bs S_{i}$ \;
  \nl Compute $\mathbb{E}[\bs f_i]$ \;
  }
 \nl Update $q(s)$ and compute $\mathbb{E}[s]$ and $\mathbb{E}[\ln s]$\;
 }
\nl Compute kernel matrices for test items, $\bs K_{**}$ and $\bs K_{*m}$, given $\bs x^*$ \;
\nl Use converged values of $\mathbb{E}[\bs f]$and $\hat{\bs f}_m$ to estimate
posterior over $\bs f^*$ at test points \;
\KwOut{ Posterior mean of the test values, $\mathbb{E}[\bs f^*]$ and covariance, $\bs C^*$ }
\vspace{0.5cm}
\caption{The SVI algorithm for preference learning with a single user.}
\label{al:singleuser}
\end{algorithm}
The complete SVI algorithm is summarized in Algorithm \ref{al:singleuser}.
The use of an inner loop to learn $\bs G_i$ avoids the need to store the complete matrix, 
$\bs G$.
The inferred distribution over the inducing points can be used 
for predicting the values of test items, $f(\bs x^*)$:
\begin{flalign}
\bs f^* \! \! &= \bs K_{*m} \bs K_{mm}^{-1} \hat{\bs f}_m, &
\bs C^* \! \! = \bs K_{**} + \bs K_{*m} \bs K_{mm}^{-1} (\bs S - \bs K_{mm} / \mathbb{E}[s] ) \bs K_{*m}^T \bs K_{mm}^{-1},
\end{flalign}
where $\bs C^*$ is the posterior covariance of the test items, $\bs K_{**}$ is their prior covariance, and
$\bs K_{*m}$ is the covariance between test and inducing items.
%It is possible to recover the lower bound proposed by 
%\citet{hensman2015scalable} for classification by generalizing the
%likelihood to arbitrary nonlinear functions, and omitting terms relating to $p(s|\alpha_0,\beta_0)$ and $q(s)$.
% However, our approach avoids expensive quadrature methods by linearizing the likelihood to enable analytical updates. We also infer $s$ in a Bayesian manner, 
% rather than treating as a hyper-parameter, which is important for preference learning where $s$ controls the noise level of the observations relative to  $f$. 

\subsection{SVI for CrowdGPPL}

We now extend the SVI method to the crowd preference learning model proposed in
Section \ref{sec:crowd_model}.
To begin with, we extend the variational posterior in Equation \ref{eq:svi_approx}
to approximate the crowd model defined in Equation \ref{eq:joint_crowd}:
\begin{flalign}
& p( \bs V, \bs V_m, \bs W, \bs W_m, \bs t, \bs t_m, s_1, ..., s^{(v)}_c, s^{(t)} | \bs y, \bs x, \bs x_m, \bs u, \bs u_m, k, \alpha_0, \beta_0 ) & \\
& 
\approx 
q(\bs t) q(\bs t_m)
\prod_{c=1}^{C} q(\bs v_{c})q(\bs w_c)q(\bs v_{c,m})q(\bs w_{c,m})
q(s^{(v)}_c)q(s^{(t)})
= q(\bs F) q(s^{(t)}) \prod_{c=1}^C q(s^{(v)}_c), & \nonumber 
\end{flalign}
where $\bs u_m$ are the feature vectors of inducing users. By updating 
each of the $q$ factors in turn,
the SVI procedure optimizes the lower bound on the marginal likelihood 
(see also Equation \ref{eq:lowerbound_crowd_full} in the Appendix):
\begin{flalign}
\mathcal{L}_{cr} & = 
\mathbb{E}_{q(\bs F)}%(\bs t, \bs t_m, \bs V, \bs V_m, \bs W, \bs W_m, s_1,...,s^{(v)}_c,s^{(t)})
[\ln p(\bs y | \bs F)] 
+ \mathbb{E}_{q(\bs t_m), q(s^{(t)})}[\ln p(\bs t_m, s^{(t)} | \bs K_{mm}, \alpha_0, \beta_0)
- \ln q(\bs t_m)] & \nonumber \\
&\sum_{c=1}^C \!\! \bigg\{ 
\mathbb{E}_{q(\bs v_{m,c}), q(s^{(v)}_c)}[\ln p(\bs v_{m,c}, s^{(v)}_c | \bs K_{mm}, \alpha_0, \beta_0) - \ln q(\bs v_{m,c})]
&  \nonumber \\ 
 & +  \mathbb{E}_{q(\bs w_{m,c})}[\ln p(\bs w_{m,c} | \bs L_{mm}/s^{(w)}_c )
  - \ln q(\bs w_{m,c} ) ] \bigg\} . & 
  \label{eq:lowerbound_crowd}
\end{flalign}
The algorithm follows the same pattern as Algorithm \ref{al:singleuser}, computing means and covariances
for  $\bs V_m$, $\bs W_m$ and $\bs T_m$ instead of $\bs f_m$.
%This approximation factorizes the joint distribution between the latent item factors, $\bs V$, the latent user factors, $\bs W$, and the common means, $\bs t$. 
%but does not requiring factorization across the latent dimensions $\bs w_1,...,\bs w_C$ and $\bs v_1,...,\bs v_C$.
The variational factor for the inducing item factors is:
%TODO make the vectors all lower case for V_c and W_c
%TODO do something to make sure all the block diags are properly indiciated including A_v... but actually I think they are useless because the off-diagonal blocks only affect the collapsed posterior covariance and are never used in computing any other variables.
%TODO intialization of the factors
%TODO put definition of f in somewhere in the model section 
%TODO big Sigma is variance of W because it's different...
%TODO why is the scaling by W nice and simple without off-diagonals, but scaling by V is not? I think that when Q is diagonal, off-diagonals in any scaling factors are irrelevant.
\begin{flalign}
\ln q(\bs v_{m,c}) = \;\;& % \mathbb{E}_{q(\bs F)}\left[ doesn't need this because it's already implicit in tilde Phi z
\ln \mathcal{N}\left( \bs y; \tilde{\Phi}(\bs z), Q \right) % \right] 
 + \ln\mathcal{N}\left(\bs v_{m,c}; \bs 0, \bs K_{mm}/ \mathbb{E}[s^{(v)}_c]\right) 
+ \textrm{const} & \nonumber \\
% are the dimensions collapsed to a single MVN?
= \;&  \ln \mathcal{N}(\bs v_{m,c}; \hat{\bs v}_{m,c}, \bs S_{v,c}), &
\end{flalign}
where the posterior mean $\hat{\bs v}_{m,c}$ and covariance $\bs S_{v,c}$ are computed using 
update equations of the same form as those of the single user GPPL in 
Equations \ref{eq:S_stochastic} and \ref{eq:fhat_stochastic}.
For reasons of space, we provide the complete equations for $\hat{\bs v}_{m,c}$ and $\bs S_{v,c}$ in 
Appendix \ref{sec:post_params}, Equations \ref{eq:Sv} and \ref{eq:hatv}.
The variational component for the inducing points of the common item mean follows a similar pattern:
\begin{flalign}
\ln q(\bs t_m) = \;\;& %\mathbb{E}_{q(\bs F)}[
\ln \mathcal{N}\left( \bs y; \tilde{\Phi}(\bs z), Q \right) %] 
+ \ln\mathcal{N}(\bs t_m; \bs 0, \bs K_{mm}/\mathbb{E}[s^{(t)}])
+ \textrm{const} & \nonumber \\
= \;\;& \ln \mathcal{N}\left( \bs t_m; \hat{\bs t}_{m}, \bs s^{(t)} \right), &
\end{flalign}
where again, the posterior mean $\hat{\bs t}$ and covariance $\bs S_{t}$ use similar updates to Equation
Equations \ref{eq:S_stochastic} and \ref{eq:fhat_stochastic}, and their full definitions
are given in Appendix \ref{sec:post_params}, Equations \ref{eq:St} and \ref{eq:hatt}.
Finally, %require a different linearization matrix, $\bs J \in P \times U$, containing partial derivatives 
%of the pairwise likelihood with respect to $\hat{w}_c$. Its elements are given by:
%\begin{flalign}
%J_{p,j} = \Phi(\mathbb{E}[z_p])(1 - \Phi(\mathbb{E}[z_p]) (2y_p - 1) [u_p = j] % needs to be added or subtracted depending on a or b
%\end{flalign} 
%now multiply by V. What about covariances between v?
the variational distribution for the inducing users factors is:% then as follows:
\begin{flalign}
\ln q(\bs w_{m,c}) = \;\;& %\mathbb{E}_{q(\bs F)}\left[
\ln \mathcal{N}\left( \bs y; \tilde{\Phi}(\bs z), Q \right) %\right] 
+ \ln\mathcal{N}(\bs w_{m,c}; \bs 0, \bs L_{mm}/s^{(w)}_c)
+ \textrm{const} & \nonumber \\
= \;\;& \ln \mathcal{N}\left( \bs w_{m,c}; \hat{\bs w}_{m,c}, \bs \Sigma_c \right), & 
\end{flalign}
where $\hat{\bs w}_c$ and $\bs \Sigma_{c}$ also follow the pattern of
Equations \ref{eq:S_stochastic} and \ref{eq:fhat_stochastic}, and their full definitions
are given in Appendix \ref{sec:post_params}, Equations \ref{eq:St} and \ref{eq:hatt}.
The expectations for the inverse scales, $s_1,...,s^{(v)}_c$ and $s^{(t)}$, can be 
computed using Equation \ref{eq:qs} by
substituting in the corresponding terms for each $\bs v_c$ or $\bs t$ instead of $\bs f$. 
% The equations for the means and covariances 
% can be adapted for stochastic updating by applying weighted sums over
% the stochastic update and the previous values in the 
% same way as  Equation \ref{eq:S_stochastic} and \ref{eq:fhat_stochastic}.
% The stochastic updates for the inducing points of the latent factors depend 
% on expectations with respect to the observed points. 
% As with the single user case, the variational factors at the observed items are independent of the observations given the variational factors of the inducing points
% (likewise for the observed users):
% \begin{flalign}
% \ln q(\bs V) & = \sum_{c=1}^C \ln \mathcal{N}\left( \bs v_c; \bs A_v\hat{\bs v}_{m,c}, 
% \frac{\bs K_{v}}{\mathbb{E}[s^{(v)}_c]} + \bs A_v (\bs S_{m,c} - \frac{\bs K_{mm}}{\mathbb{E}[s^{(v)}_c]})\bs A_v \right) & \label{eq:qv} \\
% \ln q(\bs t) & = \ln \mathcal{N}\left( \bs t; \bs A_t \hat{\bs t}_m, 
% \frac{\bs K_{t}}{\mathbb{E}[s^{(t)}]} + \bs A_t (\bs s^{(t)} - \frac{\bs K_{mm}}{\mathbb{E}[s^{(t)}]})\bs A_t \right)  & \label{eq:qt}\\
% \ln q(\bs W) & = \sum_{c=1}^C \ln \mathcal{N}\left( \bs w_c; \bs A_w \hat{\bs w}_{m,c}, \bs L_{} + \bs A_w (\bs\Sigma - \bs L_{mm}/s^{(w)}_c) \bs A_w \right). &
% \label{eq:qw}
% \end{flalign}

Predictions for crowdGPPL can be made by computing the posterior mean utilities, $\bs F^*$, 
and the covariance $\bs \Lambda_u^*$ for each user, $u$, in the test set:
\begin{flalign} \label{eq:predict_crowd}
&\bs F^* = \hat{\bs t}^* + \sum_{c=1}^C \hat{\bs v}_{c}^{*T} \hat{\bs w}_{c}^*, \hspace{1cm} \bs \Lambda_u^* = \bs C_{t}^* + \sum_{c=1}^C \omega_{c,u}^* \bs C_{v,c}^* + \hat{w}_{c,u}^2  \bs C_{v,c}^*  +\omega_{c,u}^* \hat{\bs v}_{c}\hat{\bs v}_{c}^T, &
\end{flalign}
where $\hat{\bs t}^*$, $\hat{\bs v}_{c}^*$ and $\hat{\bs w}_{c}^*$ are posterior means of the predictions,
$\bs C_{t}^*$ and $\bs C_{v,c}^*$ are posterior item covariances of the predictions,
and $\omega_{c,u}^*$ is the posterior variance for the user components for the predicted users. These
terms are defined in Appendix \ref{sec:predictions}, Equations \ref{eq:tstar} to \ref{eq:omegastar}.
The mean $\bs F^*$ and covariances $\Lambda^*_u$ can be inserted into Equations \ref{eq:predict_z} and \ref{eq:plphi} to predict pairwise labels.
In practice, the full covariance terms are needed only for Equations \ref{eq:predict_z}, so need only be computed
between items for which we wish to predict pairwise labels. %Hence, the covariance for a large test set need not be computed at once.

%In this section, we proposed an SVI scheme for GPPL and crowdGPPL, 
%which can readily be adapted to regression or classification tasks by swapping out the preference likelihood, resulting in 
%different values for $\bs G$ and $\bs H$. 
% We now show how to learn the  
% length-scale parameters required to compute the prior covariances using typical kernel functions.
%then demonstrate how our method can be applied to learning user preferences or
%consensus opinion when faced with disagreement.
 
