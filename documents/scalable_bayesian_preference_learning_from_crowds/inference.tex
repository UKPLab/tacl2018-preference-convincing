\section{Scalable Inference}\label{sec:inf}

In the single user case, the goal is to infer the posterior distribution over the utilities of test items, $\bs f^*$, 
given a set of pairwise training labels, $\bs y$. In the multi-user case, we aim to find the posterior over the matrix
$\bs F^*=\bs V^{*T} \bs W^*$ of utilities for test items and test users,
and the posterior over consensus utilities for test items, $\bs t^*$.
The non-Gaussian likelihood makes exact inference intractable, hence previous work has used
 the Laplace approximation for the single user case~\citep{chu2005preference}
or a combination of expectation propagation (EP) with variational Bayes (VB) for a 
multi-user model~\citep{houlsby2012collaborative}.
The Laplace approximation is a maximum a-posteriori (MAP) solution that
takes the most probable values of parameters rather than integrating over their distributions,
and has been shown to perform poorly for classification~\citep{nickisch2008approximations}. 
EP and VB approximate the true posterior with a simpler, factorized distribution
that can be learned using an iterative algorithm.
For crowdGPPL, the true posterior is multi-modal, since the latent factors can be re-ordered arbitrarily without
affecting $\bs F$, causing a \emph{non-identifiability problem}.
EP would average these modes and produce uninformative predictions over $\bs F$, so
\citet{houlsby2012collaborative} incorporate a VB step that approximates a single mode.
A drawback of EP is that convergence is not guaranteed,
whereas VB will always converge when distributions are conjugate 
~\citep{minka2001expectation}.
%do they also linearise in the same way? -- both linearise. But EP uses a joint over y and f as its approximation to p(y|f), then optimises the parameters iteratively. It's not guaranteed to converge. Variational EGP instead approximates
% p(y|f) directly with the best fit Gaussian. It's not clear whether this could be updated iteratively but it doesn't
% seem to work if done simultaneously with the other variables we need to learn (the linearisation), 
% perhaps because the algorithm for learning the weights breaks if the variance of q(y|f), Q, keeps changing. 
% Possibly because Q does not change incrementally. So it's
% possible that an outer loop could be used.

%TODO: remove redundancy with the related work section. Consider whether this should actually be in a background section. NM^2 is limiting, not just the other costs. The other costs NP etc come into play in this pairwise model only.
Exact inference for a Gaussian process has computational complexity $\mathcal{O}(N^3)$ 
and memory complexity $\mathcal{O}(N^2)$, where $N$ is the number of data points.
The cost of inference can be reduced using a \emph{sparse} approximation based on a set of 
\emph{inducing points}, which act as substitutes for the set of points in the training dataset.
By choosing a fixed number of inducing points, $M \ll N$, the computational cost is cut to $\mathcal{O}(NM^2)$,
and the memory complexity to $\mathcal{O}(NM)$.
These points must be selected to give a good approximation
using either heuristics or by optimizing their positions to maximize the approximate
marginal likelihood. 

One such sparse approximation is the \emph{generalized fully independent training conditional} (GFITC)~\citep{NIPS2007_3351}, 
used by \citet{houlsby2012collaborative} for the collaborative GP,
which generalizes the fully independent training conditional 
(FITC)~\citep{snelson2006sparse} method to non-Gaussian likelihoods.
FITC assumes that each training and test point is independent of all other points
given the function values at the inducing points.
However, this may be less appropriate for the pairwise likelihood (Equation \ref{eq:plphi})
because it ignores the covariance between the utilities of the items in the training
pairs.
In practice, memory costs that grow linearly with $\mathcal{O}(N)$
start to become a problem with more than a few thousands points,
which prohibits the use of GFITC in many large datasets. 
It is also unclear how to apply distributed computation to GFITC to tackle the growing
computational costs~\citep{hensman2015scalable}.

We now derive a more scalable approach for GPPL and crowdGPPL using
stochastic variational inference (SVI), an iterative scheme that limits the computational and memory costs at
each iteration~\citep{hoffman2013stochastic}.
%and allows training data to be split into mini-batches for parallel processing.
As we explain below,
this allows us to reduce the computational cost of each iteration of the algorithm from 
$\mathcal{O}(NM^2)$ with GFITC to $\mathcal{O}(P_i M^2 + Pi^2 M + M^3)$,
where $P_i$ is the mini-batch size, 
and memory complexity from $\mathcal{O}(NM)$ to
$\mathcal{O}(P_i M + M^2  + Pi^2)$.
Neither $P_i$ nor $M$ are dependent on the size of the dataset, meaning that SVI 
can be run with arbitrarily large datasets.
First, we define a suitable likelihood approximation to enable the use of SVI.

\subsection{Approximating the Posterior with a Pairwise Likelihood}

The preference likelihood in Equation \ref{eq:plphi} 
is not conjugate with the Gaussian process, which means there is no analytic expression for
the exact posterior.
For single-user GPPL, we therefore
approximate the preference likelihood with a Gaussian:
%This avoids the need for quadrature methods, as in \cite{hensman2015scalable} or ...
\begin{flalign}
p(\bs f | \bs y, s) & \propto \prod_{p=1}^P p(y_p | z_p) p(\bs f | \bs K, s)
= \prod_{p=1}^P \Phi(z_p) \mathcal{N}(\bs f; \bs 0, \bs K/s)
%= \mathbb{E}\left[\prod_{p=1}^P \Phi(z_p)\right] = \prod_{p=1}^P \Phi(\hat{z}_p) 
%\approx \mathcal{N}(\bs y; \Phi(\hat{\bs z}), \bs Q),
& \\
& \approx \prod_{p=1}^P \mathcal{N}(y_p; \Phi(z_p), Q_{p,p}) \mathcal{N}(\bs f; \bs 0, \bs K/s)
 = \mathcal{N}(\bs y; \Phi(\bs z), \bs Q) \mathcal{N}(\bs f; \bs 0, \bs K/s), &\nonumber 
\end{flalign}
where $\bs Q$ is a diagonal noise covariance matrix.
For crowdGPPL, we use the same approximation to the likelihood, but
replace $\bs f$ with $\bs F$ throughout this section.
%There are two problems with this approximation so far:
%firstly, $\Phi(\bs z)$ is a nonlinear function of $\bs f$,
%which makes the posterior intractable:
%\begin{flalign}
%p(\bs f | \bs y) %\propto p(\bs y | \bs f)\mathcal{N}(\bs f; \bs 0, \bs K/s)
%\approx \frac{ \mathcal{N}(\bs y; \Phi(\bs z), \bs Q)\mathcal{N}(\bs f; \bs 0, \bs K/s) }{
%\int \mathcal{N}(\bs y; \Phi(\bs z), \bs Q)\mathcal{N}(\bs f'; \bs 0, \bs K/s) df'}
%\end{flalign}
%Secondly, we need to estimate the diagonal variance terms in $\bs Q$.

Since each $\Phi(z_p)$ term defines a Bernoulli distribution over $y_p$,
we estimate the diagonals of $\bs Q$ 
by moment matching the variance of the Bernoulli distributions
to give $Q_{p,p} = \Phi(z_p)(1 - \Phi(z_p))$.
However, this means that the variance of the approximate likelihood
depends on $\bs z$ and therefore on $\bs f$,
which means the posterior remains intractable.
We resolve this by approximating $Q_{p,p}$ 
independently for each pairwise label, $p$,
thereby replacing intractable expectations with respect to $p(\bs f|\bs y)$
with simple updates to the parameters of the conjugate priors over each $\Phi(z_p)$.
The conjugate prior for the Bernoulli distribution with parameter $\Phi(z_p)$  
is a beta distribution with parameters $\gamma$ and $\lambda$.
We find $\gamma$ and $\lambda$ by 
matching the moments of the beta distribution to the mean and variance of the prior defined by the Gaussian process, 
$p(\Phi(z_p) | \bs K_{\theta}, \alpha_0, \beta_0)$,  
found using numerical integration. 
Assuming a beta prior over $\Phi(z_p)$ means that $y_p$ has a beta-Bernoulli
distribution. Given the observed label $y_p$, we use
the variance of the posterior beta-Bernoulli to estimate the diagonal variance terms in $\bs Q$ as:
\begin{flalign}
Q_{p,p} & \approx \frac{ (\gamma + y_p)(\lambda + 1 - y_p) }{\gamma + \lambda + 1}.
\end{flalign}
This approximation has previously given good empirical performance when used
for Gaussian process classification~\citep{reece2011determining,simpson2017bayesian} and 
classification using extended Kalman filters~\citep{lee2010sequential,lowne2010sequential}. 

Unfortunately, the nonlinear term $\Phi(\bs z)$ means that the posterior is still intractable, 
so we replace $\Phi(\bs z)$ with a linear function of $\bs f$ by taking
the first-order Taylor series expansion of $\Phi(\bs z)$ 
about the expectation of $\bs f$:
\begin{flalign}
\Phi(\bs z) &\approx \tilde{\Phi}(\bs z) = \bs G (\bs f-\mathbb{E}[\bs f]) + \Phi(\hat{\bs z}), \\
G_{p,i} &= \Phi(\mathbb{E}[z_p])(1 - \Phi(\hat{z}_p)) (2y_p - 1)( [i = a_p] - [i = b_p]) 
\end{flalign}
where $\bs G$ is a matrix containing elements $G_{p,i}$, which are the
partial derivatives of the pairwise likelihood with respect to each of 
the latent function values, $\bs f$.
This creates a circular dependency between the posterior mean of $\bs f$,
which is needed to compute $\hat{\bs z}$, and $\bs G$. %the linearization terms in the likelihood,
These terms
can be estimated using a variational inference procedure
that iterates between updating $\bs f$ and $\bs G$~\citep{steinberg2014extended},
and is described in more detail below.

The complete approximate posterior for GPPL is now as follows:
\begin{flalign}
p(\bs f | \bs y, s) & 
\approx \frac{1}{Z}
\mathcal{N}(\bs y; \bs G (\bs f-\mathbb{E}[\bs f]) + \Phi(\hat{\bs z}), \bs Q) \mathcal{N}(\bs f; \bs 0, \bs K/s) = \mathcal{N}(\bs f; \hat{\bs f}, \bs C), &
\label{eq:likelihood_approx} 
\end{flalign}
where $Z$ is a normalisation constant.
Linearisation means that our approximate likelihood is now conjugate to the prior,
so the approximate posterior is also Gaussian. 
%TODO any other variant in Nickish that uses linearization? Can we cite nickisch to say that linearization is good, m'kay?
%TODO show the posterior without further SVI approximations?
Gaussian approximations to the posterior have shown strong empirical results for tasks 
such as classification~\citep{nickisch2008approximations}, including the 
expectation propagation method~\citep{rasmussen_gaussian_2006}.
Linearisation using a Taylor expansion has also been widely tested
in the extended Kalman filter~\citep{haykin2001kalman},
as well as applied previously to Gaussian processes with non-Gaussian likelihoods~\citep{steinberg2014extended,bonilla2016extended}.
Given our approximate posterior, we can now derive an efficient inference scheme using SVI.

\subsection{SVI for Single User GPPL}

%TODO what's going on here? First, we need to learn s. Second, we need a more efficient way to learn
% f without inverting K and without using all observations at once.
Our linear approximation in the previous section permits
iterative approximate inference for GPPL given $s$. However, computing the posterior covariance and mean requires inverting
$\bs K$, which has computational cost $\mathcal{O}(N^3)$,
and taking an expectation with respect to $s$, which remains intractable. 
We address these problems using stochastic variational inference (SVI).
First,
we introduce a sparse approximation to the Gaussian process that allows
us to limit the size of the covariance matrices we need to work with.
This sparse approximation introduces a set of $M \ll N$ \emph{inducing} items with inputs 
$\bs X_m$,
utilities $\bs f_m$, covariance $\bs K_{mm}$,
and covariance between the observed and inducing items, $\bs K_{nm}$.
% The inducing points act as proxies for the observed points during inference,
% and thereby reduce the number of data points we have to perform costly operations % over.
%We modify the variational approximation in Equation \ref{eq:vb_approx} to introduce the inducing points 
For clarity, we omit $\theta$ from this point on.
We now assume a \emph{mean-field} approximation to the posterior over the inducing and training items
that factorises between different sets of latent variables:
\begin{flalign}
p(\bs f, \bs f_m, s | \bs y, \bs X, \bs X_m, k_{\theta}, \alpha_0, \beta_0) &\approx q(\bs f, \bs f_m, s) = q(s)q(\bs f)q(\bs f_m), \label{eq:svi_approx} &&
\end{flalign}
where $q(.)$ are \emph{variational factors}, which we define below. 
Each factor corresponds to a subset of latent variables, $\bs z$, and
takes the form $\ln q(\bs z) = \mathbb{E}_{\not \bs z}[\ln p(\bs z, \bs x, \bs y)]$.
That is, the expectation with respect
to all other latent variables, $\not \bs z$, of the log joint distribution
of the observations and the current subset of latent variables, $\bs z$.

To obtain the factor for $\bs f_m$, we marginalise $\bs f$ and take expectations with respect to $q(s)$:
\begin{flalign}
\ln q(\bs f_m) &= \ln \mathcal{N}\left(\bs y; \tilde{\Phi}(\bs z), \bs Q\right)
+ \ln\mathcal{N}\left(\bs f_m; \bs 0, \bs K_{mm}/\mathbb{E}\left[s\right]\right)  + \textrm{const}, & \nonumber \\
 & = \ln \mathcal{N}(\bs f_m; \hat{\bs f}_m, \bs S ),
 \label{eq:fhat_m}
\end{flalign}
where the variational parameters $\hat{\bs f}_m$ and $\bs S$ are computed using the iterative SVI procedure described below.
We choose an approximation of $q(\bs f)$ that depends only on the inducing point utilities, $\bs f_m$, and is independent of the observations:
 \begin{flalign}
\ln q(\bs f) & = \ln \mathcal{N}(\bs f; \bs A \hat{\bs f}_m, 
\bs K + \bs A (\bs S - \bs K_{mm}/\mathbb{E}[s]) \bs A^T ),
\end{flalign}
where $\bs A=\bs K_{nm} \bs K^{-1}_{mm}$.
This means we no longer need to invert an $N \times N$ covariance matrix to compute $q(\bs f)$.
The factor $q(s)$ is also modified to depend on the inducing points:
\begin{flalign}
& \ln q(s) = \mathbb{E}_{q(\bs f_m)}[\ln\mathcal{N}(\bs f_m| \bs 0, \bs K_{mm}/s)] + \ln \mathcal{G}(s; \alpha_0, \beta_0) + \mathrm{const}
= \ln \mathcal{G}(s; \alpha, \beta), & \label{eq:qs}
\end{flalign}
where $\alpha= \alpha_0 + \frac{M}{2}$ and $\beta = \beta_0 + \frac{
\textrm{tr}(\bs K^{-1}_{mm}(S + \hat{\bs f}_m \hat{\bs f}_m^T))}{2}$.
The expected value is  
$\mathbb{E}[s] = \frac{\alpha}{\beta}$.

We apply variational inference to iteratively reduce the KL-divergence between our approximate posterior
%$q(s)q(\bs f)q(\bs f_m)$
and the true posterior (Equation \ref{eq:svi_approx}) %, $p(s, \bs f, \bs f_m | \bs K, \alpha_0, \beta_0, \bs y)$,
by maximizing a lower bound, $\mathcal{L}$, on the log marginal likelihood (see also the detailed equations in Appendix \ref{sec:vb_eqns}), which is given by:
%(see also Equation \ref{eq:full_L_singleuser} in the Appendix):%, $\ln p(\bs y | \bs K, \alpha_0, \beta_0)$ :
\begin{flalign}
&\ln p(\bs y | \bs K, \alpha_0, \beta_0) = \textrm{KL}(q(\bs f, \bs f_m, s)  || p(\bs f, \bs f_m, s | \bs y, \bs K, \alpha_0, \beta_0)) 
+ \mathcal{L} & \label{eq:lowerbound}
\\
%\end{flalign}
%Taking expectations with respect to the variational $q$ distributions, $\mathcal{L}$ is:
%\begin{flalign}
&\mathcal{L} = \mathbb{E}_{q(\bs f)}[\ln p(\bs y | \bs f)]
+ \mathbb{E}_{q(\bs f_m, s))}[\ln p(\bs f_m, s | \bs K, 
\alpha_0, \beta_0) -\ln q(\bs f_m) - \ln q(s) ]. & \nonumber
\end{flalign}
%         invK_mm_expecFF = self.invK_mm.dot(self.uS + self.um_minus_mu0.dot(self.um_minus_mu0.T))
%         self.rate_s = self.rate_s0 + 0.5 * np.trace(invK_mm_expecFF)
To optimize $\mathcal{L}$,
we initialize the $q$ factors randomly, then
update each one in turn, taking expectations with respect to the other factors. 

The only term in $\mathcal{L}$ that refers to the observations, $\bs y$, 
is a sum of $P$ terms, each of which refers to one observation only.
This means that $\mathcal{L}$ can be maximized iteratively by considering a random subset of 
observations at each iteration~\citep{hensman2013gaussian}.
%Therefore, the SVI solution replaces Equations \ref{eq:fhat_m} and \ref{eq:S} for computing
%$\hat{\bs f}_m$ and $\bs S$ over all observations with a sequence of stochastic updates.
For the $i$th update of $q(\bs f_m)$, we randomly select $P_i$ 
observations $\bs y_i = \{ y_p \forall p \in \bs P_i \}$, 
where $\bs P_i$ is a random subset of indexes of observations,
and $P_i$ is a mini-batch size.
%Rather than using the complete matrices, 
We then perform updates using $\bs Q_i$ (rows and columns of $\bs Q$ for observations in $\bs P_i$),
$\bs K_{im}$ and $\bs A_i$, (rows of $\bs K_{nm}$ and $\bs A$ referred to by $\{y_p \forall p \in \bs P_i\}$),
$\bs G_i$ (rows of $\bs G$ in $\bs P_i$ and columns referred to by any items $\{a_p \forall p \in \bs P_i \} \cup \{ b_p \forall p \in \bs P_i\}$),
and $\hat{\bs z}_i = \{ \mathbb{E}[\bs z_p] \forall p \in P_i \}$.
%All matrices with subscript $_i$ contain only the subset of elements relating to 
%observations in $\bs P_i$.
% The linearization matrix $\bs G_i$ is the subset of elements in $\bs G$ relating to observations in $\bs P_i$, 
%  is the corresponding subset of elements in $\bs Q$,
%  is the covariance between the items referred to by pairs in $\bs P_i$ 
% and the inducing points,
% and  contains the corresponding rows of $\bs A$.
The update equations optimize the natural parameters of the Gaussian distribution by following the
natural gradient~\citep{hensman2015scalable}:
\begin{flalign}
\bs S^{-1}_i  & = (1 - \rho_i) \bs S^{-1}_{i-1} + \rho_i\left( \mathbb{E}[s]\bs K_{mm}^{-1} + \pi_i\bs A_i^T \bs G^T_{i} \bs Q^{-1}_i \bs G_{i} \bs A_{i} \right)& 
\label{eq:S_stochastic} \\
\hat{\bs f}_{m,i}  & = \bs S_i \left( (1 - \rho_i) \bs S^{-1}_{i-1} \hat{\bs f}_{m,i-1}  + 
%\right. \nonumber \\
%& \left.\hspace{1.5cm} 
\rho_i \pi_i  
\bs A_{i}^{T} \bs G_{i}^T \bs Q_i^{-1} \left( \bs y_i  - \Phi(\mathbb{E}[\bs z_i]) + \bs G_{i} \bs A_i \hat{\bs f}_{m,i} \right) \right) & 
\label{eq:fhat_stochastic}
\end{flalign}
where
$\rho_i=(i + \epsilon)^{-r}$ is a mixing coefficient that controls the update rate,
$\pi_i = \frac{P}{P_i}$ weights each update according to sample size,
 $\epsilon$ is a delay hyperparameter and $r$ is a forgetting rate~\citep{hoffman2013stochastic}.

By performing updates in terms of mini-batches, 
the computational complexity of Equations \ref{eq:S_stochastic} and
\ref{eq:fhat_stochastic} has order $\mathcal{O}(M^3)$, and the second term has order $\mathcal{O}(P_i M^2 + P_i^2 M + M^3)$. The $P_i^2$ term arises due to $G_i$, which is an $N_i \times P_i$ matrix, where $N_i \leq 2P_i$ is the number of 
items referred to by the pairwise labels in the mini-batch.
Memory complexity is now bounded by $\mathcal{O}(M^2 + P_i^2 + M P_i)$, where each complexity term is due
to the sizes of $K_{mm}$, $G_i$ and $K_{im}$.
Note that the only parameters that must be stored between iterations relate to the 
inducing points, hence the memory consumption does not grow with the dataset size 
as in the GFITC approximation used by \citet{houlsby2012collaborative}.
A further advantage of stochastic updating is that the $s$ parameter (and any other global
parameters not immediately depending on the data) can be learned
before the entire dataset has been processed,
which means that poor initial estimates of $s$ are rapidly improved
and the algorithm can converge faster.

The complete SVI algorithm is summarized in Algorithm \ref{al:singleuser}.
\begin{algorithm}[h]
 \KwIn{ Pairwise labels, $\bs y$, training item features, $\bs x$, 
 test item features $\bs x^*$}
 \nl Compute kernel matrices $\bs K$, $\bs K_{mm}$ and $\bs K_{nm}$ given $\bs x$
 \nl Initialise $\mathbb{E}[s]$, $\mathbb{E}[\bs f]$and $\hat{\bs f}_m$ to prior means
 and $\bs S$ to prior covariance $\bs K_mm$\;
 \While{$\mathcal{L}$ not converged}
 {
 \nl Select random sample, $\bs P_i$, of $P$ observations
 \While{$\bs G_i$ not converged}
  {
  \nl Compute $\mathbb{E}[\bs f_i]$ \;
  \nl Compute $\bs G_i$ given $\mathbb{E}[\bs f_i]$ \;
  \nl Compute $\hat{\bs f}_{m,i}$ and $\bs S_{i}$ \;
  }
 \nl Update $q(s)$ and compute $\mathbb{E}[s]$ and $\mathbb{E}[\ln s]$\;
 }
\nl Compute kernel matrices for test items, $\bs K_{**}$ and $\bs K_{*m}$, given $\bs x^*$ \;
\nl Use converged values of $\mathbb{E}[\bs f]$and $\hat{\bs f}_m$ to estimate
posterior over $\bs f^*$ at test points \;
\KwOut{ Posterior mean of the test values, $\mathbb{E}[\bs f^*]$ and covariance, $\bs C^*$ }
\vspace{0.5cm}
\caption{The SVI algorithm for GPPL: preference learning with a single user.}
\label{al:singleuser}
\end{algorithm}
Using an inner loop to learn $\bs G_i$ avoids the need to store the complete matrix, 
$\bs G$.
It is possible to distribute computation in lines 3-6 by selecting multiple random samples
to be processed in parallel. A global estimate of $\hat{\bs f}_m$ and $\bs S$
is passed to each compute node, which runs the loop over lines 4 to 6.
The resulting updated $\hat{\bs f}_m$ and $\bs S$ values are then passed back to a 
central node that combines them by taking the mean over the values computed by 
each node, weighted by the size of each batch. 
This combination step is permitted without modifying Equations \ref{eq:S_stochastic} and \ref{eq:fhat_stochastic}, since they already contain a sum weighted by $\pi_i$.


Inducing point locations can be learned
as part of the variational inference procedure ~\citep{hensman2015scalable},
or by optimizing a bound on the log marginal likelihood.
However, the former breaks the convergence guarantees, and both approaches
may add substantial computational cost. 
We find that we are able to obtain good performance by choosing inducing points up-front using K-means++ ~\citep{arthur2007k} with $M$ clusters to  
cluster the feature vectors, 
then taking the cluster centres as inducing points that represent the spread of observations across feature space.

The inferred distribution over the inducing points can be used 
to estimate the posteriors of test items, $f(\bs x^*)$, according to:
\begin{flalign}
\bs f^* \! \! &= \bs K_{*m} \bs K_{mm}^{-1} \hat{\bs f}_m, &
\bs C^* \! \! = \bs K_{**} + \bs K_{*m} \bs K_{mm}^{-1} (\bs S - \bs K_{mm} / \mathbb{E}[s] ) \bs K_{*m}^T \bs K_{mm}^{-1},
\end{flalign}
where $\bs C^*$ is the posterior covariance of the test items, $\bs K_{**}$ is their prior covariance, and
$\bs K_{*m}$ is the covariance between test and inducing items.
%It is possible to recover the lower bound proposed by 
%\citet{hensman2015scalable} for classification by generalizing the
%likelihood to arbitrary nonlinear functions, and omitting terms relating to $p(s|\alpha_0,\beta_0)$ and $q(s)$.
% However, our approach avoids expensive quadrature methods by linearizing the likelihood to enable analytical updates. We also infer $s$ in a Bayesian manner, 
% rather than treating as a hyper-parameter, which is important for preference learning where $s$ controls the noise level of the observations relative to  $f$. 

\subsection{SVI for CrowdGPPL}

We now extend the SVI method to the crowd preference learning model proposed in
Section \ref{sec:crowd_model}.
To begin with, we extend the variational posterior for GPPL (Equation \ref{eq:svi_approx})
to the crowdGPPL model defined in Equation \ref{eq:joint_crowd}:
\begin{flalign}
& p( \bs V, \bs V_m, \bs W, \bs W_m, \bs t, \bs t_m, s^{(v)}_1, .., s^{(v)}_C,
s^{(w)}_1, .., s^{(w)}_C, s^{(t)} | \bs y, \bs X, \bs X_m, \bs U, \bs U_m, k, \alpha_0, \beta_0 ) 
& \nonumber \\
& \approx q(\bs t) q(\bs t_m)q\left(s^{(t)}\right)\prod_{c=1}^{C} q(\bs v_{c})q(\bs w_c)q(\bs v_{c,m})q(\bs w_{c,m})
q\left(s^{(v)}_c\right)q\left(s^{(w)}_c\right) & %\nonumber \\
%& = q(\bs F) q(s^{(t)}) \prod_{c=1}^C q(s^{(v)}_c), &
\end{flalign}
where $\bs U_m$ are the feature vectors of inducing users and the variational $q$ factors are defined below.
SVI can then be used to optimize the lower bound on the marginal likelihood 
(see also Equation \ref{eq:lowerbound_crowd_full} in the Appendix), given by:
\begin{flalign}
\mathcal{L}_{cr} & = 
\mathbb{E}_{q(\bs F)}%(\bs t, \bs t_m, \bs V, \bs V_m, \bs W, \bs W_m, s_1,...,s^{(v)}_c,s^{(t)})
[\ln p(\bs y | \bs F)] 
+ \mathbb{E}_{q(\bs t_m), q(s^{(t)})}[\ln p(\bs t_m, s^{(t)} | \bs K_{mm}, \alpha_0, \beta_0)
- \ln q(\bs t_m)] & \nonumber \\
&\sum_{c=1}^C \!\! \bigg\{ 
\mathbb{E}_{q(\bs v_{m,c}), q(s^{(v)}_c)}[\ln p(\bs v_{m,c}, s^{(v)}_c | \bs K_{mm}, \alpha_0, \beta_0) - \ln q(\bs v_{m,c})]
&  \nonumber \\ 
 & +  \mathbb{E}_{q(\bs w_{m,c})}[\ln p(\bs w_{m,c} | \bs L_{mm}/s^{(w)}_c )
  - \ln q(\bs w_{m,c} ) ] \bigg\} . & 
  \label{eq:lowerbound_crowd}
\end{flalign}
The algorithm (detailed in Algorithm \ref{al:crowdgppl} in Appendix)
follows the same pattern as Algorithm \ref{al:singleuser}, 
updating each of the $q$ factors in turn by computing means and covariances
for  $\bs V_m$, $\bs W_m$ and $\bs t_m$ instead of $\bs f_m$.

The variational factor for the $c$th inducing item component is:
\begin{flalign}
\ln q(\bs v_{m,c}) = \;\;& 
\ln \mathcal{N}\left( \bs y; \tilde{\Phi}(\bs z), Q \right) % \right] 
 + \ln\mathcal{N}\left(\bs v_{m,c}; \bs 0, \bs K_{mm}/ \mathbb{E}[s^{(v)}_c]\right) 
+ \textrm{const} & \nonumber \\
% are the dimensions collapsed to a single MVN?
= \;&  \ln \mathcal{N}(\bs v_{m,c}; \hat{\bs v}_{m,c}, \bs S_c^{(v)}), &
\end{flalign}
where the posterior mean $\hat{\bs v}_{m,c}$ and covariance $\bs S_c^{(v)}$ are computed using 
update equations of the same form as those of the single user GPPL in 
Equations \ref{eq:S_stochastic} and \ref{eq:fhat_stochastic}.
The noise precision, $\bs Q^{-1}$, in Equation \ref{eq:S_stochastic} is scaled by 
expectation terms over $\bs w_{m,c}$,
and $\hat{\bs f}_{m,i}$ is replaced by $\hat{\bs v}_{m,c,i}$.
For reasons of space, we provide the complete equations for $\hat{\bs v}_{m,c}$ and $\bs S_c^{(v)}$ in 
Appendix \ref{sec:post_params}, Equations \ref{eq:Sv} and \ref{eq:hatv}.

The factor for the inducing points of $\bs t$ follows a similar pattern to $\bs v_{m,c}$:
\begin{flalign}
\ln q(\bs t_m) = \;\;& %\mathbb{E}_{q(\bs F)}[
\ln \mathcal{N}\left( \bs y; \tilde{\Phi}(\bs z), Q \right) %] 
+ \ln\mathcal{N}(\bs t_m; \bs 0, \bs K_{mm}/\mathbb{E}[s^{(t)}])
+ \textrm{const} & \nonumber \\
= \;\;& \ln \mathcal{N}\left( \bs t_m; \hat{\bs t}_{m}, \bs S^{(t)} \right), &
\end{flalign}
where the posterior mean $\hat{\bs t}$ and covariance $\bs S^{(t)}$ uses
the same updates Equations \ref{eq:S_stochastic} and \ref{eq:fhat_stochastic}, 
except $\hat{\bs f}_{m,i}$ is replaced by $\hat{\bs t}_{m,i}$ 
(see also Equations \ref{eq:St} and \ref{eq:hatt}).

Finally, %require a different linearization matrix, $\bs J \in P \times U$, containing partial derivatives 
%of the pairwise likelihood with respect to $\hat{w}_c$. Its elements are given by:
%\begin{flalign}
%J_{p,j} = \Phi(\mathbb{E}[z_p])(1 - \Phi(\mathbb{E}[z_p]) (2y_p - 1) [u_p = j] % needs to be added or subtracted depending on a or b
%\end{flalign} 
%now multiply by V. What about covariances between v?
the variational distribution for each inducing users component is:% then as follows:
\begin{flalign}
\ln q(\bs w_{m,c}) = \;\;& %\mathbb{E}_{q(\bs F)}\left[
\ln \mathcal{N}\left( \bs y; \tilde{\Phi}(\bs z), Q \right) %\right] 
+ \ln\mathcal{N}(\bs w_{m,c}; \bs 0, \bs L_{mm}/s^{(w)}_c)
+ \textrm{const} & \nonumber \\
= \;\;& \ln \mathcal{N}\left( \bs w_{m,c}; \hat{\bs w}_{m,c}, \bs \Sigma_c \right), & 
\end{flalign}
where $\hat{\bs w}_c$ and $\bs \Sigma_{c}$ also follow the pattern of
Equations \ref{eq:S_stochastic} and \ref{eq:fhat_stochastic},
with the noise precision, $\bs Q^{-1}$, scaled by expectations terms involving 
$\bs w_{c,m}$,
 and $\hat{\bs f}_{m,i}$ replaced by $\hat{\bs w}_{m,c,i}$.
The full definitions
are given in Appendix \ref{sec:post_params}, Equations \ref{eq:St} and \ref{eq:hatt}.
The computational and memory complexity are the same as for single-user GPPL, 
except that we now have $C$ updates, and the number of inducing points for items and users 
may be different:
$\mathcal{O}(CM_{\mathrm{items}}^3 + CM_{\mathrm{items}}^2 P_i + CM_{\mathrm{items}} P_i^2 + CM_{\mathrm{users}}^3 + CM_{\mathrm{users}}^2 P_i + CM_{\mathrm{users}} P_i^2 )$.
Memory complexity for crowdGPPL is
$\mathcal{O}(CM_{\mathrm{items}}^2 + P_i^2 + M_{\mathrm{items}} P_i + CM_{\mathrm{users}}^2 + M_{\mathrm{users}} P_i$.

The expectations for the inverse scales, $s^{(v)}_1,..,s^{(v)}_c$, $s^{(w)}_1,..,s^{(w)}_c$
 and $s^{(t)}$ can be computed using Equation \ref{eq:qs} by
substituting in the corresponding terms for each $\bs v_c$, $\bs w_c$ or $\bs t$ instead of $\bs f$. 
% The equations for the means and covariances 
% can be adapted for stochastic updating by applying weighted sums over
% the stochastic update and the previous values in the 
% same way as  Equation \ref{eq:S_stochastic} and \ref{eq:fhat_stochastic}.
% The stochastic updates for the inducing points of the latent factors depend 
% on expectations with respect to the observed points. 
% As with the single user case, the variational factors at the observed items are independent of the observations given the variational factors of the inducing points
% (likewise for the observed users):
% \begin{flalign}
% \ln q(\bs V) & = \sum_{c=1}^C \ln \mathcal{N}\left( \bs v_c; \bs A_v\hat{\bs v}_{m,c}, 
% \frac{\bs K_{v}}{\mathbb{E}[s^{(v)}_c]} + \bs A_v (\bs S_{m,c} - \frac{\bs K_{mm}}{\mathbb{E}[s^{(v)}_c]})\bs A_v \right) & \label{eq:qv} \\
% \ln q(\bs t) & = \ln \mathcal{N}\left( \bs t; \bs A_t \hat{\bs t}_m, 
% \frac{\bs K_{t}}{\mathbb{E}[s^{(t)}]} + \bs A_t (\bs s^{(t)} - \frac{\bs K_{mm}}{\mathbb{E}[s^{(t)}]})\bs A_t \right)  & \label{eq:qt}\\
% \ln q(\bs W) & = \sum_{c=1}^C \ln \mathcal{N}\left( \bs w_c; \bs A_w \hat{\bs w}_{m,c}, \bs L_{} + \bs A_w (\bs\Sigma - \bs L_{mm}/s^{(w)}_c) \bs A_w \right). &
% \label{eq:qw}
% \end{flalign}
As with GPPL, the stochastic updates are amenable to parallel computation within one iteration 
of the variational inference algorithm,
 by performing computations for mini-batches of training data in parallel. 

Predictions for crowdGPPL can be made by computing the posterior mean utilities, $\bs F^*$, 
and the covariance $\bs \Lambda_u^*$ for each user, $u$, in the test set:
\begin{flalign} \label{eq:predict_crowd}
&\bs F^* = \hat{\bs t}^* + \sum_{c=1}^C \hat{\bs v}_{c}^{*T} \hat{\bs w}_{c}^*, \hspace{1cm} \bs \Lambda_u^* = \bs C_{t}^* + \sum_{c=1}^C \omega_{c,u}^* \bs C_{v,c}^* + \hat{w}_{c,u}^2  \bs C_{v,c}^*  +\omega_{c,u}^* \hat{\bs v}_{c}\hat{\bs v}_{c}^T, &
\end{flalign}
where $\hat{\bs t}^*$, $\hat{\bs v}_{c}^*$ and $\hat{\bs w}_{c}^*$ are posterior test means,
$\bs C_{t}^*$ and $\bs C_{v,c}^*$ are posterior covariances of the test items,
and $\omega_{c,u}^*$ is the posterior variance for the user components for the test users. These
terms are defined in Appendix \ref{sec:predictions}, Equations \ref{eq:tstar} to \ref{eq:omegastar}.
The mean $\bs F^*$ and covariances $\Lambda^*_u$ can be inserted into Equation \ref{eq:plphi} to predict pairwise labels.
In practice, the full covariance terms are needed only for Equation \ref{eq:plphi}, so need only be computed
between items for which we wish to predict pairwise labels. %Hence, the covariance for a large test set need not be computed at once.

%In this section, we proposed an SVI scheme for GPPL and crowdGPPL, 
%which can readily be adapted to regression or classification tasks by swapping out the preference likelihood, resulting in 
%different values for $\bs G$ and $\bs H$. 
% We now show how to learn the  
% length-scale parameters required to compute the prior covariances using typical kernel functions.
%then demonstrate how our method can be applied to learning user preferences or
%consensus opinion when faced with disagreement.
 
