\RequirePackage[fleqn]{amsmath}
\RequirePackage{fix-cm}
%
\documentclass[smallcondensed,natbib]{svjour3}     % onecolumn (ditto)
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}

%\usepackage{mathptmx} % this causes problems with bold font

\usepackage{url}
\makeatletter
\makeatother
%\usepackage[hidelinks]{hyperref}
\DeclareMathOperator*{\argmax}{arg\,max}

\usepackage[fleqn]{amsmath}
%\usepackage{amssymb}
% \usepackage{amstext}
% \usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{algorithm2e}
\usepackage{array}
\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{url}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{numprint}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{todonotes}

\newcommand{\bs}{\boldsymbol}  
\newcommand{\wrtd}{\mathrm{d}}

\makeatletter
\makeatother %some sort of hack related to the symbol @

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{ 
Predicting Preferences for Crowds of Users: a Scalable, Bayesian Approach
%Scalable Bayesian Preference Learning for Crowds of Users
% title change to emphasise that this is a personalised model, not all about aggregation
%Scalable Bayesian Methods for Personalised Preference Learning
%One source of confusion seems to be the inclusion of features, which is a big distinction.
% predictive feature-based models.
% ... with item and user features
% predictive modelling with input features
%Learning predictive preference models from crowds of users: a scalable Bayesian method
% Predicting preferences of crowds of users: a scalable, Bayesian approach
}

\author{Edwin Simpson 
\and Iryna Gurevych \\
Ubiquitous Knowledge Processing Lab, Dept. of Computer Science, Technische Universit\"at Darmstadt, Germany\\
              \email{\{simpson,gurevych\}@ukp.informatik.tu-darmstadt.de}
}
\date{Received: date}
\begin{document}

\titlerunning{Scalable Bayesian Preference Learning}
\authorrunning{Simpson, E and Gurevych, I}

% do not exceed 20 pages including references

\maketitle

\begin{abstract}
We propose a scalable Bayesian preference learning method 
for jointly predicting the preferences of individuals as well as the consensus of a crowd
 from pairwise labels.
Peoples' opinions often differ greatly,
making it difficult to predict their preferences if the amount of data for each user is small.
These personal biases also make it harder to infer the consensus of the whole crowd
when there are few labels per item.
%In applications such as recommendation it is also necessary to predict the preferences of individual annotators or users. 
%and identify common preferences between users. % use of the latent factors 
%We address these challenges by combining matrix factorization to model individual preferences with 
%Gaussian processes to integrate user and item features. By taking a Bayesian approach, our model
We address these challenges by combining matrix factorization with 
Gaussian processes,
using a Bayesian approach to account for uncertainty arising from sparse data and annotation noise.
Our method exploiting input features, such as text embeddings, image features or metadata,
to predict preferences for new items and users with limited or no training data.
As previous methods for Gaussian process preference learning do not scale to 
large numbers of users, items or pairwise labels, 
we propose a stochastic variational inference approach that limits computational and memory costs.
Our experiments on a benchmark recommendation task show that
our method is competitive with previous approaches despite our scalable inference approximation.
We demonstrate the method's scalability empirically
on a natural language processing task with thousands of users and items.
On this task, our method improves consensus prediction over the state of the art
by modelling the preferences of individual members of the crowd.
%We also show how to  %that robustness to %able to learn the effective number of components required to model the data and
%choosing more latent components than required,
%apply gradient-based optimization to length-scale hyper-parameters to improve performance.
We make our software publicly available for future 
work~\footnote{\url{https://github.com/UKPLab/tacl2018-preference-convincing/tree/crowdGPPL}}.
%We show how to make collaborative preference learning work at scale and how it can be used to learn
%a target preference function from crowdsourced data or other noisy preference labels. 
%The collaborative model captures the reliability of each worker or data source and models their biases and error rates. 
%It uses latent factors to share information between similar workers and a target preference function.
%We devise an SVI inference schema to enable the model to scale to real-world datasets.
%Experiments compare results using standard variational inference, laplace approximation and SVI.
%On real-world data we show the benefit of the personalised model over a GP preference learning approach 
%that treats all labels as coming from the same source,
%as well as established alternative methods and classifier baselines.
%We show that the model is able to identify a number of latent features for the workers and for textual arguments.
\end{abstract}

% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
%\IEEEpeerreviewmaketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{intro}
\input{related_work}
\input{model}
\input{inference}
%\input{hyperparameters}
\input{experiments}
\input{discussion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% use section* for acknowledgment
\section*{Acknowledgments}

\bibliographystyle{spbasic}
\bibliography{simpson_scalable_bayesian_pref_learning_from_crowds}

\appendix
\input{appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
